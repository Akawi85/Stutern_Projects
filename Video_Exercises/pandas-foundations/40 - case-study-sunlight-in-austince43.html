<!DOCTYPE html><html lang="en">
<!-- Mirrored from campus.datacamp.com/courses/pandas-foundations/case-study-sunlight-in-austin?ex=6 by HTTrack Website Copier/3.x [XR&CO'2014], Mon, 25 Feb 2019 20:43:35 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><link rel="icon" type="image/png" href="https://campus.datacamp.com/favicon.ico"><link href="../../static/css/main.2a9f739f.css" rel="stylesheet"><title data-react-helmet="true">Cleaning the numeric columns | Python</title><link data-react-helmet="true" href="data:image/png;base64,bW9kdWxlLmV4cG9ydHMgPSBfX3dlYnBhY2tfcHVibGljX3BhdGhfXyArICIvc3RhdGljL21lZGlhL2FwcGxlLWljb24uNGZhMTNiMGYucG5nIjs=" rel="apple-touch-icon"><link data-react-helmet="true" href="data:image/png;base64,bW9kdWxlLmV4cG9ydHMgPSBfX3dlYnBhY2tfcHVibGljX3BhdGhfXyArICIvc3RhdGljL21lZGlhL2FwcGxlLWljb24uNGZhMTNiMGYucG5nIjs=" rel="apple-touch-icon-precomposed"><meta data-react-helmet="true" charset="utf-8"><meta data-react-helmet="true" http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta data-react-helmet="true" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta data-react-helmet="true" name="fragment" content="!"><meta data-react-helmet="true" name="keywords" content="R, Python, Data analysis, interactive, learning"><meta data-react-helmet="true" name="description" content="Here is an example of Cleaning the numeric columns: The numeric columns contain missing values labeled as &apos;M&apos;."><meta data-react-helmet="true" name="twitter:card" content="summary"><meta data-react-helmet="true" name="twitter:site" content="@DataCamp"><meta data-react-helmet="true" name="twitter:title" content="Cleaning the numeric columns | Python"><meta data-react-helmet="true" name="twitter:description" content="Here is an example of Cleaning the numeric columns: The numeric columns contain missing values labeled as &apos;M&apos;."><meta data-react-helmet="true" name="twitter:creator" content="@DataCamp"><meta data-react-helmet="true" name="twitter:image:src" content="../../public/assets/images/var/twitter_share.png"><meta data-react-helmet="true" name="twitter:domain" content="www.datacamp.com"><meta data-react-helmet="true" property="og:title" content="Cleaning the numeric columns | Python"><meta data-react-helmet="true" property="og:image" content="../../public/assets/images/var/linkedin_share.png"><meta data-react-helmet="true" name="google-signin-clientid" content="892114885437-01a7plbsu1b2vobuhvnckmmanhb58h3a.apps.googleusercontent.com"><meta data-react-helmet="true" name="google-signin-scope" content="email profile"><meta data-react-helmet="true" name="google-signin-cookiepolicy" content="single_host_origin"></head><body><script>window.PRELOADED_STATE = "[&quot;~#iM&quot;,[&quot;preFetchedData&quot;,[&quot;^0&quot;,[&quot;course&quot;,[&quot;^0&quot;,[&quot;status&quot;,&quot;SUCCESS&quot;,&quot;data&quot;,[&quot;^ &quot;,&quot;id&quot;,1639,&quot;title&quot;,&quot;pandas Foundations&quot;,&quot;description&quot;,&quot;Pandas DataFrames are the most widely used in-memory representation of complex data collections within Python.  Whether in finance, scientific fields, or data science, a familiarity with Pandas is essential.  This course teaches you to work with real-world data sets containing both string and numeric data, often structured around time series.  You will learn powerful analysis, selection, and visualization techniques in this course.&quot;,&quot;short_description&quot;,&quot;Learn how to use the industry-standard pandas library to import, build, and manipulate DataFrames. &quot;,&quot;author_field&quot;,null,&quot;author_bio&quot;,null,&quot;author_image&quot;,&quot;placeholder.png&quot;,&quot;nb_of_subscriptions&quot;,85428,&quot;slug&quot;,&quot;pandas-foundations&quot;,&quot;image_url&quot;,&quot;https://assets.datacamp.com/production/course_1639/shields/thumb/shield_image_course_1639_20190204-13-1xw9geb?1549311490&quot;,&quot;image_thumbnail_url&quot;,&quot;https://assets.datacamp.com/production/course_1639/shields/thumb_home/shield_image_course_1639_20190204-13-1xw9geb?1549311490&quot;,&quot;last_updated_on&quot;,&quot;04/02/2019&quot;,&quot;link&quot;,&quot;https://www.datacamp.com/courses/pandas-foundations&quot;,&quot;should_cache&quot;,true,&quot;type&quot;,&quot;datacamp&quot;,&quot;difficulty_level&quot;,2,&quot;state&quot;,&quot;live&quot;,&quot;university&quot;,null,&quot;sharing_links&quot;,[&quot;^ &quot;,&quot;twitter&quot;,&quot;http://bit.ly/1eWTMJh&quot;,&quot;facebook&quot;,&quot;http://bit.ly/1iS42Do&quot;],&quot;programming_language&quot;,&quot;python&quot;,&quot;paid&quot;,true,&quot;time_needed&quot;,&quot;4 hours&quot;,&quot;xp&quot;,5150,&quot;topic_id&quot;,3,&quot;reduced_outline&quot;,null,&quot;runtime_config&quot;,&quot;heavy&quot;,&quot;lti_only&quot;,false,&quot;chapters&quot;,[[&quot;^ &quot;,&quot;id&quot;,4259,&quot;title_meta&quot;,null,&quot;^1&quot;,&quot;Data ingestion &amp; inspection&quot;,&quot;^2&quot;,&quot;In this chapter, you will be introduced to Panda&#39;s DataFrames. You will use Pandas to import and inspect a variety of datasets, ranging from population data obtained from The World Bank to monthly stock data obtained via Yahoo! Finance. You will also practice building DataFrames from scratch, and become familiar with Pandas&#39; intrinsic data visualization capabilities.&quot;,&quot;number&quot;,1,&quot;^8&quot;,&quot;data-ingestion-inspection&quot;,&quot;nb_exercises&quot;,14,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^;&quot;,&quot;04/02/2019&quot;,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_2357/slides/ch1_slides.pdf&quot;,&quot;free_preview&quot;,true,&quot;xp&quot;,1100],[&quot;^ &quot;,&quot;id&quot;,4260,&quot;^M&quot;,null,&quot;^1&quot;,&quot;Exploratory data analysis&quot;,&quot;^2&quot;,&quot;Having learned how to ingest and inspect your data, you will next explore it visually as well as quantitatively. This process, known as exploratory data analysis (EDA), is a crucial component of any data science project. Pandas has powerful methods that help with statistical and visual EDA. In this chapter, you will learn how and when to apply these techniques.&quot;,&quot;^N&quot;,2,&quot;^8&quot;,&quot;exploratory-data-analysis&quot;,&quot;^O&quot;,15,&quot;^P&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^Q&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^;&quot;,&quot;13/11/2018&quot;,&quot;^R&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_2357/slides/ch2_slides.pdf&quot;,&quot;^S&quot;,null,&quot;xp&quot;,1250],[&quot;^ &quot;,&quot;id&quot;,4261,&quot;^M&quot;,null,&quot;^1&quot;,&quot;Time series in pandas&quot;,&quot;^2&quot;,&quot;In this chapter, you will learn how to manipulate and visualize time series data using Pandas. You will become familiar with concepts such as upsampling, downsampling, and interpolation. You will practice using Pandas&#39; method chaining to efficiently filter your data and perform time series analyses. From stock prices to flight timings, time series data are found in a wide variety of domains and being able to effectively work with such data can be an invaluable skill.&quot;,&quot;^N&quot;,3,&quot;^8&quot;,&quot;time-series-in-pandas&quot;,&quot;^O&quot;,17,&quot;^P&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^Q&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^;&quot;,&quot;08/01/2019&quot;,&quot;^R&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_2357/slides/ch3_slides.pdf&quot;,&quot;^S&quot;,null,&quot;xp&quot;,1450],[&quot;^ &quot;,&quot;id&quot;,4284,&quot;^M&quot;,null,&quot;^1&quot;,&quot;Case Study - Sunlight in Austin&quot;,&quot;^2&quot;,&quot;Working with real-world weather and climate data, in this chapter you will bring together and apply all of the skills you have acquired in this course. You will use Pandas to manipulate the data into a form usable for analysis, and then systematically explore it using the techniques you learned in the prior chapters. Enjoy!&quot;,&quot;^N&quot;,4,&quot;^8&quot;,&quot;case-study-sunlight-in-austin&quot;,&quot;^O&quot;,16,&quot;^P&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^Q&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^;&quot;,&quot;04/02/2019&quot;,&quot;^R&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_2357/slides/ch4_slides.pdf&quot;,&quot;^S&quot;,null,&quot;xp&quot;,1350]]]]],&quot;chapter&quot;,[&quot;^0&quot;,[&quot;status&quot;,&quot;SUCCESS&quot;,&quot;data&quot;,[&quot;^ &quot;,&quot;id&quot;,4284,&quot;^M&quot;,null,&quot;^1&quot;,&quot;Case Study - Sunlight in Austin&quot;,&quot;^2&quot;,&quot;Working with real-world weather and climate data, in this chapter you will bring together and apply all of the skills you have acquired in this course. You will use Pandas to manipulate the data into a form usable for analysis, and then systematically explore it using the techniques you learned in the prior chapters. Enjoy!&quot;,&quot;^N&quot;,4,&quot;^8&quot;,&quot;case-study-sunlight-in-austin&quot;,&quot;^O&quot;,16,&quot;^P&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^Q&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^;&quot;,&quot;04/02/2019&quot;,&quot;^R&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_2357/slides/ch4_slides.pdf&quot;,&quot;^S&quot;,null,&quot;xp&quot;,1350]]],&quot;exercises&quot;,[&quot;^0&quot;,[&quot;status&quot;,&quot;SUCCESS&quot;,&quot;data&quot;,[[&quot;^ &quot;,&quot;id&quot;,44654,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;assignment&quot;,null,&quot;^1&quot;,&quot;Reading and cleaning the data&quot;,&quot;sample_code&quot;,&quot;&quot;,&quot;instructions&quot;,null,&quot;^N&quot;,1,&quot;sct&quot;,&quot;&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;attachments&quot;,null,&quot;xp&quot;,50,&quot;possible_answers&quot;,[],&quot;feedbacks&quot;,[],&quot;question&quot;,&quot;&quot;,&quot;video_link&quot;,null,&quot;video_hls&quot;,&quot;//videos.datacamp.com/transcoded/1639_pandas-1/v6/hls-ch4_1.master.m3u8&quot;,&quot;aspect_ratio&quot;,56.25,&quot;projector_key&quot;,&quot;course_1639_76ca82bd71594808142775690560f17f&quot;,&quot;language&quot;,&quot;python&quot;,&quot;randomNumber&quot;,0.6686500359838821,&quot;externalId&quot;,44654],[&quot;^ &quot;,&quot;id&quot;,44655,&quot;^&gt;&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;^T&quot;,&quot;&lt;p&gt;The first step in our analysis is to read in the data. Upon inspection with a certain system tool, we find that the data appears to be ASCII encoded with comma delimited columns, but has no header and no column labels. Which of the following is the best method to start with to read the data files?&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;What method should we use to read the data?&quot;,&quot;^U&quot;,&quot;&quot;,&quot;^V&quot;,null,&quot;^N&quot;,2,&quot;sct&quot;,&quot;&quot;,&quot;^W&quot;,&quot;&quot;,&quot;^X&quot;,&quot;&quot;,&quot;^Y&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Which pandas read method have you used in prior chapters to read in your data?&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,50,&quot;^[&quot;,[&quot;[&lt;code&gt;pd.read_csv()&lt;/code&gt;]&quot;,&quot;&lt;code&gt;pd.to_csv()&lt;/code&gt;&quot;,&quot;&lt;code&gt;pd.read_hdf()&lt;/code&gt;&quot;,&quot;&lt;code&gt;np.load()&lt;/code&gt;&quot;],&quot;^10&quot;,[&quot;Correct! The &lt;code&gt;read_csv()&lt;/code&gt; function will become second nature to you as you continue using pandas.&quot;,&quot;Incorrect.&quot;,&quot;Incorrect.&quot;,&quot;Incorrect.&quot;],&quot;^11&quot;,&quot;&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.5829517747139357,&quot;^18&quot;,44655],[&quot;^ &quot;,&quot;id&quot;,44656,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^T&quot;,&quot;&lt;p&gt;Now that you have identified the method to use to read the data, let&#39;s try to read one file. The problem with real data such as this is that the files are almost never formatted in a convenient way. In this exercise, there are several problems to overcome in reading the file. First, there is no header, and thus the columns don&#39;t have labels. There is also no obvious index column, since none of the data columns contain a full date or time.&lt;/p&gt;\\n&lt;p&gt;Your job is to read the file into a DataFrame using the default arguments. After inspecting it, you will re-read the file specifying that there are no headers supplied. &lt;/p&gt;\\n&lt;p&gt;The CSV file has been provided for you as the variable &lt;code&gt;data_file&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Reading in a data file&quot;,&quot;^U&quot;,&quot;# Import pandas\\n\\n\\n# Read in the data file: df\\ndf = pd.read_csv(____)\\n\\n# Print the output of df.head()\\nprint(df.head())\\n\\n# Read in the data file with header=None: df_headers\\ndf_headers = pd.read_csv(____, ____=None)\\n\\n# Print the output of df_headers.head()\\nprint(df_headers.head())&quot;,&quot;^V&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Import &lt;code&gt;pandas&lt;/code&gt; as &lt;code&gt;pd&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Read the file &lt;code&gt;data_file&lt;/code&gt; into a DataFrame called &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Print the output of &lt;code&gt;df.head()&lt;/code&gt;. This has been done for you. Notice the formatting problems in &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Re-read the data using specifying the keyword argument &lt;code&gt;header=None&lt;/code&gt; and assign it to &lt;code&gt;df_headers&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Print the output of &lt;code&gt;df_headers.head()&lt;/code&gt;. This has already been done for you. Hit &#39;Submit Answer&#39; and see how this resolves the formatting issues.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^N&quot;,3,&quot;sct&quot;,&quot;Ex().has_import(\\&quot;pandas\\&quot;)\\n\\nEx().check_correct(\\n\\tcheck_object(\\&quot;df\\&quot;).has_equal_value(),\\n\\tcheck_function(\\&quot;pandas.read_csv\\&quot;).check_args(0).has_equal_value()  \\t\\n)\\n\\nEx().check_function(&#39;print&#39;, index = 0).check_args(0).has_equal_value()\\n\\nEx().check_correct(\\n\\tcheck_object(\\&quot;df_headers\\&quot;).has_equal_value(),\\n\\tcheck_function(\\&quot;pandas.read_csv\\&quot;, signature = sig_from_obj(&#39;pandas.read_csv&#39;)).multi(\\n    \\tcheck_args(0).has_equal_value()\\n#      \\tcheck_args(&#39;header&#39;).has_equal_value()\\n    )  \\n)\\n\\nEx().check_function(&#39;print&#39;, index = 0).check_args(0).has_equal_value()\\n\\nsuccess_msg(\\&quot;Well done! Note how the column names are not informative. You&#39;ll fix this in the next exercise!\\&quot;)&quot;,&quot;^W&quot;,&quot;data_file = \\&quot;/usr/local/share/datasets/NOAA_QCLCD_2011_hourly_13904.csv\\&quot;\\n\\n&quot;,&quot;^X&quot;,&quot;# Import pandas\\nimport pandas as pd\\n\\n# Read in the data file: df\\ndf = pd.read_csv(data_file)\\n\\n# Print the output of df.head()\\nprint(df.head())\\n\\n# Read in the data file with header=None: df_headers\\ndf_headers = pd.read_csv(data_file, header=None)\\n\\n# Print the output of df_headers.head()\\nprint(df_headers.head())&quot;,&quot;^Y&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;You can import &lt;code&gt;x&lt;/code&gt; from &lt;code&gt;y&lt;/code&gt; using the command &lt;code&gt;from y import x&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;You can read in &lt;code&gt;data_file&lt;/code&gt; by passing it in as an argument to &lt;code&gt;pd.read_csv()&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;&lt;code&gt;df.head()&lt;/code&gt; has already been printed for you.&lt;/li&gt;\\n&lt;li&gt;To indicate that there are no headers in &lt;code&gt;&#39;data.csv&#39;&lt;/code&gt;, specify &lt;code&gt;header=None&lt;/code&gt; in your &lt;code&gt;pd.read_csv()&lt;/code&gt; function.&lt;/li&gt;\\n&lt;li&gt;&lt;code&gt;df_headers.head()&lt;/code&gt; has already been printed for you. Hit &#39;Submit Answer` to see the results!&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,100,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.12915190112887842,&quot;^18&quot;,44656],[&quot;^ &quot;,&quot;id&quot;,44657,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^T&quot;,&quot;&lt;p&gt;After the initial step of reading in the data, the next step is to clean and tidy it so that it is easier to work with.&lt;/p&gt;\\n&lt;p&gt;In this exercise, you will begin this cleaning process by re-assigning column names and dropping unnecessary columns.&lt;/p&gt;\\n&lt;p&gt;pandas has been imported in the workspace as &lt;code&gt;pd&lt;/code&gt;, and the file &lt;code&gt;NOAA_QCLCD_2011_hourly_13904.txt&lt;/code&gt; has been parsed and loaded into a DataFrame &lt;code&gt;df&lt;/code&gt;. The comma separated string of column names, &lt;code&gt;column_labels&lt;/code&gt;, and list of columns to drop, &lt;code&gt;list_to_drop&lt;/code&gt;, have also been loaded for you.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Re-assigning column names&quot;,&quot;^U&quot;,&quot;# Split on the comma to create a list: column_labels_list\\ncolumn_labels_list = ____\\n\\n# Assign the new column labels to the DataFrame: df.columns\\n____ = column_labels_list\\n\\n# Remove the appropriate columns: df_dropped\\ndf_dropped = ____\\n\\n# Print the output of df_dropped.head()\\nprint(df_dropped.head())&quot;,&quot;^V&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Convert the comma separated string &lt;code&gt;column_labels&lt;/code&gt; to a list of strings using &lt;code&gt;.split(&#39;,&#39;)&lt;/code&gt;. Assign the result to &lt;code&gt;column_labels_list&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Reassign &lt;code&gt;df.columns&lt;/code&gt; using the list of strings &lt;code&gt;column_labels_list&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Call &lt;code&gt;df.drop()&lt;/code&gt; with &lt;code&gt;list_to_drop&lt;/code&gt; and &lt;code&gt;axis=&#39;columns&#39;&lt;/code&gt;. Assign the result to &lt;code&gt;df_dropped&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Print &lt;code&gt;df_dropped.head()&lt;/code&gt; to examine the result. This has already been done for you.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^N&quot;,4,&quot;sct&quot;,&quot;Ex().check_correct(\\n\\tcheck_object(\\&quot;column_labels_list\\&quot;).has_equal_value(),\\n\\tcheck_function(\\&quot;column_labels.split\\&quot;, \\n                   signature = sig_from_obj(&#39;column_lables.split&#39;)).check_args(0).has_equal_value(\\&quot;The contents of `column_labels_list` are not correct. Did you correctly call the `split.()` method on `column_labels` to split by `&#39;,&#39;`?\\&quot;)\\n)\\n\\nEx().check_correct(\\n\\tcheck_object(&#39;df&#39;).has_equal_value(),\\n  \\thas_equal_ast(code = &#39;column_labels_list&#39;, incorrect_msg = &#39;Did you assign `column_labels_list` to `df.columns`?&#39;)\\n)\\n\\nEx().check_correct(\\n\\tcheck_object(\\&quot;df_dropped\\&quot;).has_equal_value(),\\n  \\tcheck_function(&#39;df.drop&#39;).multi(\\n\\t\\tcheck_args(0).has_equal_value(),\\n      \\tcheck_args(&#39;axis&#39;).has_equal_value()\\n    )\\n)\\n\\nEx().check_function(&#39;print&#39;).check_args(0).has_equal_value()\\n\\nsuccess_msg(\\&quot;Fantastic! Now that you have informative column names, it is a lot easier to interpret the data! But there is still some tidying work to be done: You&#39;ll clean the datetime data in the next exercise.\\&quot;)&quot;,&quot;^W&quot;,&quot;#import pandas\\nimport pandas as pd\\ndata_file = \\&quot;/usr/local/share/datasets/NOAA_QCLCD_2011_hourly_13904.csv\\&quot;\\n# read the data file\\ndf = pd.read_csv(data_file, index_col=False, header=None)\\n# define the column labels for the real data file from NOAA: column_labels\\ncolumn_labels = \\&quot;Wban,date,Time,StationType,sky_condition,sky_conditionFlag,visibility,visibilityFlag,wx_and_obst_to_vision,wx_and_obst_to_visionFlag,dry_bulb_faren,dry_bulb_farenFlag,dry_bulb_cel,dry_bulb_celFlag,wet_bulb_faren,wet_bulb_farenFlag,wet_bulb_cel,wet_bulb_celFlag,dew_point_faren,dew_point_farenFlag,dew_point_cel,dew_point_celFlag,relative_humidity,relative_humidityFlag,wind_speed,wind_speedFlag,wind_direction,wind_directionFlag,value_for_wind_character,value_for_wind_characterFlag,station_pressure,station_pressureFlag,pressure_tendency,pressure_tendencyFlag,presschange,presschangeFlag,sea_level_pressure,sea_level_pressureFlag,record_type,hourly_precip,hourly_precipFlag,altimeter,altimeterFlag,junk\\&quot;\\n\\n# define the sub-set list of columns to drop\\nlist_to_drop = [ &#39;sky_conditionFlag&#39;, &#39;visibilityFlag&#39;, &#39;wx_and_obst_to_vision&#39;, &#39;wx_and_obst_to_visionFlag&#39;,\\n  &#39;dry_bulb_farenFlag&#39;, &#39;dry_bulb_celFlag&#39;, &#39;wet_bulb_farenFlag&#39;, &#39;wet_bulb_celFlag&#39;, &#39;dew_point_farenFlag&#39;,\\n  &#39;dew_point_celFlag&#39;, &#39;relative_humidityFlag&#39;, &#39;wind_speedFlag&#39;, &#39;wind_directionFlag&#39;, &#39;value_for_wind_character&#39;,\\n  &#39;value_for_wind_characterFlag&#39;, &#39;station_pressureFlag&#39;, &#39;pressure_tendencyFlag&#39;, &#39;pressure_tendency&#39;,\\n  &#39;presschange&#39;,&#39;presschangeFlag&#39;, &#39;sea_level_pressureFlag&#39;, &#39;hourly_precip&#39;,  &#39;hourly_precipFlag&#39;, &#39;altimeter&#39;,  &#39;record_type&#39;, &#39;altimeterFlag&#39;, &#39;junk&#39;]&quot;,&quot;^X&quot;,&quot;# Split on the comma to create a list: column_labels_list\\ncolumn_labels_list = column_labels.split(\\&quot;,\\&quot;)\\n\\n# Assign the new column labels to the DataFrame: df.columns\\ndf.columns = column_labels_list\\n\\n# Remove the appropriate columns: df_dropped\\ndf_dropped = df.drop(list_to_drop, axis=&#39;columns&#39;)\\n\\n# Print the output of df_dropped.head()\\nprint(df_dropped.head())&quot;,&quot;^Y&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;By using &lt;code&gt;column_labels.split(&#39;,&#39;)&lt;/code&gt;, you can convert &lt;code&gt;column_labels&lt;/code&gt; into a list of strings. &lt;/li&gt;\\n&lt;li&gt;Set &lt;code&gt;df.columns&lt;/code&gt; to be equal to the list of strings you created above.&lt;/li&gt;\\n&lt;li&gt;The list of columns to drop is called &lt;code&gt;list_to_drop&lt;/code&gt;. This is what you must specify inside &lt;code&gt;df.drop()&lt;/code&gt;, along with &lt;code&gt;axis=&#39;columns&#39;&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;The code to print the output of &lt;code&gt;df_dropped.head()&lt;/code&gt; has already been written for you, so click &#39;Submit Answer&#39; to see the result!&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,100,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.8494507892251537,&quot;^18&quot;,44657],[&quot;^ &quot;,&quot;id&quot;,47096,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^T&quot;,&quot;&lt;p&gt;In order to use the full power of pandas time series, you must construct a &lt;code&gt;DatetimeIndex&lt;/code&gt;. To do so, it is necessary to clean and transform the date and time columns. &lt;/p&gt;\\n&lt;p&gt;The DataFrame &lt;code&gt;df_dropped&lt;/code&gt; you created in the last exercise is provided for you and pandas has been imported as &lt;code&gt;pd&lt;/code&gt;.&lt;/p&gt;\\n&lt;p&gt;Your job is to clean up the &lt;code&gt;date&lt;/code&gt; and &lt;code&gt;Time&lt;/code&gt; columns and combine them into a datetime collection to be used as the Index.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Cleaning and tidying datetime data&quot;,&quot;^U&quot;,&quot;# Convert the date column to string: df_dropped[&#39;date&#39;]\\ndf_dropped[&#39;date&#39;] = ____\\n\\n# Pad leading zeros to the Time column: df_dropped[&#39;Time&#39;]\\ndf_dropped[&#39;Time&#39;] = df_dropped[&#39;Time&#39;].apply(lambda x:&#39;{:0&gt;4}&#39;.format(x))\\n\\n# Concatenate the new date and Time columns: date_string\\ndate_string = ____\\n\\n# Convert the date_string Series to datetime: date_times\\ndate_times = pd.____(date_string, ____=&#39;%Y%m%d%H%M&#39;)\\n\\n# Set the index to be the new date_times container: df_clean\\ndf_clean = ____\\n\\n# Print the output of df_clean.head()\\nprint(df_clean.head())&quot;,&quot;^V&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Convert the &lt;code&gt;&#39;date&#39;&lt;/code&gt; column to a string with &lt;code&gt;.astype(str)&lt;/code&gt; and assign to &lt;code&gt;df_dropped[&#39;date&#39;]&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Add leading zeros to the &lt;code&gt;&#39;Time&#39;&lt;/code&gt; column. This has been done for you.&lt;/li&gt;\\n&lt;li&gt;Concatenate the new &lt;code&gt;&#39;date&#39;&lt;/code&gt; and &lt;code&gt;&#39;Time&#39;&lt;/code&gt; columns together. Assign to &lt;code&gt;date_string&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Convert the &lt;code&gt;date_string&lt;/code&gt; Series to datetime values with &lt;code&gt;pd.to_datetime()&lt;/code&gt;. Specify the &lt;code&gt;format&lt;/code&gt; parameter.&lt;/li&gt;\\n&lt;li&gt;Set the index of the &lt;code&gt;df_dropped&lt;/code&gt; DataFrame to be &lt;code&gt;date_times&lt;/code&gt;. Assign the result to &lt;code&gt;df_clean&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^N&quot;,5,&quot;sct&quot;,&quot;Ex().check_or(\\n    has_equal_ast(\\&quot;Did you convert the `&#39;date&#39;` column of `df_dropped` to strings with `.astype(str)`?\\&quot;,\\n                  code=\\&quot;df_dropped[&#39;date&#39;].astype(str)\\&quot;,\\n                  exact=False),\\n    has_equal_ast(\\&quot;Did you convert the `&#39;date&#39;` column of `df_dropped` to strings with `.astype(str)`?\\&quot;,\\n                  code=\\&quot;df_dropped.date.astype(str)\\&quot;,\\n                  exact=False))\\n\\nEx().check_or(\\n    has_equal_ast(\\&quot;You do not need to alter the provided code to pad leading zeros to the `&#39;Time&#39;` column.\\&quot;,\\n                  code=\\&quot;df_dropped[&#39;Time&#39;].apply(lambda x:&#39;{:0&gt;4}&#39;.format(x))\\&quot;,\\n                  exact=False),\\n    has_equal_ast(\\&quot;You do not need to alter the provided code to pad leading zeros to the `&#39;Time&#39;` column.\\&quot;,\\n                           code=\\&quot;df_dropped.Time.apply(lambda x:&#39;{:0&gt;4}&#39;.format(x))\\&quot;,\\n                           exact=False))\\n\\nEx().check_correct(\\n\\tcheck_object(&#39;date_string&#39;).has_equal_value(),\\n  \\thas_equal_ast(code = &#39;df_dropped[\\\\&#39;date\\\\&#39;] + df_dropped[\\\\&#39;Time\\\\&#39;]&#39;, incorrect_msg = &#39;Did you concatenate the `\\\\&#39;date\\\\&#39;` and `\\\\&#39;time\\\\&#39;` columns of `df_dropped` to create `date_string`?&#39;)\\n)\\n\\nEx().check_correct(\\n\\tcheck_object(\\&quot;date_times\\&quot;).has_equal_value(),\\n\\tcheck_function(\\&quot;pandas.to_datetime\\&quot;).multi(\\n    \\tcheck_args(0).has_equal_value(),\\n      \\tcheck_args(&#39;format&#39;).has_equal_value()\\n    )\\n)\\n\\nEx().check_correct(\\n\\tcheck_object(\\&quot;df_clean\\&quot;).has_equal_value(),\\n    check_function(\\&quot;df_dropped.set_index\\&quot;).check_args(0).has_equal_value(\\&quot;The contents of `df_clean` are not correct. Did you set its index to be `date_times`?\\&quot;)\\n)\\n\\nEx().check_function(&#39;print&#39;).check_args(0).has_equal_value()\\n\\nsuccess_msg(\\&quot;Well done! All that&#39;s left now is to clean the numeric columns.\\&quot;)&quot;,&quot;^W&quot;,&quot;#import pandas\\nimport pandas as pd\\n\\n# define the data file name to be read\\ndata_file = \\&quot;/usr/local/share/datasets/NOAA_QCLCD_2011_hourly_13904.csv\\&quot;\\n\\n# read the data file\\ndf = pd.read_csv(data_file, index_col=False, header=None)\\n\\n# define the column labels for the real data file from NOAA: column_labels\\ncolumn_labels = \\&quot;Wban,date,Time,StationType,sky_condition,sky_conditionFlag,visibility,visibilityFlag,wx_and_obst_to_vision,wx_and_obst_to_visionFlag,dry_bulb_faren,dry_bulb_farenFlag,dry_bulb_cel,dry_bulb_celFlag,wet_bulb_faren,wet_bulb_farenFlag,wet_bulb_cel,wet_bulb_celFlag,dew_point_faren,dew_point_farenFlag,dew_point_cel,dew_point_celFlag,relative_humidity,relative_humidityFlag,wind_speed,wind_speedFlag,wind_direction,wind_directionFlag,value_for_wind_character,value_for_wind_characterFlag,station_pressure,station_pressureFlag,pressure_tendency,pressure_tendencyFlag,presschange,presschangeFlag,sea_level_pressure,sea_level_pressureFlag,record_type,hourly_precip,hourly_precipFlag,altimeter,altimeterFlag\\&quot;\\n\\n# define the sub-set list of columns to drop\\nlist_to_drop = [ &#39;sky_conditionFlag&#39;, &#39;visibilityFlag&#39;, &#39;wx_and_obst_to_vision&#39;, &#39;wx_and_obst_to_visionFlag&#39;,\\n  &#39;dry_bulb_farenFlag&#39;, &#39;dry_bulb_celFlag&#39;, &#39;wet_bulb_farenFlag&#39;, &#39;wet_bulb_celFlag&#39;, &#39;dew_point_farenFlag&#39;,\\n  &#39;dew_point_celFlag&#39;, &#39;relative_humidityFlag&#39;, &#39;wind_speedFlag&#39;, &#39;wind_directionFlag&#39;, &#39;value_for_wind_character&#39;,\\n  &#39;value_for_wind_characterFlag&#39;, &#39;station_pressureFlag&#39;, &#39;pressure_tendencyFlag&#39;, &#39;pressure_tendency&#39;,\\n  &#39;presschange&#39;,&#39;presschangeFlag&#39;, &#39;sea_level_pressureFlag&#39;, &#39;hourly_precip&#39;,  &#39;hourly_precipFlag&#39;, &#39;altimeter&#39;,  &#39;record_type&#39;, &#39;altimeterFlag&#39;, &#39;junk&#39;]\\n\\n# split on the comma to create a list, then append one more element as the data files all have a trailing comma at the end of each line: column_labels_list\\ncolumn_labels_list = column_labels.split(\\&quot;,\\&quot;)\\ncolumn_labels_list.append(\\&quot;junk\\&quot;)\\n\\n# assign the new colum labels to the dataframe\\ndf.columns = column_labels_list\\n\\n# Drop all the column data that we don&#39;t need\\ndf_dropped = df.drop(list_to_drop, axis=&#39;columns&#39;)&quot;,&quot;^X&quot;,&quot;# Convert the date column to string: df_dropped[&#39;date&#39;]\\ndf_dropped[&#39;date&#39;] = df_dropped[&#39;date&#39;].astype(str)\\n\\n# Pad leading zeros to the Time column: df_dropped[&#39;Time&#39;]\\ndf_dropped[&#39;Time&#39;] = df_dropped[&#39;Time&#39;].apply(lambda x:&#39;{:0&gt;4}&#39;.format(x))\\n\\n# Concatenate the new date and Time columns: date_string\\ndate_string = df_dropped[&#39;date&#39;] + df_dropped[&#39;Time&#39;]\\n\\n# Convert the date_string Series to datetime: date_times\\ndate_times = pd.to_datetime(date_string, format=&#39;%Y%m%d%H%M&#39;)\\n\\n# Set the index to be the new date_times container: df_clean\\ndf_clean = df_dropped.set_index(date_times)\\n\\n# Print the output of df_clean.head()\\nprint(df_clean.head())&quot;,&quot;^Y&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;First, you need to access the &lt;code&gt;&#39;date&#39;&lt;/code&gt; column of &lt;code&gt;df_dropped&lt;/code&gt; with &lt;code&gt;df_dropped[&#39;date&#39;]&lt;/code&gt;. Then, you should call the &lt;code&gt;.astype(str)&lt;/code&gt; method on this to convert it to a string.&lt;/li&gt;\\n&lt;li&gt;To concatenate the new &lt;code&gt;&#39;date&#39;&lt;/code&gt; and &lt;code&gt;&#39;Time&#39;&lt;/code&gt; columns, first access them with &lt;code&gt;df_dropped[&#39;date&#39;]&lt;/code&gt; and &lt;code&gt;df_dropped[&#39;Time&#39;]&lt;/code&gt;. You can then concatenate them by adding them together.&lt;/li&gt;\\n&lt;li&gt;Inside &lt;code&gt;pd.to_datetime()&lt;/code&gt;, pass in the concatenated &lt;code&gt;date_string&lt;/code&gt; Series and specify the &lt;code&gt;format&lt;/code&gt; parameter to convert it to datetime.&lt;/li&gt;\\n&lt;li&gt;Use the &lt;code&gt;.set_index()&lt;/code&gt; method to set the index of &lt;code&gt;df_dropped&lt;/code&gt; to be &lt;code&gt;date_times&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,100,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.2731044844596118,&quot;^18&quot;,47096],[&quot;^ &quot;,&quot;id&quot;,70132,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^T&quot;,&quot;&lt;p&gt;The numeric columns contain missing values labeled as &lt;code&gt;&#39;M&#39;&lt;/code&gt;. In this exercise, your job is to transform these columns such that they contain only numeric values and interpret missing data as &lt;code&gt;NaN&lt;/code&gt;.&lt;/p&gt;\\n&lt;p&gt;The pandas function &lt;code&gt;pd.to_numeric()&lt;/code&gt; is ideal for this purpose: It converts a Series of values to floating-point values. Furthermore, by specifying the keyword argument &lt;code&gt;errors=&#39;coerce&#39;&lt;/code&gt;, you can force strings like &lt;code&gt;&#39;M&#39;&lt;/code&gt; to be interpreted as &lt;code&gt;NaN&lt;/code&gt;. &lt;/p&gt;\\n&lt;p&gt;A DataFrame &lt;code&gt;df_clean&lt;/code&gt; is provided for you at the start of the exercise, and as usual, pandas has been imported as &lt;code&gt;pd&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Cleaning the numeric columns&quot;,&quot;^U&quot;,&quot;# Print the dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\\nprint(df_clean.loc[____:____, ____])\\n\\n# Convert the dry_bulb_faren column to numeric values: df_clean[&#39;dry_bulb_faren&#39;]\\ndf_clean[&#39;dry_bulb_faren&#39;] = ____(df_clean[&#39;dry_bulb_faren&#39;], ____=____)\\n\\n# Print the transformed dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\\nprint(df_clean.____[____:____, ____])\\n\\n# Convert the wind_speed and dew_point_faren columns to numeric values\\ndf_clean[&#39;wind_speed&#39;] = pd.to_numeric(____, ____=____)\\ndf_clean[&#39;dew_point_faren&#39;] = pd.to_numeric(____, ____=____)&quot;,&quot;^V&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Print the &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; temperature between 8 AM and 9 AM on June 20, 2011.&lt;/li&gt;\\n&lt;li&gt;Convert the &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; column to numeric values with &lt;code&gt;pd.to_numeric()&lt;/code&gt;. Specify &lt;code&gt;errors=&#39;coerce&#39;&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Print the transformed &lt;code&gt;dry_bulb_faren&lt;/code&gt; temperature between 8 AM and 9 AM on June 20, 2011.&lt;/li&gt;\\n&lt;li&gt;Convert the &lt;code&gt;&#39;wind_speed&#39;&lt;/code&gt; and &lt;code&gt;&#39;dew_point_faren&#39;&lt;/code&gt; columns to numeric values with &lt;code&gt;pd.to_numeric()&lt;/code&gt;. Again, specify &lt;code&gt;errors=&#39;coerce&#39;&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^N&quot;,6,&quot;sct&quot;,&quot;Ex().check_function(\\&quot;print\\&quot;, index=0)\\n\\nEx().check_correct(\\n  \\tcheck_df(\\&quot;df_clean\\&quot;).check_keys(\\&quot;dry_bulb_faren\\&quot;).has_equal_value(),\\n    has_equal_ast(\\&quot;Did you use `pd.to_numeric()` to convert the `&#39;dry_bulb_faren&#39;` column of `df_clean` to floating-point values, specifying `errors=&#39;coerce&#39;`?\\&quot;,\\n                  code=\\&quot;pd.to_numeric(df_clean[&#39;dry_bulb_faren&#39;], errors=&#39;coerce&#39;)\\&quot;,\\n                  exact=False))\\n\\nEx().check_function(\\&quot;print\\&quot;, index=1)\\n\\nEx().check_correct(\\n\\tcheck_df(\\&quot;df_clean\\&quot;).check_keys(\\&quot;wind_speed\\&quot;).has_equal_value(),\\n    has_equal_ast(\\&quot;Did you use `pd.to_numeric()` to convert the `&#39;wind_speed&#39;` column of `df` to floating-point values, specifying `errors=&#39;coerce&#39;`?\\&quot;,\\n                  code=\\&quot;pd.to_numeric(df_clean[&#39;wind_speed&#39;], errors=&#39;coerce&#39;)\\&quot;,\\n                  exact=False)\\n)\\n\\nEx().check_correct(\\n  \\tcheck_df(\\&quot;df_clean\\&quot;).check_keys(\\&quot;dew_point_faren\\&quot;).has_equal_value(),\\n    has_equal_ast(\\&quot;Did you use `pd.to_numeric()` to convert the `&#39;dew_point_faren&#39;` column of `df_clean` to floating-point values, specifying `errors=&#39;coerce&#39;`?\\&quot;,\\n                  code=\\&quot;pd.to_numeric(df_clean[&#39;dew_point_faren&#39;], errors=&#39;coerce&#39;)\\&quot;,\\n                  exact=False))\\n\\nsuccess_msg(\\&quot;Excellent job! Now that your data are clean, you can begin with your exploratory analysis.\\&quot;)&quot;,&quot;^W&quot;,&quot;import pandas as pd\\ndef read_data(file_name):\\n    # read the data file\\n    df = pd.read_csv(file_name, index_col=False, header=None)\\n\\n    # define the column labels for the real data file from NOAA: column_labels\\n    column_labels = \\&quot;Wban,date,Time,StationType,sky_condition,sky_conditionFlag,visibility,visibilityFlag,wx_and_obst_to_vision,wx_and_obst_to_visionFlag,dry_bulb_faren,dry_bulb_farenFlag,dry_bulb_cel,dry_bulb_celFlag,wet_bulb_faren,wet_bulb_farenFlag,wet_bulb_cel,wet_bulb_celFlag,dew_point_faren,dew_point_farenFlag,dew_point_cel,dew_point_celFlag,relative_humidity,relative_humidityFlag,wind_speed,wind_speedFlag,wind_direction,wind_directionFlag,value_for_wind_character,value_for_wind_characterFlag,station_pressure,station_pressureFlag,pressure_tendency,pressure_tendencyFlag,presschange,presschangeFlag,sea_level_pressure,sea_level_pressureFlag,record_type,hourly_precip,hourly_precipFlag,altimeter,altimeterFlag\\&quot;\\n    \\n    # define the sub-set list of columns to drop\\n    list_to_drop = [ &#39;sky_conditionFlag&#39;, &#39;visibilityFlag&#39;, &#39;wx_and_obst_to_vision&#39;, &#39;wx_and_obst_to_visionFlag&#39;,\\n      &#39;dry_bulb_farenFlag&#39;, &#39;dry_bulb_celFlag&#39;, &#39;wet_bulb_farenFlag&#39;, &#39;wet_bulb_celFlag&#39;, &#39;dew_point_farenFlag&#39;,\\n      &#39;dew_point_celFlag&#39;, &#39;relative_humidityFlag&#39;, &#39;wind_speedFlag&#39;, &#39;wind_directionFlag&#39;, &#39;value_for_wind_character&#39;,\\n      &#39;value_for_wind_characterFlag&#39;, &#39;station_pressureFlag&#39;, &#39;pressure_tendencyFlag&#39;, &#39;pressure_tendency&#39;,\\n      &#39;presschange&#39;,&#39;presschangeFlag&#39;, &#39;sea_level_pressureFlag&#39;, &#39;hourly_precip&#39;,  &#39;hourly_precipFlag&#39;, &#39;altimeter&#39;,  &#39;record_type&#39;, &#39;altimeterFlag&#39;, &#39;junk&#39;]\\n    \\n    # split on the comma to create a list, then append one more element as the data files all have a trailing comma at the end of each line: column_labels_list\\n    column_labels_list = column_labels.split(\\&quot;,\\&quot;)\\n    column_labels_list.append(\\&quot;junk\\&quot;)\\n    \\n    # assign the new colum labels to the dataframe\\n    df.columns = column_labels_list\\n    \\n    # Drop all the column data that we don&#39;t need\\n    df = df.drop(list_to_drop, axis=&#39;columns&#39;)\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with leading (left) zeros, for hours\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:0&gt;4}&#39;.format(x))\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with trailing (right) zeros, for seconds\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:&lt;06}&#39;.format(x))\\n    \\n    # Convert the `date` column to a string so it can be combined with the `Time` column\\n    df[&#39;date&#39;] = df[&#39;date&#39;].apply(str)\\n    \\n    # Create a new date-time container from the updated `date` and `Time` columns: date_times\\n    date_times = pd.to_datetime( df[&#39;date&#39;] + \\&quot; \\&quot; + df[&#39;Time&#39;], format=\\&quot;%Y%m%d %H%M%S\\&quot; )\\n    \\n    # Set the DataFrame index to this new `date_times` container:\\n    df.set_index(date_times, inplace=True)\\n\\n    return df\\ndata_file = \\&quot;/usr/local/share/datasets/NOAA_QCLCD_2011_hourly_13904.csv\\&quot;\\ndf_clean = read_data(data_file)&quot;,&quot;^X&quot;,&quot;# Print the dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\\nprint(df_clean.loc[&#39;2011-6-20 8:00:00&#39;:&#39;2011-6-20 9:00:00&#39;, &#39;dry_bulb_faren&#39;])\\n\\n# Convert the dry_bulb_faren column to numeric values: df_clean[&#39;dry_bulb_faren&#39;]\\ndf_clean[&#39;dry_bulb_faren&#39;] = pd.to_numeric(df_clean[&#39;dry_bulb_faren&#39;], errors=&#39;coerce&#39;)\\n\\n# Print the transformed dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\\nprint(df_clean.loc[&#39;2011-6-20 8:00:00&#39;:&#39;2011-6-20 9:00:00&#39;, &#39;dry_bulb_faren&#39;])\\n\\n# Convert the wind_speed and dew_point_faren columns to numeric values\\ndf_clean[&#39;wind_speed&#39;] = pd.to_numeric(df_clean[&#39;wind_speed&#39;], errors=&#39;coerce&#39;)\\ndf_clean[&#39;dew_point_faren&#39;] = pd.to_numeric(df_clean[&#39;dew_point_faren&#39;], errors=&#39;coerce&#39;)&quot;,&quot;^Y&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Inside &lt;code&gt;df_clean.loc[]&lt;/code&gt;, you need to first use bracket slicing to select the time range &lt;code&gt;&#39;2011-6-20 8:00:00&#39;:&#39;2011-6-20 9:00:00&#39;&lt;/code&gt;. Then, you have to specify the column, which in this case, is &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;You can use &lt;code&gt;pd.to_numeric()&lt;/code&gt; to convert the column to numeric values. In addition to specifying the column to convert, don&#39;t forget to include the keyword argument &lt;code&gt;errors=&#39;coerce&#39;&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Follow the same steps as above to print the transformed &lt;code&gt;dry_bulb_faren&lt;/code&gt;, and then convert the &lt;code&gt;&#39;wind_speed&#39;&lt;/code&gt; and &lt;code&gt;&#39;dew_point_faren&#39;&lt;/code&gt; to numeric values.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,100,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.32052274374606715,&quot;^18&quot;,70132],[&quot;^ &quot;,&quot;id&quot;,44659,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^T&quot;,null,&quot;^1&quot;,&quot;Statistical exploratory data analysis&quot;,&quot;^U&quot;,&quot;&quot;,&quot;^V&quot;,null,&quot;^N&quot;,7,&quot;sct&quot;,&quot;&quot;,&quot;^W&quot;,&quot;&quot;,&quot;^X&quot;,&quot;&quot;,&quot;^Y&quot;,null,&quot;^Z&quot;,null,&quot;xp&quot;,50,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^12&quot;,null,&quot;^13&quot;,&quot;//videos.datacamp.com/transcoded/1639_pandas-1/v2/hls-ch4_2.master.m3u8&quot;,&quot;^14&quot;,56.25,&quot;^15&quot;,&quot;course_1639_3fc6cb80f71423139e0fdd1111fa115a&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.4979193849354817,&quot;^18&quot;,44659],[&quot;^ &quot;,&quot;id&quot;,44660,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^T&quot;,&quot;&lt;p&gt;Now that you have the data read and cleaned, you can begin with statistical EDA. First, you will analyze the 2011 Austin weather data. &lt;/p&gt;\\n&lt;p&gt;Your job in this exercise is to analyze the &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; column and print the median temperatures for specific time ranges. You can do this using &lt;em&gt;partial datetime string&lt;/em&gt; selection.&lt;/p&gt;\\n&lt;p&gt;The cleaned dataframe is provided in the workspace as &lt;code&gt;df_clean&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Signal min, max, median&quot;,&quot;^U&quot;,&quot;# Print the median of the dry_bulb_faren column\\nprint(____)\\n\\n# Print the median of the dry_bulb_faren column for the time range &#39;2011-Apr&#39;:&#39;2011-Jun&#39;\\nprint(df_clean.loc[____:____, &#39;dry_bulb_faren&#39;].____)\\n\\n# Print the median of the dry_bulb_faren column for the month of January\\nprint(df_clean.____[____, ____].____)&quot;,&quot;^V&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Select the &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; column and print the output of &lt;code&gt;.median()&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Use &lt;code&gt;.loc[]&lt;/code&gt; to select the range &lt;code&gt;&#39;2011-Apr&#39;:&#39;2011-Jun&#39;&lt;/code&gt; from &lt;code&gt;dry_bulb_faren&#39;&lt;/code&gt; and print the output of &lt;code&gt;.median()&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Use &lt;code&gt;.loc[]&lt;/code&gt; to select the month &lt;code&gt;&#39;2011-Jan&#39;&lt;/code&gt; from &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; and print the output of &lt;code&gt;.median()&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^N&quot;,8,&quot;sct&quot;,&quot;Ex().check_function(\\&quot;print\\&quot;, index=0).check_args(0).has_equal_value(\\&quot;Did you apply the `.median()` method on `df_clean[&#39;dry_bulb_faren&#39;]` to compute its median?\\&quot;)\\n              \\nEx().check_function(\\&quot;print\\&quot;, index=1).check_args(0).has_equal_value(\\&quot;Did you apply the `.median()` method on the range `&#39;2011-Apr&#39;:&#39;2011-Jun&#39;` of the `&#39;dry_bulb_faren`&#39; column? \\&quot;)\\n\\nEx().check_function(\\&quot;print\\&quot;, index=2).check_args(0).has_equal_value(\\&quot;Did you select the month `&#39;2011-Jan&#39;` of `&#39;dry_bulb_faren&#39;` with `.loc()` and compute its median with `.median()`?\\&quot;)\\n              \\nsuccess_msg(\\&quot;Great work!\\&quot;)&quot;,&quot;^W&quot;,&quot;# pec\\nimport pandas as pd\\n\\ndef read_data(file_name):\\n    # read the data file\\n    df = pd.read_csv(file_name, index_col=False, header=None)\\n\\n    # define the column labels for the real data file from NOAA: column_labels\\n    column_labels = \\&quot;Wban,date,Time,StationType,sky_condition,sky_conditionFlag,visibility,visibilityFlag,wx_and_obst_to_vision,wx_and_obst_to_visionFlag,dry_bulb_faren,dry_bulb_farenFlag,dry_bulb_cel,dry_bulb_celFlag,wet_bulb_faren,wet_bulb_farenFlag,wet_bulb_cel,wet_bulb_celFlag,dew_point_faren,dew_point_farenFlag,dew_point_cel,dew_point_celFlag,relative_humidity,relative_humidityFlag,wind_speed,wind_speedFlag,wind_direction,wind_directionFlag,value_for_wind_character,value_for_wind_characterFlag,station_pressure,station_pressureFlag,pressure_tendency,pressure_tendencyFlag,presschange,presschangeFlag,sea_level_pressure,sea_level_pressureFlag,record_type,hourly_precip,hourly_precipFlag,altimeter,altimeterFlag\\&quot;\\n    \\n    # define the sub-set list of columns to drop\\n    list_to_drop = [ &#39;sky_conditionFlag&#39;, &#39;visibilityFlag&#39;, &#39;wx_and_obst_to_vision&#39;, &#39;wx_and_obst_to_visionFlag&#39;,\\n      &#39;dry_bulb_farenFlag&#39;, &#39;dry_bulb_celFlag&#39;, &#39;wet_bulb_farenFlag&#39;, &#39;wet_bulb_celFlag&#39;, &#39;dew_point_farenFlag&#39;,\\n      &#39;dew_point_celFlag&#39;, &#39;relative_humidityFlag&#39;, &#39;wind_speedFlag&#39;, &#39;wind_directionFlag&#39;, &#39;value_for_wind_character&#39;,\\n      &#39;value_for_wind_characterFlag&#39;, &#39;station_pressureFlag&#39;, &#39;pressure_tendencyFlag&#39;, &#39;pressure_tendency&#39;,\\n      &#39;presschange&#39;,&#39;presschangeFlag&#39;, &#39;sea_level_pressureFlag&#39;, &#39;hourly_precip&#39;,  &#39;hourly_precipFlag&#39;, &#39;altimeter&#39;,  &#39;record_type&#39;, &#39;altimeterFlag&#39;, &#39;junk&#39;]\\n    \\n    # split on the comma to create a list, then append one more element as the data files all have a trailing comma at the end of each line: column_labels_list\\n    column_labels_list = column_labels.split(\\&quot;,\\&quot;)\\n    column_labels_list.append(\\&quot;junk\\&quot;)\\n    \\n    # assign the new colum labels to the dataframe\\n    df.columns = column_labels_list\\n    \\n    # Drop all the column data that we don&#39;t need\\n    df = df.drop(list_to_drop, axis=&#39;columns&#39;)\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with leading (left) zeros, for hours\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:0&gt;4}&#39;.format(x))\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with trailing (right) zeros, for seconds\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:&lt;06}&#39;.format(x))\\n    \\n    # Convert the `date` column to a string so it can be combined with the `Time` column\\n    df[&#39;date&#39;] = df[&#39;date&#39;].apply(str)\\n    \\n    # Create a new date-time container from the updated `date` and `Time` columns: date_times\\n    date_times = pd.to_datetime( df[&#39;date&#39;] + \\&quot; \\&quot; + df[&#39;Time&#39;], format=\\&quot;%Y%m%d %H%M%S\\&quot; )\\n    \\n    # Set the DataFrame index to this new `date_times` container:\\n    df.set_index(date_times, inplace=True)\\n\\n    for col in [&#39;wind_speed&#39;,&#39;dry_bulb_faren&#39;,&#39;dew_point_faren&#39;]:\\n        df[col] = pd.to_numeric(df[col], errors=&#39;coerce&#39;)\\n\\n    return df\\ndata_file = \\&quot;/usr/local/share/datasets/NOAA_QCLCD_2011_hourly_13904.csv\\&quot;\\ndf_clean = read_data(data_file)\\n&quot;,&quot;^X&quot;,&quot;# Print the median of the dry_bulb_faren column\\nprint(df_clean[&#39;dry_bulb_faren&#39;].median())\\n\\n# Print the median of the dry_bulb_faren column for the time range &#39;2011-Apr&#39;:&#39;2011-Jun&#39;\\nprint(df_clean.loc[&#39;2011-Apr&#39;:&#39;2011-Jun&#39;, &#39;dry_bulb_faren&#39;].median())\\n\\n# Print the median of the dry_bulb_faren column for the month of January\\nprint(df_clean.loc[&#39;2011-Jan&#39;, &#39;dry_bulb_faren&#39;].median())&quot;,&quot;^Y&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Use bracket slicing on &lt;code&gt;df_clean&lt;/code&gt; to select the &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; column. Then, you can use &lt;code&gt;.median()&lt;/code&gt; to compute the median. Make sure that all of this is inside a &lt;code&gt;print()&lt;/code&gt; function.&lt;/li&gt;\\n&lt;li&gt;Use bracket slicing inside &lt;code&gt;df_clean.loc[]&lt;/code&gt; to select the range &lt;code&gt;&#39;2011-Apr&#39;:&#39;2011-Jun&#39;&lt;/code&gt; from &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt;. Then, use &lt;code&gt;.median()&lt;/code&gt; as before to compute the median.&lt;/li&gt;\\n&lt;li&gt;Follow the same approach as above, this time selecting &lt;code&gt;&#39;2011-Jan&#39;&lt;/code&gt; and &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; with &lt;code&gt;df_clean.loc[]&lt;/code&gt;, and then using &lt;code&gt;.median()&lt;/code&gt; to compute the median.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,100,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.25770543745189967,&quot;^18&quot;,44660],[&quot;^ &quot;,&quot;id&quot;,44661,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^T&quot;,&quot;&lt;p&gt;You&#39;re now ready to compare the 2011 weather data with the 30-year normals reported in 2010.\\nYou can ask questions such as, on average, how much hotter was every day in 2011 than expected from the 30-year average?&lt;/p&gt;\\n&lt;p&gt;The DataFrames &lt;code&gt;df_clean&lt;/code&gt; and &lt;code&gt;df_climate&lt;/code&gt; from previous exercises are available in the workspace.&lt;/p&gt;\\n&lt;p&gt;Your job is to first resample &lt;code&gt;df_clean&lt;/code&gt; and &lt;code&gt;df_climate&lt;/code&gt; by day and aggregate the mean temperatures. You will then extract the temperature related columns from each - &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; in &lt;code&gt;df_clean&lt;/code&gt;, and &lt;code&gt;&#39;Temperature&#39;&lt;/code&gt; in &lt;code&gt;df_climate&lt;/code&gt; - as NumPy arrays and compute the difference.&lt;/p&gt;\\n&lt;p&gt;Notice that the indexes of &lt;code&gt;df_clean&lt;/code&gt; and &lt;code&gt;df_climate&lt;/code&gt; are not aligned - &lt;code&gt;df_clean&lt;/code&gt; has dates in 2011, while &lt;code&gt;df_climate&lt;/code&gt; has dates in 2010. This is why you extract the temperature columns as NumPy arrays. An alternative approach is to use the pandas &lt;code&gt;.reset_index()&lt;/code&gt; method to make sure the Series align properly. You will practice this approach as well.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Signal variance&quot;,&quot;^U&quot;,&quot;# Downsample df_clean by day and aggregate by mean: daily_mean_2011\\ndaily_mean_2011 = ____\\n\\n# Extract the dry_bulb_faren column from daily_mean_2011 using .values: daily_temp_2011\\ndaily_temp_2011 = ____\\n\\n# Downsample df_climate by day and aggregate by mean: daily_climate\\ndaily_climate = ____\\n\\n# Extract the Temperature column from daily_climate using .reset_index(): daily_temp_climate\\ndaily_temp_climate = ____\\n\\n# Compute the difference between the two arrays and print the mean difference\\ndifference = daily_temp_2011 - daily_temp_climate\\nprint(difference.mean())&quot;,&quot;^V&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Downsample &lt;code&gt;df_clean&lt;/code&gt; with &lt;em&gt;daily&lt;/em&gt; frequency and aggregate by the mean. Store the result as &lt;code&gt;daily_mean_2011&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Extract the &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; column from &lt;code&gt;daily_mean_2011&lt;/code&gt; as a NumPy array using &lt;code&gt;.values&lt;/code&gt;. Store the result as &lt;code&gt;daily_temp_2011&lt;/code&gt;. Note: &lt;code&gt;.values&lt;/code&gt; is an attribute, not a method, so you don&#39;t have to use &lt;code&gt;()&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Downsample &lt;code&gt;df_climate&lt;/code&gt;  with &lt;em&gt;daily&lt;/em&gt; frequency and aggregate by the mean. Store the result as &lt;code&gt;daily_climate&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Extract the &lt;code&gt;&#39;Temperature&#39;&lt;/code&gt; column from &lt;code&gt;daily_climate&lt;/code&gt; using the &lt;code&gt;.reset_index()&lt;/code&gt; method. To do this, first reset the index of &lt;code&gt;daily_climate&lt;/code&gt;, and then use bracket slicing to access &lt;code&gt;&#39;Temperature&#39;&lt;/code&gt;. Store the result as &lt;code&gt;daily_temp_climate&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^N&quot;,9,&quot;sct&quot;,&quot;Ex().check_correct(\\n  \\tcheck_object(\\&quot;daily_mean_2011\\&quot;),\\n  \\thas_equal_ast(code = &#39;df_clean.resample(\\\\&#39;D\\\\&#39;).mean()&#39;, incorrect_msg = \\&quot;The contents of `&#39;daily_mean_2011&#39;` aren&#39;t correct. Did you correctly downsample `df_clean` to daily data and then call `.mean()` to calculate the mean?\\&quot;)\\n)\\n\\nEx().check_correct(\\n\\tcheck_object(\\&quot;daily_temp_2011\\&quot;).has_equal_value(),\\n  \\thas_equal_ast(code = &#39;daily_mean_2011[\\\\&#39;dry_bulb_faren\\\\&#39;].values&#39;, incorrect_msg = \\&quot;The contents of `&#39;daily_temp_2011&#39;` aren&#39;t correct. Did you use `.values` on the `&#39;dry_bulb_faren&#39;` column of `daily_mean_2011`?\\&quot;)\\n\\t\\n)\\n\\nEx().check_correct(\\n  \\tcheck_object(\\&quot;daily_climate\\&quot;),\\n  \\thas_equal_ast(code = &#39;df_climate.resample(\\\\&#39;D\\\\&#39;).mean()&#39;, incorrect_msg = \\&quot;The contents of `&#39;daily_climate&#39;` aren&#39;t correct. Did you correctly downsample `df_climate` to daily data and then call `.mean()` to calculate the mean?\\&quot;)\\n)\\n\\nEx().check_correct(\\n\\tcheck_object(\\&quot;daily_temp_climate\\&quot;).has_equal_value(),\\n  \\thas_equal_ast(code = &#39;daily_climate.reset_index()[\\\\&#39;Temperature\\\\&#39;]&#39;, incorrect_msg = &#39;Be sure you call `reset_index()` on `daily_climate` and then access the `\\\\&#39;Temperature\\\\&#39;` column using bracket indexing.&#39;)\\n)\\n\\nEx().check_correct(\\n\\tcheck_object(\\&quot;difference\\&quot;).has_equal_value(),\\n    has_equal_ast(code = &#39;daily_temp_2011 - daily_temp_climate&#39;, incorrect_msg = &#39;Did you subtract `daily_temp_climate` from `daily_temp_2011` to create `difference`?&#39;)\\n)\\n\\nEx().check_function(\\&quot;print\\&quot;).check_args(0).has_equal_value()\\n\\nsuccess_msg(\\&quot;Well done!\\&quot;)&quot;,&quot;^W&quot;,&quot;import pandas as pd\\n\\ndef read_data(file_name):\\n    # read the data file\\n    df = pd.read_csv(file_name, index_col=False, header=None)\\n\\n    # define the column labels for the real data file from NOAA: column_labels\\n    column_labels = \\&quot;Wban,date,Time,StationType,sky_condition,sky_conditionFlag,visibility,visibilityFlag,wx_and_obst_to_vision,wx_and_obst_to_visionFlag,dry_bulb_faren,dry_bulb_farenFlag,dry_bulb_cel,dry_bulb_celFlag,wet_bulb_faren,wet_bulb_farenFlag,wet_bulb_cel,wet_bulb_celFlag,dew_point_faren,dew_point_farenFlag,dew_point_cel,dew_point_celFlag,relative_humidity,relative_humidityFlag,wind_speed,wind_speedFlag,wind_direction,wind_directionFlag,value_for_wind_character,value_for_wind_characterFlag,station_pressure,station_pressureFlag,pressure_tendency,pressure_tendencyFlag,presschange,presschangeFlag,sea_level_pressure,sea_level_pressureFlag,record_type,hourly_precip,hourly_precipFlag,altimeter,altimeterFlag\\&quot;\\n    \\n    # define the sub-set list of columns to drop\\n    list_to_drop = [ &#39;sky_conditionFlag&#39;, &#39;visibilityFlag&#39;, &#39;wx_and_obst_to_vision&#39;, &#39;wx_and_obst_to_visionFlag&#39;,\\n      &#39;dry_bulb_farenFlag&#39;, &#39;dry_bulb_celFlag&#39;, &#39;wet_bulb_farenFlag&#39;, &#39;wet_bulb_celFlag&#39;, &#39;dew_point_farenFlag&#39;,\\n      &#39;dew_point_celFlag&#39;, &#39;relative_humidityFlag&#39;, &#39;wind_speedFlag&#39;, &#39;wind_directionFlag&#39;, &#39;value_for_wind_character&#39;,\\n      &#39;value_for_wind_characterFlag&#39;, &#39;station_pressureFlag&#39;, &#39;pressure_tendencyFlag&#39;, &#39;pressure_tendency&#39;,\\n      &#39;presschange&#39;,&#39;presschangeFlag&#39;, &#39;sea_level_pressureFlag&#39;, &#39;hourly_precip&#39;,  &#39;hourly_precipFlag&#39;, &#39;altimeter&#39;,  &#39;record_type&#39;, &#39;altimeterFlag&#39;, &#39;junk&#39;]\\n    \\n    # split on the comma to create a list, then append one more element as the data files all have a trailing comma at the end of each line: column_labels_list\\n    column_labels_list = column_labels.split(\\&quot;,\\&quot;)\\n    column_labels_list.append(\\&quot;junk\\&quot;)\\n    \\n    # assign the new colum labels to the dataframe\\n    df.columns = column_labels_list\\n    \\n    # Drop all the column data that we don&#39;t need\\n    df = df.drop(list_to_drop, axis=&#39;columns&#39;)\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with leading (left) zeros, for hours\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:0&gt;4}&#39;.format(x))\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with trailing (right) zeros, for seconds\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:&lt;06}&#39;.format(x))\\n    \\n    # Convert the `date` column to a string so it can be combined with the `Time` column\\n    df[&#39;date&#39;] = df[&#39;date&#39;].apply(str)\\n    \\n    # Create a new date-time container from the updated `date` and `Time` columns: date_times\\n    date_times = pd.to_datetime( df[&#39;date&#39;] + \\&quot; \\&quot; + df[&#39;Time&#39;], format=\\&quot;%Y%m%d %H%M%S\\&quot; )\\n    \\n    # Set the DataFrame index to this new `date_times` container:\\n    df.set_index(date_times, inplace=True)\\n\\n    for col in [&#39;wind_speed&#39;,&#39;dry_bulb_faren&#39;,&#39;dew_point_faren&#39;]:\\n        df[col] = pd.to_numeric(df[col], errors=&#39;coerce&#39;)\\n\\n    return df\\ndata_file = \\&quot;/usr/local/share/datasets/NOAA_QCLCD_2011_hourly_13904.csv\\&quot;\\ndf_clean = read_data(data_file)\\n\\nclimatology_data_file = \\&quot;/usr/local/share/datasets/weather_data_austin_2010.csv\\&quot;\\n\\ndf_climate = pd.read_csv(climatology_data_file, parse_dates=True, index_col=&#39;Date&#39;)&quot;,&quot;^X&quot;,&quot;# Downsample df_clean by day and aggregate by mean: daily_mean_2011\\ndaily_mean_2011 = df_clean.resample(&#39;D&#39;).mean()\\n\\n# Extract the dry_bulb_faren column from daily_mean_2011 using .values: daily_temp_2011\\ndaily_temp_2011 = daily_mean_2011[&#39;dry_bulb_faren&#39;].values\\n\\n# Downsample df_climate by day and aggregate by mean: daily_climate\\ndaily_climate = df_climate.resample(&#39;D&#39;).mean()\\n\\n# Extract the Temperature column from daily_climate using .reset_index(): daily_temp_climate\\ndaily_temp_climate = daily_climate.reset_index()[&#39;Temperature&#39;]\\n\\n# Compute the difference between the two arrays and print the mean difference\\ndifference = daily_temp_2011 - daily_temp_climate\\nprint(difference.mean())&quot;,&quot;^Y&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;You can downsample &lt;code&gt;df_clean&lt;/code&gt; with a daily frequency using &lt;code&gt;.resample()&lt;/code&gt; and specifying &lt;code&gt;&#39;D&#39;&lt;/code&gt;. To aggregate by the mean, chain &lt;code&gt;.mean()&lt;/code&gt; onto &lt;code&gt;.resample(&#39;D&#39;)&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;To extract the column as an array, first access it using bracket slicing. Then, access its &lt;code&gt;.values&lt;/code&gt; attribute.&lt;/li&gt;\\n&lt;li&gt;You can downsample &lt;code&gt;df_climate&lt;/code&gt; using the same approach as you did with &lt;code&gt;df_clean&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;First, you need to reset the index of &lt;code&gt;daily_climate&lt;/code&gt; using &lt;code&gt;.reset_index()&lt;/code&gt;. Then, access the &lt;code&gt;&#39;Temperature&#39;&lt;/code&gt; column using bracket slicing.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,100,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.5318762346619463,&quot;^18&quot;,44661],[&quot;^ &quot;,&quot;id&quot;,59225,&quot;^&gt;&quot;,&quot;TabExercise&quot;,&quot;^T&quot;,&quot;&lt;p&gt;On average, how much hotter is it when the sun is shining? In this exercise, you will compare temperatures on sunny days against temperatures on overcast days.&lt;/p&gt;\\n&lt;p&gt;Your job is to use Boolean selection to filter out sunny and overcast days, and then compute the difference of the mean daily maximum temperatures between each type of day.&lt;/p&gt;\\n&lt;p&gt;The DataFrame &lt;code&gt;df_clean&lt;/code&gt; from previous exercises has been provided for you. The column &lt;code&gt;&#39;sky_condition&#39;&lt;/code&gt; provides information about whether the day was sunny (&lt;code&gt;&#39;CLR&#39;&lt;/code&gt;) or overcast (&lt;code&gt;&#39;OVC&#39;&lt;/code&gt;).&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Sunny or cloudy&quot;,&quot;^U&quot;,&quot;&quot;,&quot;^V&quot;,null,&quot;^N&quot;,10,&quot;sct&quot;,&quot;&quot;,&quot;^W&quot;,&quot;import pandas as pd\\n\\ndef read_data(file_name):\\n    # read the data file\\n    df = pd.read_csv(file_name, index_col=False, header=None)\\n\\n    # define the column labels for the real data file from NOAA: column_labels\\n    column_labels = \\&quot;Wban,date,Time,StationType,sky_condition,sky_conditionFlag,visibility,visibilityFlag,wx_and_obst_to_vision,wx_and_obst_to_visionFlag,dry_bulb_faren,dry_bulb_farenFlag,dry_bulb_cel,dry_bulb_celFlag,wet_bulb_faren,wet_bulb_farenFlag,wet_bulb_cel,wet_bulb_celFlag,dew_point_faren,dew_point_farenFlag,dew_point_cel,dew_point_celFlag,relative_humidity,relative_humidityFlag,wind_speed,wind_speedFlag,wind_direction,wind_directionFlag,value_for_wind_character,value_for_wind_characterFlag,station_pressure,station_pressureFlag,pressure_tendency,pressure_tendencyFlag,presschange,presschangeFlag,sea_level_pressure,sea_level_pressureFlag,record_type,hourly_precip,hourly_precipFlag,altimeter,altimeterFlag\\&quot;\\n    \\n    # define the sub-set list of columns to drop\\n    list_to_drop = [ &#39;sky_conditionFlag&#39;, &#39;visibilityFlag&#39;, &#39;wx_and_obst_to_vision&#39;, &#39;wx_and_obst_to_visionFlag&#39;,\\n      &#39;dry_bulb_farenFlag&#39;, &#39;dry_bulb_celFlag&#39;, &#39;wet_bulb_farenFlag&#39;, &#39;wet_bulb_celFlag&#39;, &#39;dew_point_farenFlag&#39;,\\n      &#39;dew_point_celFlag&#39;, &#39;relative_humidityFlag&#39;, &#39;wind_speedFlag&#39;, &#39;wind_directionFlag&#39;, &#39;value_for_wind_character&#39;,\\n      &#39;value_for_wind_characterFlag&#39;, &#39;station_pressureFlag&#39;, &#39;pressure_tendencyFlag&#39;, &#39;pressure_tendency&#39;,\\n      &#39;presschange&#39;,&#39;presschangeFlag&#39;, &#39;sea_level_pressureFlag&#39;, &#39;hourly_precip&#39;,  &#39;hourly_precipFlag&#39;, &#39;altimeter&#39;,  &#39;record_type&#39;, &#39;altimeterFlag&#39;, &#39;junk&#39;]\\n    \\n    # split on the comma to create a list, then append one more element as the data files all have a trailing comma at the end of each line: column_labels_list\\n    column_labels_list = column_labels.split(\\&quot;,\\&quot;)\\n    column_labels_list.append(\\&quot;junk\\&quot;)\\n    \\n    # assign the new colum labels to the dataframe\\n    df.columns = column_labels_list\\n    \\n    # Drop all the column data that we don&#39;t need\\n    df = df.drop(list_to_drop, axis=&#39;columns&#39;)\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with leading (left) zeros, for hours\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:0&gt;4}&#39;.format(x))\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with trailing (right) zeros, for seconds\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:&lt;06}&#39;.format(x))\\n    \\n    # Convert the `date` column to a string so it can be combined with the `Time` column\\n    df[&#39;date&#39;] = df[&#39;date&#39;].apply(str)\\n    \\n    # Create a new date-time container from the updated `date` and `Time` columns: date_times\\n    date_times = pd.to_datetime( df[&#39;date&#39;] + \\&quot; \\&quot; + df[&#39;Time&#39;], format=\\&quot;%Y%m%d %H%M%S\\&quot; )\\n    \\n    # Set the DataFrame index to this new `date_times` container:\\n    df.set_index(date_times, inplace=True)\\n\\n    for col in [&#39;wind_speed&#39;,&#39;dry_bulb_faren&#39;,&#39;dew_point_faren&#39;]:\\n        df[col] = pd.to_numeric(df[col], errors=&#39;coerce&#39;)\\n\\n    return df\\ndata_file = \\&quot;/usr/local/share/datasets/NOAA_QCLCD_2011_hourly_13904.csv\\&quot;\\ndf_clean = read_data(data_file)&quot;,&quot;^X&quot;,&quot;&quot;,&quot;^Y&quot;,null,&quot;^Z&quot;,null,&quot;xp&quot;,100,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;subexercises&quot;,[[&quot;^ &quot;,&quot;id&quot;,624210,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^T&quot;,null,&quot;^1&quot;,null,&quot;^U&quot;,&quot;# Using df_clean, when is sky_condition &#39;CLR&#39;?\\nis_sky_clear = ____[&#39;____&#39;]==&#39;____&#39;\\n\\n# Filter df_clean using is_sky_clear\\nsunny = ____[____]\\n\\n# Resample sunny by day then calculate the max\\nsunny_daily_max = ____.____(&#39;____&#39;).____()\\n\\n# See the result\\nsunny_daily_max.head()&quot;,&quot;^V&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Get the cases in &lt;code&gt;df_clean&lt;/code&gt; where the sky is clear. That is, when &lt;code&gt;&#39;sky_condition&#39;&lt;/code&gt; equals &lt;code&gt;&#39;CLR&#39;&lt;/code&gt;, assigning to &lt;code&gt;is_sky_clear&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Use &lt;code&gt;.loc[]&lt;/code&gt; to filter &lt;code&gt;df_clean&lt;/code&gt; by &lt;code&gt;is_sky_clear&lt;/code&gt;, assigning to &lt;code&gt;sunny&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Resample &lt;code&gt;sunny&lt;/code&gt; by day (&lt;code&gt;&#39;D&#39;&lt;/code&gt;), and take the max to find the maximum daily temperature.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^N&quot;,1,&quot;sct&quot;,&quot;Ex().check_correct(\\n    check_object(&#39;is_sky_clear&#39;).has_equal_value(),\\n    multi(\\n        has_code(\\&quot;df_clean\\\\[ *&#39;sky_condition&#39; *\\\\]\\&quot;, not_typed_msg = \\&quot;Did you select the `sky_condition` column with `df_clean[&#39;sky_condition&#39;]`?\\&quot;),\\n        has_code(\\&quot;\\\\] *== *&#39;CLR&#39;\\&quot;, not_typed_msg = \\&quot;Did you check for `sky_condition` equalling `&#39;CLR&#39;`?\\&quot;)\\n    )\\n)\\nEx().check_correct(\\n    check_object(&#39;sunny&#39;).has_equal_value(),\\n    has_equal_ast(code = &#39;df_clean.loc[is_sky_clear]&#39;, incorrect_msg = &#39;Did you filter `df_clean` using `.loc[]` and `is_sky_clear`?&#39;)\\n)\\nEx().check_correct(\\n    check_object(&#39;sunny_daily_max&#39;).has_equal_value(),\\n    multi(\\n        check_function(&#39;sunny.resample&#39;).check_args(0).has_equal_value(),\\n        check_function(&#39;sunny.resample.max&#39;, signature=False)\\n    )\\n)&quot;,&quot;^W&quot;,&quot;&quot;,&quot;^X&quot;,&quot;# Using df_clean, when is sky_condition &#39;CLR&#39;?\\nis_sky_clear = df_clean[&#39;sky_condition&#39;]==&#39;CLR&#39;\\n\\n# Filter df_clean using is_sky_clear\\nsunny = df_clean.loc[is_sky_clear]\\n\\n# Resample sunny by day then calculate the max\\nsunny_daily_max = sunny.resample(&#39;D&#39;).max()\\n\\n# See the result\\nsunny_daily_max.head()&quot;,&quot;^Y&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;The logical condition takes the form &lt;code&gt;dataset[&#39;column&#39;]==&#39;value&#39;&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;The filtering command takes the form &lt;code&gt;dataset[filter_condition]&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Use &lt;code&gt;resample()&lt;/code&gt;, passing &lt;code&gt;&#39;D&#39;&lt;/code&gt; for daily, then take the &lt;code&gt;max()&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,35,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;],[&quot;^ &quot;,&quot;id&quot;,624211,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^T&quot;,null,&quot;^1&quot;,null,&quot;^U&quot;,&quot;# Using df_clean, when does sky_condition contain &#39;OVC&#39;?\\nis_sky_overcast = ____[&#39;____&#39;].____.____(&#39;____&#39;)\\n\\n# Filter df_clean using is_sky_overcast\\novercast = ____\\n\\n# Resample overcast by day then calculate the max\\novercast_daily_max = ____\\n\\n# See the result\\novercast_daily_max.head()&quot;,&quot;^V&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Get the cases in &lt;code&gt;df_clean&lt;/code&gt; where the sky is overcast. Using &lt;code&gt;.str.contains()&lt;/code&gt;, find when &lt;code&gt;&#39;sky_condition&#39;&lt;/code&gt; contains &lt;code&gt;&#39;OVC&#39;&lt;/code&gt;, assigning to &lt;code&gt;is_sky_overcast&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Use &lt;code&gt;.loc[]&lt;/code&gt; to filter &lt;code&gt;df_clean&lt;/code&gt; by &lt;code&gt;is_sky_overcast&lt;/code&gt;, assigning to &lt;code&gt;overcast&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Resample &lt;code&gt;overcast&lt;/code&gt; by day (&lt;code&gt;&#39;D&#39;&lt;/code&gt;), and take the max to find the maximum daily temperature.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^N&quot;,2,&quot;sct&quot;,&quot;Ex().check_correct(\\n    check_object(&#39;is_sky_overcast&#39;).has_equal_value(),\\n    multi(\\n        has_code(\\&quot;df_clean\\\\[ *&#39;sky_condition&#39; *\\\\]\\&quot;, not_typed_msg = \\&quot;Did you select the `sky_condition` column with `df_clean[&#39;sky_condition&#39;]`?\\&quot;),\\n        has_code(\\&quot;\\\\] *== *&#39;OVC&#39;\\&quot;, not_typed_msg = \\&quot;Did you check for `sky_condition` equalling `&#39;OVC&#39;`?\\&quot;)\\n    )\\n)\\nEx().check_correct(\\n    check_object(&#39;overcast&#39;).has_equal_value(),\\n    has_equal_ast(code = &#39;df_clean.loc[is_sky_overcast]&#39;, incorrect_msg = &#39;Did you filter `df_clean` using `.loc[]` and `is_sky_overcast`?&#39;)\\n)\\nEx().check_correct(\\n    check_object(&#39;overcast_daily_max&#39;).has_equal_value(),\\n    multi(\\n        check_function(&#39;overcast.resample&#39;).check_args(0).has_equal_value(),\\n        check_function(&#39;overcast.resample.max&#39;, signature=False)\\n    )\\n)&quot;,&quot;^W&quot;,&quot;&quot;,&quot;^X&quot;,&quot;# Using df_clean, when does sky_condition contain &#39;OVC&#39;?\\nis_sky_overcast = df_clean[&#39;sky_condition&#39;].str.contains(&#39;OVC&#39;)\\n\\n# Filter df_clean using is_sky_overcast\\novercast = df_clean.loc[is_sky_overcast]\\n\\n# Resample overcast by day then calculate the max\\novercast_daily_max = overcast.resample(&#39;D&#39;).max()\\n\\n# See the result\\novercast_daily_max.head()&quot;,&quot;^Y&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;The logical condition takes the form &lt;code&gt;dataset[&#39;column&#39;].str.contains(&#39;value&#39;)&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;The filtering command takes the form &lt;code&gt;dataset[filter_condition]&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Use &lt;code&gt;resample()&lt;/code&gt;, passing &lt;code&gt;&#39;D&#39;&lt;/code&gt; for daily, then take the &lt;code&gt;max()&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,35,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;],[&quot;^ &quot;,&quot;id&quot;,624212,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^T&quot;,null,&quot;^1&quot;,null,&quot;^U&quot;,&quot;# From previous steps\\nis_sky_clear = df_clean[&#39;sky_condition&#39;]==&#39;CLR&#39;\\nsunny = df_clean.loc[is_sky_clear]\\nsunny_daily_max = sunny.resample(&#39;D&#39;).max()\\nis_sky_overcast = df_clean[&#39;sky_condition&#39;].str.contains(&#39;OVC&#39;)\\novercast = df_clean.loc[is_sky_overcast]\\novercast_daily_max = overcast.resample(&#39;D&#39;).max()\\n\\n# Calculate the mean of sunny_daily_max\\nsunny_daily_max_mean = ____\\n\\n# Calculate the mean of overcast_daily_max\\novercast_daily_max_mean = ____\\n\\n# Print the difference (sunny minus overcast)\\n____&quot;,&quot;^V&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Calculate the mean of &lt;code&gt;sunny_daily_max&lt;/code&gt;, assigning to &lt;code&gt;sunny_daily_max_mean&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Calculate the mean of &lt;code&gt;overcast_daily_max&lt;/code&gt;, assigning to &lt;code&gt;overcast_daily_max_mean&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Print &lt;code&gt;sunny_daily_max_mean&lt;/code&gt; minus &lt;code&gt;overcast_daily_max_mean&lt;/code&gt;. &lt;em&gt;How much hotter are sunny days?&lt;/em&gt;&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^N&quot;,3,&quot;sct&quot;,&quot;Ex().check_correct(\\n    check_object(&#39;sunny_daily_max_mean&#39;).has_equal_value(),\\n    check_function(&#39;sunny_daily_max.mean&#39;, signature=False)\\n)\\nEx().check_correct(\\n    check_object(&#39;overcast_daily_max_mean&#39;).has_equal_value(),\\n    check_function(&#39;overcast_daily_max.mean&#39;, signature=False)\\n)\\nEx().has_printout(0, not_printed_msg=\\&quot;Did you print `sunny_daily_max_mean` minus `overcast_daily_max_mean`?\\&quot;)\\nsuccess_msg(\\&quot;Terrific temperature computing! The average daily maximum dry bulb temperature was 6.5 degrees Fahrenheit higher on sunny days compared to overcast days.\\&quot;)&quot;,&quot;^W&quot;,&quot;&quot;,&quot;^X&quot;,&quot;# From previous steps\\nis_sky_clear = df_clean[&#39;sky_condition&#39;]==&#39;CLR&#39;\\nsunny = df_clean.loc[is_sky_clear]\\nsunny_daily_max = sunny.resample(&#39;D&#39;).max()\\nis_sky_overcast = df_clean[&#39;sky_condition&#39;].str.contains(&#39;OVC&#39;)\\novercast = df_clean.loc[is_sky_overcast]\\novercast_daily_max = overcast.resample(&#39;D&#39;).max()\\n\\n# Calculate the mean of sunny_daily_max\\nsunny_daily_max_mean = sunny_daily_max.mean()\\n\\n# Calculate the mean of overcast_daily_max\\novercast_daily_max_mean = overcast_daily_max.mean()\\n\\n# Print the difference (sunny minus overcast)\\nprint(sunny_daily_max_mean - overcast_daily_max_mean)&quot;,&quot;^Y&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Use &lt;code&gt;.mean()&lt;/code&gt; to calculate the mean of a value.&lt;/li&gt;\\n&lt;li&gt;The print command takes the form &lt;code&gt;print(x - y)&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,30,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;]],&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.25104653643763086,&quot;^18&quot;,59225],[&quot;^ &quot;,&quot;id&quot;,44663,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^T&quot;,null,&quot;^1&quot;,&quot;Visual exploratory data analysis&quot;,&quot;^U&quot;,&quot;&quot;,&quot;^V&quot;,null,&quot;^N&quot;,11,&quot;sct&quot;,&quot;&quot;,&quot;^W&quot;,&quot;&quot;,&quot;^X&quot;,&quot;&quot;,&quot;^Y&quot;,null,&quot;^Z&quot;,null,&quot;xp&quot;,50,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^12&quot;,null,&quot;^13&quot;,&quot;//videos.datacamp.com/transcoded/1639_pandas-1/v2/hls-ch4_3.master.m3u8&quot;,&quot;^14&quot;,56.25,&quot;^15&quot;,&quot;course_1639_9f7496c6bcaa543b2cfa9d8c4410aff2&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.885095036502584,&quot;^18&quot;,44663],[&quot;^ &quot;,&quot;id&quot;,44664,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^T&quot;,&quot;&lt;p&gt;Is there a correlation between temperature and visibility? Let&#39;s find out. &lt;/p&gt;\\n&lt;p&gt;In this exercise, your job is to plot the weekly average temperature and visibility as subplots. To do this, you need to first select the appropriate columns and then resample by week, aggregating the mean. &lt;/p&gt;\\n&lt;p&gt;In addition to creating the subplots, you will compute the Pearson correlation coefficient using &lt;code&gt;.corr()&lt;/code&gt;. The Pearson correlation coefficient, known also as Pearson&#39;s r, ranges from -1 (indicating total negative linear correlation) to 1 (indicating total positive linear correlation). A value close to 1 here would indicate that there is a strong correlation between temperature and visibility.&lt;/p&gt;\\n&lt;p&gt;The DataFrame &lt;code&gt;df_clean&lt;/code&gt; has been pre-loaded for you.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Weekly average temperature and visibility&quot;,&quot;^U&quot;,&quot;# Import matplotlib.pyplot as plt\\n\\n\\n# Select the visibility and dry_bulb_faren columns and resample them: weekly_mean\\nweekly_mean = ____\\n\\n# Print the output of weekly_mean.corr()\\nprint(____)\\n\\n# Plot weekly_mean with subplots=True\\nweekly_mean.____(____=____)\\nplt.show()&quot;,&quot;^V&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Import &lt;code&gt;matplotlib.pyplot&lt;/code&gt; as &lt;code&gt;plt&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Select the &lt;code&gt;&#39;visibility&#39;&lt;/code&gt; and &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; columns and resample them by week, aggregating the mean. Assign the result to &lt;code&gt;weekly_mean&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Print the output of &lt;code&gt;weekly_mean.corr()&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Plot the &lt;code&gt;weekly_mean&lt;/code&gt; dataframe with &lt;code&gt;.plot()&lt;/code&gt;, specifying &lt;code&gt;subplots=True&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^N&quot;,12,&quot;sct&quot;,&quot;Ex().has_import(\\&quot;matplotlib.pyplot\\&quot;)\\n\\nEx().check_correct(\\n  \\tcheck_object(\\&quot;weekly_mean\\&quot;).has_equal_value(),\\n    has_equal_ast(\\&quot;It seems like you did not resample the data correctly. Did you select the `&#39;visibility&#39;` and `&#39;dry_bulb_faren&#39;` columns, then use `.resample(&#39;W&#39;)` along with `.mean()`?\\&quot;,\\n                                code=\\&quot;df_clean[[&#39;visibility&#39;,&#39;dry_bulb_faren&#39;]].resample(&#39;W&#39;).mean()\\&quot;,\\n                                exact=False))\\n\\nEx().check_function(\\&quot;print\\&quot;).check_args(0).has_equal_value()\\n\\nEx().check_function(\\&quot;weekly_mean.plot\\&quot;)\\nEx().check_function(\\&quot;matplotlib.pyplot.show\\&quot;)\\n\\nsuccess_msg(\\&quot;Great work!\\&quot;)&quot;,&quot;^W&quot;,&quot;# pec\\nimport pandas as pd\\n\\ndef read_data(file_name):\\n    # read the data file\\n    df = pd.read_csv(file_name, index_col=False, header=None)\\n\\n    # define the column labels for the real data file from NOAA: column_labels\\n    column_labels = \\&quot;Wban,date,Time,StationType,sky_condition,sky_conditionFlag,visibility,visibilityFlag,wx_and_obst_to_vision,wx_and_obst_to_visionFlag,dry_bulb_faren,dry_bulb_farenFlag,dry_bulb_cel,dry_bulb_celFlag,wet_bulb_faren,wet_bulb_farenFlag,wet_bulb_cel,wet_bulb_celFlag,dew_point_faren,dew_point_farenFlag,dew_point_cel,dew_point_celFlag,relative_humidity,relative_humidityFlag,wind_speed,wind_speedFlag,wind_direction,wind_directionFlag,value_for_wind_character,value_for_wind_characterFlag,station_pressure,station_pressureFlag,pressure_tendency,pressure_tendencyFlag,presschange,presschangeFlag,sea_level_pressure,sea_level_pressureFlag,record_type,hourly_precip,hourly_precipFlag,altimeter,altimeterFlag\\&quot;\\n    \\n    # define the sub-set list of columns to drop\\n    list_to_drop = [ &#39;sky_conditionFlag&#39;, &#39;visibilityFlag&#39;, &#39;wx_and_obst_to_vision&#39;, &#39;wx_and_obst_to_visionFlag&#39;,\\n      &#39;dry_bulb_farenFlag&#39;, &#39;dry_bulb_celFlag&#39;, &#39;wet_bulb_farenFlag&#39;, &#39;wet_bulb_celFlag&#39;, &#39;dew_point_farenFlag&#39;,\\n      &#39;dew_point_celFlag&#39;, &#39;relative_humidityFlag&#39;, &#39;wind_speedFlag&#39;, &#39;wind_directionFlag&#39;, &#39;value_for_wind_character&#39;,\\n      &#39;value_for_wind_characterFlag&#39;, &#39;station_pressureFlag&#39;, &#39;pressure_tendencyFlag&#39;, &#39;pressure_tendency&#39;,\\n      &#39;presschange&#39;,&#39;presschangeFlag&#39;, &#39;sea_level_pressureFlag&#39;, &#39;hourly_precip&#39;,  &#39;hourly_precipFlag&#39;, &#39;altimeter&#39;,  &#39;record_type&#39;, &#39;altimeterFlag&#39;, &#39;junk&#39;]\\n    \\n    # split on the comma to create a list, then append one more element as the data files all have a trailing comma at the end of each line: column_labels_list\\n    column_labels_list = column_labels.split(\\&quot;,\\&quot;)\\n    column_labels_list.append(\\&quot;junk\\&quot;)\\n    \\n    # assign the new colum labels to the dataframe\\n    df.columns = column_labels_list\\n    \\n    # Drop all the column data that we don&#39;t need\\n    df = df.drop(list_to_drop, axis=&#39;columns&#39;)\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with leading (left) zeros, for hours\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:0&gt;4}&#39;.format(x))\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with trailing (right) zeros, for seconds\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:&lt;06}&#39;.format(x))\\n    \\n    # Convert the `date` column to a string so it can be combined with the `Time` column\\n    df[&#39;date&#39;] = df[&#39;date&#39;].apply(str)\\n    \\n    # Create a new date-time container from the updated `date` and `Time` columns: date_times\\n    date_times = pd.to_datetime( df[&#39;date&#39;] + \\&quot; \\&quot; + df[&#39;Time&#39;], format=\\&quot;%Y%m%d %H%M%S\\&quot; )\\n    \\n    # Set the DataFrame index to this new `date_times` container:\\n    df.set_index(date_times, inplace=True)\\n\\n    for col in [&#39;wind_speed&#39;,&#39;dry_bulb_faren&#39;,&#39;dew_point_faren&#39;]:\\n        df[col] = pd.to_numeric(df[col], errors=&#39;coerce&#39;)\\n\\n    return df\\ndata_file = \\&quot;/usr/local/share/datasets/NOAA_QCLCD_2011_hourly_13904.csv\\&quot;\\ndf_clean = read_data(data_file)\\ndf_clean[&#39;visibility&#39;] = pd.to_numeric(df_clean[&#39;visibility&#39;], errors=&#39;coerce&#39;)&quot;,&quot;^X&quot;,&quot;# Import matplotlib.pyplot as plt\\nimport matplotlib.pyplot as plt\\n\\n# Select the visibility and dry_bulb_faren columns and resample them: weekly_mean\\nweekly_mean = df_clean[[&#39;visibility&#39;,&#39;dry_bulb_faren&#39;]].resample(&#39;W&#39;).mean()\\n\\n# Print the output of weekly_mean.corr()\\nprint(weekly_mean.corr())\\n\\n# Plot weekly_mean with subplots=True\\nweekly_mean.plot(subplots=True)\\nplt.show()&quot;,&quot;^Y&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;You can import x from y using the command &lt;code&gt;from y import x&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;To select &lt;code&gt;&#39;visibility&#39;&lt;/code&gt; and &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt;, use bracket slicing and pass them in as a list inside &lt;code&gt;df_clean[]&lt;/code&gt;. Then, use &lt;code&gt;.resample()&lt;/code&gt; with &lt;code&gt;&#39;W&#39;&lt;/code&gt; to resample by week, and chain on the &lt;code&gt;.mean()&lt;/code&gt; method. &lt;/li&gt;\\n&lt;li&gt;Compute the correlation of &lt;code&gt;weekly_mean&lt;/code&gt; with &lt;code&gt;.corr()&lt;/code&gt;, and print it by passing it to the provided &lt;code&gt;print()&lt;/code&gt; function.&lt;/li&gt;\\n&lt;li&gt;Use the &lt;code&gt;.plot()&lt;/code&gt; with the keyword argument &lt;code&gt;subplots=True&lt;/code&gt; to plot the weekly average temperature and visibility as subplots.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,100,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.7042982793996471,&quot;^18&quot;,44664],[&quot;^ &quot;,&quot;id&quot;,44665,&quot;^&gt;&quot;,&quot;TabExercise&quot;,&quot;^T&quot;,&quot;&lt;p&gt;In a previous exercise, you analyzed the &lt;code&gt;&#39;sky_condition&#39;&lt;/code&gt; column to explore the difference in temperature on sunny days compared to overcast days. Recall that a &lt;code&gt;&#39;sky_condition&#39;&lt;/code&gt; of &lt;code&gt;&#39;CLR&#39;&lt;/code&gt; represents a sunny day. In this exercise, you will explore sunny days in greater detail. Specifically, you will use a box plot to visualize the fraction of days that are sunny.&lt;/p&gt;\\n&lt;p&gt;The &lt;code&gt;&#39;sky_condition&#39;&lt;/code&gt; column is recorded hourly. Your job is to resample this column appropriately such that you can extract the number of sunny hours in a day and the number of total hours. Then, you can divide the number of sunny hours by the number of total hours, and generate a box plot of the resulting fraction.&lt;/p&gt;\\n&lt;p&gt;As before, &lt;code&gt;df_clean&lt;/code&gt; is available for you in the workspace.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Daily hours of clear sky&quot;,&quot;^U&quot;,&quot;&quot;,&quot;^V&quot;,null,&quot;^N&quot;,13,&quot;sct&quot;,&quot;&quot;,&quot;^W&quot;,&quot;import pandas as pd\\n\\ndef read_data(file_name):\\n    # read the data file\\n    df = pd.read_csv(file_name, index_col=False, header=None)\\n\\n    # define the column labels for the real data file from NOAA: column_labels\\n    column_labels = \\&quot;Wban,date,Time,StationType,sky_condition,sky_conditionFlag,visibility,visibilityFlag,wx_and_obst_to_vision,wx_and_obst_to_visionFlag,dry_bulb_faren,dry_bulb_farenFlag,dry_bulb_cel,dry_bulb_celFlag,wet_bulb_faren,wet_bulb_farenFlag,wet_bulb_cel,wet_bulb_celFlag,dew_point_faren,dew_point_farenFlag,dew_point_cel,dew_point_celFlag,relative_humidity,relative_humidityFlag,wind_speed,wind_speedFlag,wind_direction,wind_directionFlag,value_for_wind_character,value_for_wind_characterFlag,station_pressure,station_pressureFlag,pressure_tendency,pressure_tendencyFlag,presschange,presschangeFlag,sea_level_pressure,sea_level_pressureFlag,record_type,hourly_precip,hourly_precipFlag,altimeter,altimeterFlag\\&quot;\\n    \\n    # define the sub-set list of columns to drop\\n    list_to_drop = [ &#39;sky_conditionFlag&#39;, &#39;visibilityFlag&#39;, &#39;wx_and_obst_to_vision&#39;, &#39;wx_and_obst_to_visionFlag&#39;,\\n      &#39;dry_bulb_farenFlag&#39;, &#39;dry_bulb_celFlag&#39;, &#39;wet_bulb_farenFlag&#39;, &#39;wet_bulb_celFlag&#39;, &#39;dew_point_farenFlag&#39;,\\n      &#39;dew_point_celFlag&#39;, &#39;relative_humidityFlag&#39;, &#39;wind_speedFlag&#39;, &#39;wind_directionFlag&#39;, &#39;value_for_wind_character&#39;,\\n      &#39;value_for_wind_characterFlag&#39;, &#39;station_pressureFlag&#39;, &#39;pressure_tendencyFlag&#39;, &#39;pressure_tendency&#39;,\\n      &#39;presschange&#39;,&#39;presschangeFlag&#39;, &#39;sea_level_pressureFlag&#39;, &#39;hourly_precip&#39;,  &#39;hourly_precipFlag&#39;, &#39;altimeter&#39;,  &#39;record_type&#39;, &#39;altimeterFlag&#39;, &#39;junk&#39;]\\n    \\n    # split on the comma to create a list, then append one more element as the data files all have a trailing comma at the end of each line: column_labels_list\\n    column_labels_list = column_labels.split(\\&quot;,\\&quot;)\\n    column_labels_list.append(\\&quot;junk\\&quot;)\\n    \\n    # assign the new colum labels to the dataframe\\n    df.columns = column_labels_list\\n    \\n    # Drop all the column data that we don&#39;t need\\n    df = df.drop(list_to_drop, axis=&#39;columns&#39;)\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with leading (left) zeros, for hours\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:0&gt;4}&#39;.format(x))\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with trailing (right) zeros, for seconds\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:&lt;06}&#39;.format(x))\\n    \\n    # Convert the `date` column to a string so it can be combined with the `Time` column\\n    df[&#39;date&#39;] = df[&#39;date&#39;].apply(str)\\n    \\n    # Create a new date-time container from the updated `date` and `Time` columns: date_times\\n    date_times = pd.to_datetime( df[&#39;date&#39;] + \\&quot; \\&quot; + df[&#39;Time&#39;], format=\\&quot;%Y%m%d %H%M%S\\&quot; )\\n    \\n    # Set the DataFrame index to this new `date_times` container:\\n    df.set_index(date_times, inplace=True)\\n\\n    for col in [&#39;wind_speed&#39;,&#39;dry_bulb_faren&#39;,&#39;dew_point_faren&#39;]:\\n        df[col] = pd.to_numeric(df[col], errors=&#39;coerce&#39;)\\n\\n    return df\\ndata_file = \\&quot;/usr/local/share/datasets/NOAA_QCLCD_2011_hourly_13904.csv\\&quot;\\ndf_clean = read_data(data_file)\\n\\n\\n# Import matplotlib.pyplot\\nimport matplotlib.pyplot as plt&quot;,&quot;^X&quot;,&quot;&quot;,&quot;^Y&quot;,null,&quot;^Z&quot;,null,&quot;xp&quot;,100,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^19&quot;,[[&quot;^ &quot;,&quot;id&quot;,624213,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^T&quot;,null,&quot;^1&quot;,null,&quot;^U&quot;,&quot;# Using df_clean, when is sky_condition &#39;CLR&#39;?\\nis_sky_clear = ____\\n\\n# Resample is_sky_clear by day\\nresampled = ____\\n\\n# See the result\\nresampled&quot;,&quot;^V&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Get the cases in &lt;code&gt;df_clean&lt;/code&gt; where the sky is clear. That is, when &lt;code&gt;&#39;sky_condition&#39;&lt;/code&gt; equals &lt;code&gt;&#39;CLR&#39;&lt;/code&gt;, assigning to &lt;code&gt;is_sky_clear&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Resample &lt;code&gt;is_sky_clear&lt;/code&gt; by day, assigning to &lt;code&gt;resampled&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^N&quot;,1,&quot;sct&quot;,&quot;Ex().check_correct(\\n    check_object(&#39;is_sky_clear&#39;).has_equal_value(),\\n    multi(\\n        has_code(\\&quot;df_clean\\\\[ *&#39;sky_condition&#39; *\\\\]\\&quot;, not_typed_msg = \\&quot;Did you select the `sky_condition` column with `df_clean[&#39;sky_condition&#39;]`?\\&quot;),\\n        has_code(\\&quot;\\\\] *== *&#39;CLR&#39;\\&quot;, not_typed_msg = \\&quot;Did you check for `sky_condition` equalling `&#39;CLR&#39;`?\\&quot;)\\n    )\\n)\\nEx().multi(\\n    check_object(&#39;resampled&#39;),\\n    check_function(&#39;is_sky_clear.resample&#39;).check_args(0).has_equal_value()\\n)&quot;,&quot;^W&quot;,&quot;&quot;,&quot;^X&quot;,&quot;# Using df_clean, when is sky_condition &#39;CLR&#39;?\\nis_sky_clear = df_clean[&#39;sky_condition&#39;] == &#39;CLR&#39;\\n\\n# Resample is_sky_clear by day\\nresampled = is_sky_clear.resample(&#39;D&#39;)\\n\\n# See the result\\nresampled&quot;,&quot;^Y&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;The logical condition takes the form &lt;code&gt;dataset[&#39;column&#39;]==&#39;value&#39;&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Use the &lt;code&gt;.resample()&lt;/code&gt; method with &lt;code&gt;&#39;D&#39;&lt;/code&gt; for daily.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,35,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;],[&quot;^ &quot;,&quot;id&quot;,624214,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^T&quot;,null,&quot;^1&quot;,null,&quot;^U&quot;,&quot;# From previous step\\nis_sky_clear = df_clean[&#39;sky_condition&#39;] == &#39;CLR&#39;\\nresampled = is_sky_clear.resample(&#39;D&#39;)\\n\\n# Calculate the number of sunny hours per day\\nsunny_hours = ____\\n\\n# Calculate the number of measured hours per day\\ntotal_hours = ____\\n\\n# Calculate the fraction of hours per day that were sunny\\nsunny_fraction = ____&quot;,&quot;^V&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Calculate the number of measured sunny hours per day as the sum of &lt;code&gt;resampled&lt;/code&gt;, assigning to &lt;code&gt;sunny_hours&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Calculate the total number of measured hours per day as the count of &lt;code&gt;resampled&lt;/code&gt;, assigning to &lt;code&gt;total_hours&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Calculate the fraction of hours per day that were sunny as the ratio of sunny hours to total hours.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^N&quot;,2,&quot;sct&quot;,&quot;Ex().check_correct(\\n    check_object(&#39;sunny_hours&#39;).has_equal_value(),\\n    check_function(&#39;resampled.sum&#39;, signature=False)\\n)\\nEx().check_correct(\\n    check_object(&#39;total_hours&#39;).has_equal_value(),\\n    check_function(&#39;resampled.count&#39;, signature=False)\\n)\\nEx().check_correct(\\n    check_object(&#39;sunny_fraction&#39;).has_equal_value(),\\n    has_code(text=&#39;sunny_hours / total_hours&#39;, not_typed_msg=\\&quot;Did you divide `sunny_hours` by `total_hours`?\\&quot;)\\n)&quot;,&quot;^W&quot;,&quot;&quot;,&quot;^X&quot;,&quot;# From previous step\\nis_sky_clear = df_clean[&#39;sky_condition&#39;] == &#39;CLR&#39;\\nresampled = is_sky_clear.resample(&#39;D&#39;)\\n\\n# Calculate the number of sunny hours per day\\nsunny_hours = resampled.sum()\\n\\n# Calculate the number of measured hours per day\\ntotal_hours = resampled.count()\\n\\n# Calculate the fraction of hours per day that were sunny\\nsunny_fraction = sunny_hours / total_hours&quot;,&quot;^Y&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Call &lt;code&gt;.sum()&lt;/code&gt;, then &lt;code&gt;.count()&lt;/code&gt; on &lt;code&gt;resampled&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;The fraction on sunny days is &lt;code&gt;sunny_hours&lt;/code&gt; divided by &lt;code&gt;total_hours&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,35,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;],[&quot;^ &quot;,&quot;id&quot;,624215,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^T&quot;,null,&quot;^1&quot;,null,&quot;^U&quot;,&quot;# From previous steps\\nis_sky_clear = df_clean[&#39;sky_condition&#39;] == &#39;CLR&#39;\\nresampled = is_sky_clear.resample(&#39;D&#39;)\\nsunny_hours = resampled.sum()\\ntotal_hours = resampled.count()\\nsunny_fraction = sunny_hours / total_hours\\n\\n# Make a box plot of sunny_fraction\\n____.____(kind=&#39;____&#39;)\\nplt.show()&quot;,&quot;^V&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Draw a box plot of &lt;code&gt;sunny_fraction&lt;/code&gt; using &lt;code&gt;.plot()&lt;/code&gt; with &lt;code&gt;kind&lt;/code&gt; set to `&#39;box&#39;``.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^N&quot;,3,&quot;sct&quot;,&quot;Ex().check_function(\\&quot;sunny_fraction.plot\\&quot;).check_args(&#39;kind&#39;).has_equal_value(copy=False)\\nEx().check_function(\\&quot;matplotlib.pyplot.show\\&quot;)\\nsuccess_msg(\\&quot;The outlook for your Pandas skills is sunny! By contrast, the weather in the dataset is typically sunny less than 40% of the time.\\&quot;)&quot;,&quot;^W&quot;,&quot;&quot;,&quot;^X&quot;,&quot;# From previous steps\\nis_sky_clear = df_clean[&#39;sky_condition&#39;] == &#39;CLR&#39;\\nresampled = is_sky_clear.resample(&#39;D&#39;)\\nsunny_hours = resampled.sum()\\ntotal_hours = resampled.count()\\nsunny_fraction = sunny_hours / total_hours\\n\\n# Make a box plot of sunny_fraction\\nsunny_fraction.plot(kind=&#39;box&#39;)\\nplt.show()&quot;,&quot;^Y&quot;,&quot;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,30,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;]],&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.9794599945909943,&quot;^18&quot;,44665],[&quot;^ &quot;,&quot;id&quot;,44666,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^T&quot;,&quot;&lt;p&gt;Dew point is a measure of relative humidity based on pressure and temperature.\\nA dew point above 65 is considered uncomfortable while a temperature above 90 is also considered\\nuncomfortable.&lt;/p&gt;\\n&lt;p&gt;In this exercise, you will explore the maximum temperature and dew point of each month. The columns of interest are &lt;code&gt;&#39;dew_point_faren&#39;&lt;/code&gt; and &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt;. After resampling them appropriately to get the maximum temperature and dew point in each month, generate a histogram of these values as subplots. Uncomfortably, you will notice that the maximum dew point is above 65 every month! &lt;/p&gt;\\n&lt;p&gt;&lt;code&gt;df_clean&lt;/code&gt; has been pre-loaded for you.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Heat or humidity&quot;,&quot;^U&quot;,&quot;# Resample dew_point_faren and dry_bulb_faren by Month, aggregating the maximum values: monthly_max\\nmonthly_max = ____\\n\\n# Generate a histogram with bins=8, alpha=0.5, subplots=True\\n____\\n\\n# Show the plot\\nplt.show()&quot;,&quot;^V&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Select the &lt;code&gt;&#39;dew_point_faren&#39;&lt;/code&gt; and &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; columns (in that order). Resample by month and aggregate the maximum monthly temperatures. Assign the result to &lt;code&gt;monthly_max&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Plot a histogram of the resampled data with &lt;code&gt;bins=8&lt;/code&gt;, &lt;code&gt;alpha=0.5&lt;/code&gt;, and &lt;code&gt;subplots=True&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^N&quot;,14,&quot;sct&quot;,&quot;Ex().check_correct(\\n  \\tcheck_object(\\&quot;monthly_max\\&quot;).has_equal_value(\\&quot;The contents of `monthly_max` are not correct. It seems like you did not resample the data correctly. Did you place the two column names in a list?\\&quot;),\\n    has_equal_ast(\\&quot;It seems like you did not resample the data correctly. Did you select the `&#39;dew_point_faren&#39;` and `&#39;dry_bulb_faren&#39;` columns, then use `.resample(&#39;M&#39;)` along with `.max()`?\\&quot;,\\n                                code=\\&quot;df_clean[[&#39;dew_point_faren&#39;,&#39;dry_bulb_faren&#39;]].resample(&#39;M&#39;).max()\\&quot;,\\n                                exact=False))\\n\\nEx().check_function(&#39;monthly_max.plot&#39;)\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.show\\&quot;)\\n\\nsuccess_msg(\\&quot;Excellent job!\\&quot;)&quot;,&quot;^W&quot;,&quot;import pandas as pd\\n\\ndef read_data(file_name):\\n    # read the data file\\n    df = pd.read_csv(file_name, index_col=False, header=None)\\n\\n    # define the column labels for the real data file from NOAA: column_labels\\n    column_labels = \\&quot;Wban,date,Time,StationType,sky_condition,sky_conditionFlag,visibility,visibilityFlag,wx_and_obst_to_vision,wx_and_obst_to_visionFlag,dry_bulb_faren,dry_bulb_farenFlag,dry_bulb_cel,dry_bulb_celFlag,wet_bulb_faren,wet_bulb_farenFlag,wet_bulb_cel,wet_bulb_celFlag,dew_point_faren,dew_point_farenFlag,dew_point_cel,dew_point_celFlag,relative_humidity,relative_humidityFlag,wind_speed,wind_speedFlag,wind_direction,wind_directionFlag,value_for_wind_character,value_for_wind_characterFlag,station_pressure,station_pressureFlag,pressure_tendency,pressure_tendencyFlag,presschange,presschangeFlag,sea_level_pressure,sea_level_pressureFlag,record_type,hourly_precip,hourly_precipFlag,altimeter,altimeterFlag\\&quot;\\n    \\n    # define the sub-set list of columns to drop\\n    list_to_drop = [ &#39;sky_conditionFlag&#39;, &#39;visibilityFlag&#39;, &#39;wx_and_obst_to_vision&#39;, &#39;wx_and_obst_to_visionFlag&#39;,\\n      &#39;dry_bulb_farenFlag&#39;, &#39;dry_bulb_celFlag&#39;, &#39;wet_bulb_farenFlag&#39;, &#39;wet_bulb_celFlag&#39;, &#39;dew_point_farenFlag&#39;,\\n      &#39;dew_point_celFlag&#39;, &#39;relative_humidityFlag&#39;, &#39;wind_speedFlag&#39;, &#39;wind_directionFlag&#39;, &#39;value_for_wind_character&#39;,\\n      &#39;value_for_wind_characterFlag&#39;, &#39;station_pressureFlag&#39;, &#39;pressure_tendencyFlag&#39;, &#39;pressure_tendency&#39;,\\n      &#39;presschange&#39;,&#39;presschangeFlag&#39;, &#39;sea_level_pressureFlag&#39;, &#39;hourly_precip&#39;,  &#39;hourly_precipFlag&#39;, &#39;altimeter&#39;,  &#39;record_type&#39;, &#39;altimeterFlag&#39;, &#39;junk&#39;]\\n    \\n    # split on the comma to create a list, then append one more element as the data files all have a trailing comma at the end of each line: column_labels_list\\n    column_labels_list = column_labels.split(\\&quot;,\\&quot;)\\n    column_labels_list.append(\\&quot;junk\\&quot;)\\n    \\n    # assign the new colum labels to the dataframe\\n    df.columns = column_labels_list\\n    \\n    # Drop all the column data that we don&#39;t need\\n    df = df.drop(list_to_drop, axis=&#39;columns&#39;)\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with leading (left) zeros, for hours\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:0&gt;4}&#39;.format(x))\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with trailing (right) zeros, for seconds\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:&lt;06}&#39;.format(x))\\n    \\n    # Convert the `date` column to a string so it can be combined with the `Time` column\\n    df[&#39;date&#39;] = df[&#39;date&#39;].apply(str)\\n    \\n    # Create a new date-time container from the updated `date` and `Time` columns: date_times\\n    date_times = pd.to_datetime( df[&#39;date&#39;] + \\&quot; \\&quot; + df[&#39;Time&#39;], format=\\&quot;%Y%m%d %H%M%S\\&quot; )\\n    \\n    # Set the DataFrame index to this new `date_times` container:\\n    df.set_index(date_times, inplace=True)\\n\\n    for col in [&#39;wind_speed&#39;,&#39;dry_bulb_faren&#39;,&#39;dew_point_faren&#39;]:\\n        df[col] = pd.to_numeric(df[col], errors=&#39;coerce&#39;)\\n\\n    return df\\ndata_file = \\&quot;/usr/local/share/datasets/NOAA_QCLCD_2011_hourly_13904.csv\\&quot;\\ndf_clean = read_data(data_file)\\nimport matplotlib.pyplot as plt&quot;,&quot;^X&quot;,&quot;# Resample dew_point_faren and dry_bulb_faren by Month, aggregating the maximum values: monthly_max\\nmonthly_max = df_clean[[&#39;dew_point_faren&#39;,&#39;dry_bulb_faren&#39;]].resample(&#39;M&#39;).max()\\n\\n# Generate a histogram with bins=8, alpha=0.5, subplots=True\\nmonthly_max.plot(kind=&#39;hist&#39;, bins=8, alpha=0.5, subplots=True)\\n\\n# Show the plot\\nplt.show()&quot;,&quot;^Y&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;To select the two columns, pass them in as a list inside &lt;code&gt;df.clean[]&lt;/code&gt;. Once you&#39;ve done this, you can resample by month using &lt;code&gt;.resample(&#39;M&#39;)&lt;/code&gt; and aggregate the maximum monthly temperatures with &lt;code&gt;.max()&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;To generate the histogram, apply the &lt;code&gt;.plot()&lt;/code&gt; method on &lt;code&gt;monthly_max&lt;/code&gt; and specify the keyword arguments &lt;code&gt;bins=8&lt;/code&gt;, &lt;code&gt;alpha=0.5&lt;/code&gt;, and &lt;code&gt;subplots=True&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,100,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.35646497171946145,&quot;^18&quot;,44666],[&quot;^ &quot;,&quot;id&quot;,71565,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^T&quot;,&quot;&lt;p&gt;We already know that 2011 was hotter than the climate normals for the previous thirty years. In this final exercise, you will compare the maximum temperature in August 2011 against that of the August 2010 climate normals. More specifically, you will use a CDF plot to determine the probability of the 2011 daily maximum temperature in August being above the 2010 climate normal value. To do this, you will leverage the data manipulation, filtering, resampling, and visualization skills you have acquired throughout this course. &lt;/p&gt;\\n&lt;p&gt;The two DataFrames &lt;code&gt;df_clean&lt;/code&gt; and &lt;code&gt;df_climate&lt;/code&gt; are available in the workspace. Your job is to select the maximum temperature in August in &lt;code&gt;df_climate&lt;/code&gt;, and then maximum daily temperatures in  August 2011. You will then filter out the days in August 2011 that were above the August 2010 maximum, and use this to construct a CDF plot. &lt;/p&gt;\\n&lt;p&gt;Once you&#39;ve generated the CDF, notice how it shows that there was a 50% probability of the 2011 daily maximum temperature in August being 5 degrees\\nabove the 2010 climate normal value!&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Probability of high temperatures&quot;,&quot;^U&quot;,&quot;# Extract the maximum temperature in August 2010 from df_climate: august_max\\naugust_max = ____\\nprint(august_max)\\n\\n# Resample August 2011 temps in df_clean by day &amp; aggregate the max value: august_2011\\naugust_2011 = ____\\n\\n\\n# Filter for days in august_2011 where the value exceeds august_max: august_2011_high\\n\\naugust_2011_high = ____\\n\\n# Construct a CDF of august_2011_high\\n____\\n\\n# Display the plot\\nplt.show()&quot;,&quot;^V&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;From &lt;code&gt;df_climate&lt;/code&gt;, extract the maximum temperature observed in August 2010. The relevant column here is &lt;code&gt;&#39;Temperature&#39;&lt;/code&gt;. You can select the rows corresponding to August 2010 in multiple ways. For example, &lt;code&gt;df_climate.loc[&#39;2011-Feb&#39;]&lt;/code&gt; selects all rows corresponding to February 2011, while &lt;code&gt;df_climate.loc[&#39;2009-09&#39;, &#39;Pressure&#39;]&lt;/code&gt; selects the rows corresponding to September 2009 from the &lt;code&gt;&#39;Pressure&#39;&lt;/code&gt; column.&lt;/li&gt;\\n&lt;li&gt;From &lt;code&gt;df_clean&lt;/code&gt;, select the August 2011 temperature data from the &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt;. Resample this data by day and aggregate the maximum value. Store the result in &lt;code&gt;august_2011&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Filter rows of &lt;code&gt;august_2011&lt;/code&gt; to keep days where the value exceeded &lt;code&gt;august_max&lt;/code&gt;. Store the result in &lt;code&gt;august_2011_high&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Construct a CDF of &lt;code&gt;august_2011_high&lt;/code&gt; using 25 bins. Remember to specify the &lt;code&gt;kind&lt;/code&gt;, &lt;code&gt;normed&lt;/code&gt;, and &lt;code&gt;cumulative&lt;/code&gt; parameters in addition to &lt;code&gt;bins&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^N&quot;,15,&quot;sct&quot;,&quot;Ex().check_correct(\\n  \\tcheck_object(\\&quot;august_max\\&quot;).has_equal_value(\\&quot;The contents of `august_max` are not correct. Did you correctly extract the maximum temperature in August 2010 by selecting the `&#39;2010-Aug&#39;` and `&#39;Temperature&#39;` columns from `df_climate` using `.loc[]` and chaining the `.max()` method?\\&quot;),\\n    has_equal_ast(\\&quot;Did you correctly extract the maximum temperature in August 2010 by selecting the `&#39;2010-Aug&#39;` and `&#39;Temperature&#39;` columns from `df_climate` using `.loc[]` and chaining the `.max()` method?\\&quot;,\\n                  code=\\&quot;df_climate.loc[&#39;2010-Aug&#39;,&#39;Temperature&#39;].max()\\&quot;,\\n                  exact=False))\\n\\nEx().check_function(\\&quot;print\\&quot;).check_args(0).has_equal_value()\\n\\nEx().check_correct(\\n  \\tcheck_object(\\&quot;august_2011\\&quot;).has_equal_value(\\&quot;The contents of `august_2011` are not correct. It seems like you did not resample the data correctly.\\&quot;),\\n    has_equal_ast(\\&quot;It seems like you did not resample the data correctly. Did you select the `&#39;2011-Aug&#39;` and `&#39;dry_bulb_faren&#39;` columns, then use `.resample(&#39;D&#39;)` along with `.max()`?\\&quot;,\\n                  code=\\&quot;df_clean.loc[&#39;2011-Aug&#39;,&#39;dry_bulb_faren&#39;].resample(&#39;D&#39;).max()\\&quot;,\\n                  exact=False))\\n\\nEx().check_correct(\\n  \\tcheck_object(\\&quot;august_2011_high\\&quot;).has_equal_value(\\&quot;The contents of `august_2011_high` aren&#39;t correct. It looks like you didn&#39;t correctly filter out the days in `august_2011` where the value exceeded `august_max`.\\&quot;),\\n    has_equal_ast(\\&quot;The contents of `august_2011_high` aren&#39;t correct. It looks like you didn&#39;t correctly filter out the days in `august_2011` where the value exceeded `august_max`.\\&quot;,\\n                  code=\\&quot;august_2011_high = august_2011.loc[august_2011 &gt; august_max]\\&quot;,\\n                  exact=False))\\n\\nEx().check_function(\\&quot;august_2011_high.plot\\&quot;)\\nEx().check_function(\\&quot;matplotlib.pyplot.show\\&quot;)\\n\\nsuccess_msg(\\&quot;Fantastic work - you&#39;ve reached the end of the course!\\&quot;)&quot;,&quot;^W&quot;,&quot;import pandas as pd\\n\\ndef read_data(file_name):\\n    # read the data file\\n    df = pd.read_csv(file_name, index_col=False, header=None)\\n\\n    # define the column labels for the real data file from NOAA: column_labels\\n    column_labels = \\&quot;Wban,date,Time,StationType,sky_condition,sky_conditionFlag,visibility,visibilityFlag,wx_and_obst_to_vision,wx_and_obst_to_visionFlag,dry_bulb_faren,dry_bulb_farenFlag,dry_bulb_cel,dry_bulb_celFlag,wet_bulb_faren,wet_bulb_farenFlag,wet_bulb_cel,wet_bulb_celFlag,dew_point_faren,dew_point_farenFlag,dew_point_cel,dew_point_celFlag,relative_humidity,relative_humidityFlag,wind_speed,wind_speedFlag,wind_direction,wind_directionFlag,value_for_wind_character,value_for_wind_characterFlag,station_pressure,station_pressureFlag,pressure_tendency,pressure_tendencyFlag,presschange,presschangeFlag,sea_level_pressure,sea_level_pressureFlag,record_type,hourly_precip,hourly_precipFlag,altimeter,altimeterFlag\\&quot;\\n    \\n    # define the sub-set list of columns to drop\\n    list_to_drop = [ &#39;sky_conditionFlag&#39;, &#39;visibilityFlag&#39;, &#39;wx_and_obst_to_vision&#39;, &#39;wx_and_obst_to_visionFlag&#39;,\\n      &#39;dry_bulb_farenFlag&#39;, &#39;dry_bulb_celFlag&#39;, &#39;wet_bulb_farenFlag&#39;, &#39;wet_bulb_celFlag&#39;, &#39;dew_point_farenFlag&#39;,\\n      &#39;dew_point_celFlag&#39;, &#39;relative_humidityFlag&#39;, &#39;wind_speedFlag&#39;, &#39;wind_directionFlag&#39;, &#39;value_for_wind_character&#39;,\\n      &#39;value_for_wind_characterFlag&#39;, &#39;station_pressureFlag&#39;, &#39;pressure_tendencyFlag&#39;, &#39;pressure_tendency&#39;,\\n      &#39;presschange&#39;,&#39;presschangeFlag&#39;, &#39;sea_level_pressureFlag&#39;, &#39;hourly_precip&#39;,  &#39;hourly_precipFlag&#39;, &#39;altimeter&#39;,  &#39;record_type&#39;, &#39;altimeterFlag&#39;, &#39;junk&#39;]\\n    \\n    # split on the comma to create a list, then append one more element as the data files all have a trailing comma at the end of each line: column_labels_list\\n    column_labels_list = column_labels.split(\\&quot;,\\&quot;)\\n    column_labels_list.append(\\&quot;junk\\&quot;)\\n    \\n    # assign the new colum labels to the dataframe\\n    df.columns = column_labels_list\\n    \\n    # Drop all the column data that we don&#39;t need\\n    df = df.drop(list_to_drop, axis=&#39;columns&#39;)\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with leading (left) zeros, for hours\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:0&gt;4}&#39;.format(x))\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with trailing (right) zeros, for seconds\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:&lt;06}&#39;.format(x))\\n    \\n    # Convert the `date` column to a string so it can be combined with the `Time` column\\n    df[&#39;date&#39;] = df[&#39;date&#39;].apply(str)\\n    \\n    # Create a new date-time container from the updated `date` and `Time` columns: date_times\\n    date_times = pd.to_datetime( df[&#39;date&#39;] + \\&quot; \\&quot; + df[&#39;Time&#39;], format=\\&quot;%Y%m%d %H%M%S\\&quot; )\\n    \\n    # Set the DataFrame index to this new `date_times` container:\\n    df.set_index(date_times, inplace=True)\\n\\n    for col in [&#39;wind_speed&#39;,&#39;dry_bulb_faren&#39;,&#39;dew_point_faren&#39;]:\\n        df[col] = pd.to_numeric(df[col], errors=&#39;coerce&#39;)\\n\\n    return df\\ndata_file = \\&quot;/usr/local/share/datasets/NOAA_QCLCD_2011_hourly_13904.csv\\&quot;\\ndf_clean = read_data(data_file)\\n\\nclimatology_data_file = \\&quot;/usr/local/share/datasets/weather_data_austin_2010.csv\\&quot;\\n\\ndf_climate = pd.read_csv(climatology_data_file, parse_dates=True, index_col=&#39;Date&#39;)\\n\\n# Import matplotlib.pyplot\\nimport matplotlib.pyplot as plt\\n&quot;,&quot;^X&quot;,&quot;# Extract the maximum temperature in August 2010 from df_climate: august_max\\naugust_max = df_climate.loc[&#39;2010-Aug&#39;,&#39;Temperature&#39;].max()\\nprint(august_max)\\n\\n# Resample August 2011 temps in df_clean by day &amp; aggregate the max value: august_2011\\naugust_2011 = df_clean.loc[&#39;2011-Aug&#39;,&#39;dry_bulb_faren&#39;].resample(&#39;D&#39;).max()\\n\\n# Filter for days in august_2011 where the value exceeds august_max: august_2011_high\\n\\naugust_2011_high = august_2011.loc[august_2011 &gt; august_max]\\n\\n# Construct a CDF of august_2011_high\\naugust_2011_high.plot(kind=&#39;hist&#39;, normed=True, cumulative=True, bins=25)\\n\\n# Display the plot\\nplt.show()&quot;,&quot;^Y&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Use &lt;code&gt;.loc[]&lt;/code&gt; on &lt;code&gt;df_climate&lt;/code&gt; to select &lt;code&gt;&#39;2010-Aug&#39;&lt;/code&gt; from the &lt;code&gt;&#39;Temperature&#39;&lt;/code&gt; column. Then, apply the &lt;code&gt;.max()&lt;/code&gt; method to your &lt;code&gt;.loc[]&lt;/code&gt; selection.&lt;/li&gt;\\n&lt;li&gt;As you did above, use &lt;code&gt;.loc[]&lt;/code&gt;, this time on &lt;code&gt;df_clean&lt;/code&gt;, selecting &lt;code&gt;&#39;2011-Aug&#39;&lt;/code&gt; from the &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; column. To resample by day, use &lt;code&gt;.resample(&#39;D&#39;)&lt;/code&gt;, and to aggregate the maximum value, use &lt;code&gt;.max()&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Use &lt;code&gt;.loc[]&lt;/code&gt; on &lt;code&gt;august_2011&lt;/code&gt; with a Boolean condition inside that returns &lt;code&gt;True&lt;/code&gt; if &lt;code&gt;august_2011&lt;/code&gt; is &lt;code&gt;&amp;gt;&lt;/code&gt; &lt;code&gt;august_max&lt;/code&gt;. &lt;/li&gt;\\n&lt;li&gt;To construct a CDF of &lt;code&gt;august_2011_high&lt;/code&gt;, use the &lt;code&gt;.plot()&lt;/code&gt; method on &lt;code&gt;august_2011_high&lt;/code&gt; with &lt;code&gt;kind=&#39;hist&#39;&lt;/code&gt;, &lt;code&gt;normed=True&lt;/code&gt;, &lt;code&gt;cumulative=True&lt;/code&gt;, and the desired number of &lt;code&gt;bins&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,100,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.053210473284661,&quot;^18&quot;,71565],[&quot;^ &quot;,&quot;id&quot;,71697,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^T&quot;,null,&quot;^1&quot;,&quot;Congratulations!&quot;,&quot;^U&quot;,&quot;&quot;,&quot;^V&quot;,null,&quot;^N&quot;,16,&quot;sct&quot;,&quot;&quot;,&quot;^W&quot;,&quot;&quot;,&quot;^X&quot;,&quot;&quot;,&quot;^Y&quot;,null,&quot;^Z&quot;,null,&quot;xp&quot;,50,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^12&quot;,null,&quot;^13&quot;,&quot;//videos.datacamp.com/transcoded/1639_pandas-1/v2/hls-ch4_4.master.m3u8&quot;,&quot;^14&quot;,56.25,&quot;^15&quot;,&quot;course_1639_346f5e3f0814296f20bfe050c6c12275&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.3854862055280488,&quot;^18&quot;,71697]]]],&quot;activeImage&quot;,[&quot;^0&quot;,[&quot;status&quot;,&quot;SUCCESS&quot;,&quot;data&quot;,&quot;course-1639-master:013883417c7ca80ce5f0186f4636f631-20190204201642849&quot;]],&quot;sharedImage&quot;,[&quot;^0&quot;,[&quot;status&quot;,&quot;SUCCESS&quot;,&quot;data&quot;,&quot;shared-python:a3c4bdd667d4bbe8788c137413874aaa-20190215130059186&quot;]]]],&quot;systemStatus&quot;,[&quot;^0&quot;,[&quot;indicator&quot;,&quot;none&quot;,&quot;description&quot;,&quot;No status has been fetched from the Status Page.&quot;]],&quot;backendSession&quot;,[&quot;^0&quot;,[&quot;status&quot;,[&quot;^0&quot;,[&quot;code&quot;,&quot;none&quot;,&quot;text&quot;,&quot;&quot;]],&quot;isInitSession&quot;,false,&quot;message&quot;,null]],&quot;settings&quot;,[&quot;^0&quot;,[&quot;uiTheme&quot;,&quot;LIGHT&quot;,&quot;isOnboarding&quot;,false]],&quot;autocomplete&quot;,[&quot;^0&quot;,[]],&quot;user&quot;,[&quot;^0&quot;,[&quot;status&quot;,null,&quot;settings&quot;,[&quot;^0&quot;,[]]]],&quot;fileBrowser&quot;,[&quot;^0&quot;,[&quot;isVisible&quot;,true,&quot;fileSelected&quot;,null]],&quot;chapter&quot;,[&quot;^0&quot;,[&quot;current&quot;,[&quot;^0&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,4,&quot;slug&quot;,&quot;case-study-sunlight-in-austin&quot;,&quot;last_updated_on&quot;,&quot;04/02/2019&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,16,&quot;free_preview&quot;,null,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_2357/slides/ch4_slides.pdf&quot;,&quot;title&quot;,&quot;Case Study - Sunlight in Austin&quot;,&quot;xp&quot;,1350,&quot;id&quot;,4284,&quot;description&quot;,&quot;Working with real-world weather and climate data, in this chapter you will bring together and apply all of the skills you have acquired in this course. You will use Pandas to manipulate the data into a form usable for analysis, and then systematically explore it using the techniques you learned in the prior chapters. Enjoy!&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]]]],&quot;boot&quot;,[&quot;^0&quot;,[&quot;bootState&quot;,&quot;PRE_BOOTED&quot;,&quot;error&quot;,null]],&quot;location&quot;,[&quot;^0&quot;,[&quot;current&quot;,[&quot;^0&quot;,[&quot;pathname&quot;,&quot;/courses/pandas-foundations/case-study-sunlight-in-austin&quot;,&quot;query&quot;,[&quot;^0&quot;,[&quot;ex&quot;,&quot;6&quot;]]]],&quot;canonical&quot;,null]],&quot;course&quot;,[&quot;^0&quot;,[&quot;difficulty_level&quot;,2,&quot;reduced_outline&quot;,null,&quot;shared_image&quot;,&quot;shared-python:a3c4bdd667d4bbe8788c137413874aaa-20190215130059186&quot;,&quot;active_image&quot;,&quot;course-1639-master:013883417c7ca80ce5f0186f4636f631-20190204201642849&quot;,&quot;author_field&quot;,null,&quot;chapters&quot;,[&quot;~#iL&quot;,[[&quot;^0&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,1,&quot;slug&quot;,&quot;data-ingestion-inspection&quot;,&quot;last_updated_on&quot;,&quot;04/02/2019&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,14,&quot;free_preview&quot;,true,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_2357/slides/ch1_slides.pdf&quot;,&quot;title&quot;,&quot;Data ingestion &amp; inspection&quot;,&quot;xp&quot;,1100,&quot;id&quot;,4259,&quot;description&quot;,&quot;In this chapter, you will be introduced to Panda&#39;s DataFrames. You will use Pandas to import and inspect a variety of datasets, ranging from population data obtained from The World Bank to monthly stock data obtained via Yahoo! Finance. You will also practice building DataFrames from scratch, and become familiar with Pandas&#39; intrinsic data visualization capabilities.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]],[&quot;^0&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,2,&quot;slug&quot;,&quot;exploratory-data-analysis&quot;,&quot;last_updated_on&quot;,&quot;13/11/2018&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,15,&quot;free_preview&quot;,null,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_2357/slides/ch2_slides.pdf&quot;,&quot;title&quot;,&quot;Exploratory data analysis&quot;,&quot;xp&quot;,1250,&quot;id&quot;,4260,&quot;description&quot;,&quot;Having learned how to ingest and inspect your data, you will next explore it visually as well as quantitatively. This process, known as exploratory data analysis (EDA), is a crucial component of any data science project. Pandas has powerful methods that help with statistical and visual EDA. In this chapter, you will learn how and when to apply these techniques.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]],[&quot;^0&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,3,&quot;slug&quot;,&quot;time-series-in-pandas&quot;,&quot;last_updated_on&quot;,&quot;08/01/2019&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,17,&quot;free_preview&quot;,null,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_2357/slides/ch3_slides.pdf&quot;,&quot;title&quot;,&quot;Time series in pandas&quot;,&quot;xp&quot;,1450,&quot;id&quot;,4261,&quot;description&quot;,&quot;In this chapter, you will learn how to manipulate and visualize time series data using Pandas. You will become familiar with concepts such as upsampling, downsampling, and interpolation. You will practice using Pandas&#39; method chaining to efficiently filter your data and perform time series analyses. From stock prices to flight timings, time series data are found in a wide variety of domains and being able to effectively work with such data can be an invaluable skill.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]],[&quot;^0&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,4,&quot;slug&quot;,&quot;case-study-sunlight-in-austin&quot;,&quot;last_updated_on&quot;,&quot;04/02/2019&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,16,&quot;free_preview&quot;,null,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_2357/slides/ch4_slides.pdf&quot;,&quot;title&quot;,&quot;Case Study - Sunlight in Austin&quot;,&quot;xp&quot;,1350,&quot;id&quot;,4284,&quot;description&quot;,&quot;Working with real-world weather and climate data, in this chapter you will bring together and apply all of the skills you have acquired in this course. You will use Pandas to manipulate the data into a form usable for analysis, and then systematically explore it using the techniques you learned in the prior chapters. Enjoy!&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]]]],&quot;time_needed&quot;,&quot;4 hours&quot;,&quot;author_image&quot;,&quot;placeholder.png&quot;,&quot;runtime_config&quot;,&quot;heavy&quot;,&quot;lti_only&quot;,false,&quot;image_url&quot;,&quot;https://assets.datacamp.com/production/course_1639/shields/thumb/shield_image_course_1639_20190204-13-1xw9geb?1549311490&quot;,&quot;topic_id&quot;,3,&quot;slug&quot;,&quot;pandas-foundations&quot;,&quot;last_updated_on&quot;,&quot;04/02/2019&quot;,&quot;paid&quot;,true,&quot;university&quot;,null,&quot;state&quot;,&quot;live&quot;,&quot;author_bio&quot;,null,&quot;should_cache&quot;,true,&quot;sharing_links&quot;,[&quot;^0&quot;,[&quot;twitter&quot;,&quot;http://bit.ly/1eWTMJh&quot;,&quot;facebook&quot;,&quot;http://bit.ly/1iS42Do&quot;]],&quot;title&quot;,&quot;pandas Foundations&quot;,&quot;xp&quot;,5150,&quot;image_thumbnail_url&quot;,&quot;https://assets.datacamp.com/production/course_1639/shields/thumb_home/shield_image_course_1639_20190204-13-1xw9geb?1549311490&quot;,&quot;short_description&quot;,&quot;Learn how to use the industry-standard pandas library to import, build, and manipulate DataFrames. &quot;,&quot;nb_of_subscriptions&quot;,85428,&quot;type&quot;,&quot;datacamp&quot;,&quot;link&quot;,&quot;https://www.datacamp.com/courses/pandas-foundations&quot;,&quot;id&quot;,1639,&quot;description&quot;,&quot;Pandas DataFrames are the most widely used in-memory representation of complex data collections within Python.  Whether in finance, scientific fields, or data science, a familiarity with Pandas is essential.  This course teaches you to work with real-world data sets containing both string and numeric data, often structured around time series.  You will learn powerful analysis, selection, and visualization techniques in this course.&quot;,&quot;programming_language&quot;,&quot;python&quot;]],&quot;exercises&quot;,[&quot;^0&quot;,[&quot;current&quot;,5,&quot;all&quot;,[&quot;^1:&quot;,[[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;externalId&quot;,44654,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^1:&quot;,[]],&quot;number&quot;,1,&quot;video_hls&quot;,&quot;//videos.datacamp.com/transcoded/1639_pandas-1/v6/hls-ch4_1.master.m3u8&quot;,&quot;randomNumber&quot;,0.6686500359838821,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^1:&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Reading and cleaning the data&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,44654,&quot;projector_key&quot;,&quot;course_1639_76ca82bd71594808142775690560f17f&quot;,&quot;video_link&quot;,null]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;instructions&quot;,null,&quot;externalId&quot;,44655,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Which pandas read method have you used in prior chapters to read in your data?&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1:&quot;,[&quot;[&lt;code&gt;pd.read_csv()&lt;/code&gt;]&quot;,&quot;&lt;code&gt;pd.to_csv()&lt;/code&gt;&quot;,&quot;&lt;code&gt;pd.read_hdf()&lt;/code&gt;&quot;,&quot;&lt;code&gt;np.load()&lt;/code&gt;&quot;]],&quot;number&quot;,2,&quot;randomNumber&quot;,0.5829517747139357,&quot;assignment&quot;,&quot;&lt;p&gt;The first step in our analysis is to read in the data. Upon inspection with a certain system tool, we find that the data appears to be ASCII encoded with comma delimited columns, but has no header and no column labels. Which of the following is the best method to start with to read the data files?&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1:&quot;,[&quot;Correct! The &lt;code&gt;read_csv()&lt;/code&gt; function will become second nature to you as you continue using pandas.&quot;,&quot;Incorrect.&quot;,&quot;Incorrect.&quot;,&quot;Incorrect.&quot;]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;What method should we use to read the data?&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;id&quot;,44655]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Import pandas\\n\\n\\n# Read in the data file: df\\ndf = pd.read_csv(____)\\n\\n# Print the output of df.head()\\nprint(df.head())\\n\\n# Read in the data file with header=None: df_headers\\ndf_headers = pd.read_csv(____, ____=None)\\n\\n# Print the output of df_headers.head()\\nprint(df_headers.head())&quot;,&quot;sct&quot;,&quot;Ex().has_import(\\&quot;pandas\\&quot;)\\n\\nEx().check_correct(\\n\\tcheck_object(\\&quot;df\\&quot;).has_equal_value(),\\n\\tcheck_function(\\&quot;pandas.read_csv\\&quot;).check_args(0).has_equal_value()  \\t\\n)\\n\\nEx().check_function(&#39;print&#39;, index = 0).check_args(0).has_equal_value()\\n\\nEx().check_correct(\\n\\tcheck_object(\\&quot;df_headers\\&quot;).has_equal_value(),\\n\\tcheck_function(\\&quot;pandas.read_csv\\&quot;, signature = sig_from_obj(&#39;pandas.read_csv&#39;)).multi(\\n    \\tcheck_args(0).has_equal_value()\\n#      \\tcheck_args(&#39;header&#39;).has_equal_value()\\n    )  \\n)\\n\\nEx().check_function(&#39;print&#39;, index = 0).check_args(0).has_equal_value()\\n\\nsuccess_msg(\\&quot;Well done! Note how the column names are not informative. You&#39;ll fix this in the next exercise!\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Import &lt;code&gt;pandas&lt;/code&gt; as &lt;code&gt;pd&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Read the file &lt;code&gt;data_file&lt;/code&gt; into a DataFrame called &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Print the output of &lt;code&gt;df.head()&lt;/code&gt;. This has been done for you. Notice the formatting problems in &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Re-read the data using specifying the keyword argument &lt;code&gt;header=None&lt;/code&gt; and assign it to &lt;code&gt;df_headers&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Print the output of &lt;code&gt;df_headers.head()&lt;/code&gt;. This has already been done for you. Hit &#39;Submit Answer&#39; and see how this resolves the formatting issues.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,44656,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;You can import &lt;code&gt;x&lt;/code&gt; from &lt;code&gt;y&lt;/code&gt; using the command &lt;code&gt;from y import x&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;You can read in &lt;code&gt;data_file&lt;/code&gt; by passing it in as an argument to &lt;code&gt;pd.read_csv()&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;&lt;code&gt;df.head()&lt;/code&gt; has already been printed for you.&lt;/li&gt;\\n&lt;li&gt;To indicate that there are no headers in &lt;code&gt;&#39;data.csv&#39;&lt;/code&gt;, specify &lt;code&gt;header=None&lt;/code&gt; in your &lt;code&gt;pd.read_csv()&lt;/code&gt; function.&lt;/li&gt;\\n&lt;li&gt;&lt;code&gt;df_headers.head()&lt;/code&gt; has already been printed for you. Hit &#39;Submit Answer` to see the results!&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1:&quot;,[]],&quot;number&quot;,3,&quot;randomNumber&quot;,0.12915190112887842,&quot;assignment&quot;,&quot;&lt;p&gt;Now that you have identified the method to use to read the data, let&#39;s try to read one file. The problem with real data such as this is that the files are almost never formatted in a convenient way. In this exercise, there are several problems to overcome in reading the file. First, there is no header, and thus the columns don&#39;t have labels. There is also no obvious index column, since none of the data columns contain a full date or time.&lt;/p&gt;\\n&lt;p&gt;Your job is to read the file into a DataFrame using the default arguments. After inspecting it, you will re-read the file specifying that there are no headers supplied. &lt;/p&gt;\\n&lt;p&gt;The CSV file has been provided for you as the variable &lt;code&gt;data_file&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1:&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Reading in a data file&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;data_file = \\&quot;/usr/local/share/datasets/NOAA_QCLCD_2011_hourly_13904.csv\\&quot;\\n\\n&quot;,&quot;solution&quot;,&quot;# Import pandas\\nimport pandas as pd\\n\\n# Read in the data file: df\\ndf = pd.read_csv(data_file)\\n\\n# Print the output of df.head()\\nprint(df.head())\\n\\n# Read in the data file with header=None: df_headers\\ndf_headers = pd.read_csv(data_file, header=None)\\n\\n# Print the output of df_headers.head()\\nprint(df_headers.head())&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,44656]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Split on the comma to create a list: column_labels_list\\ncolumn_labels_list = ____\\n\\n# Assign the new column labels to the DataFrame: df.columns\\n____ = column_labels_list\\n\\n# Remove the appropriate columns: df_dropped\\ndf_dropped = ____\\n\\n# Print the output of df_dropped.head()\\nprint(df_dropped.head())&quot;,&quot;sct&quot;,&quot;Ex().check_correct(\\n\\tcheck_object(\\&quot;column_labels_list\\&quot;).has_equal_value(),\\n\\tcheck_function(\\&quot;column_labels.split\\&quot;, \\n                   signature = sig_from_obj(&#39;column_lables.split&#39;)).check_args(0).has_equal_value(\\&quot;The contents of `column_labels_list` are not correct. Did you correctly call the `split.()` method on `column_labels` to split by `&#39;,&#39;`?\\&quot;)\\n)\\n\\nEx().check_correct(\\n\\tcheck_object(&#39;df&#39;).has_equal_value(),\\n  \\thas_equal_ast(code = &#39;column_labels_list&#39;, incorrect_msg = &#39;Did you assign `column_labels_list` to `df.columns`?&#39;)\\n)\\n\\nEx().check_correct(\\n\\tcheck_object(\\&quot;df_dropped\\&quot;).has_equal_value(),\\n  \\tcheck_function(&#39;df.drop&#39;).multi(\\n\\t\\tcheck_args(0).has_equal_value(),\\n      \\tcheck_args(&#39;axis&#39;).has_equal_value()\\n    )\\n)\\n\\nEx().check_function(&#39;print&#39;).check_args(0).has_equal_value()\\n\\nsuccess_msg(\\&quot;Fantastic! Now that you have informative column names, it is a lot easier to interpret the data! But there is still some tidying work to be done: You&#39;ll clean the datetime data in the next exercise.\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Convert the comma separated string &lt;code&gt;column_labels&lt;/code&gt; to a list of strings using &lt;code&gt;.split(&#39;,&#39;)&lt;/code&gt;. Assign the result to &lt;code&gt;column_labels_list&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Reassign &lt;code&gt;df.columns&lt;/code&gt; using the list of strings &lt;code&gt;column_labels_list&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Call &lt;code&gt;df.drop()&lt;/code&gt; with &lt;code&gt;list_to_drop&lt;/code&gt; and &lt;code&gt;axis=&#39;columns&#39;&lt;/code&gt;. Assign the result to &lt;code&gt;df_dropped&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Print &lt;code&gt;df_dropped.head()&lt;/code&gt; to examine the result. This has already been done for you.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,44657,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;By using &lt;code&gt;column_labels.split(&#39;,&#39;)&lt;/code&gt;, you can convert &lt;code&gt;column_labels&lt;/code&gt; into a list of strings. &lt;/li&gt;\\n&lt;li&gt;Set &lt;code&gt;df.columns&lt;/code&gt; to be equal to the list of strings you created above.&lt;/li&gt;\\n&lt;li&gt;The list of columns to drop is called &lt;code&gt;list_to_drop&lt;/code&gt;. This is what you must specify inside &lt;code&gt;df.drop()&lt;/code&gt;, along with &lt;code&gt;axis=&#39;columns&#39;&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;The code to print the output of &lt;code&gt;df_dropped.head()&lt;/code&gt; has already been written for you, so click &#39;Submit Answer&#39; to see the result!&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1:&quot;,[]],&quot;number&quot;,4,&quot;randomNumber&quot;,0.8494507892251537,&quot;assignment&quot;,&quot;&lt;p&gt;After the initial step of reading in the data, the next step is to clean and tidy it so that it is easier to work with.&lt;/p&gt;\\n&lt;p&gt;In this exercise, you will begin this cleaning process by re-assigning column names and dropping unnecessary columns.&lt;/p&gt;\\n&lt;p&gt;pandas has been imported in the workspace as &lt;code&gt;pd&lt;/code&gt;, and the file &lt;code&gt;NOAA_QCLCD_2011_hourly_13904.txt&lt;/code&gt; has been parsed and loaded into a DataFrame &lt;code&gt;df&lt;/code&gt;. The comma separated string of column names, &lt;code&gt;column_labels&lt;/code&gt;, and list of columns to drop, &lt;code&gt;list_to_drop&lt;/code&gt;, have also been loaded for you.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1:&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Re-assigning column names&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;#import pandas\\nimport pandas as pd\\ndata_file = \\&quot;/usr/local/share/datasets/NOAA_QCLCD_2011_hourly_13904.csv\\&quot;\\n# read the data file\\ndf = pd.read_csv(data_file, index_col=False, header=None)\\n# define the column labels for the real data file from NOAA: column_labels\\ncolumn_labels = \\&quot;Wban,date,Time,StationType,sky_condition,sky_conditionFlag,visibility,visibilityFlag,wx_and_obst_to_vision,wx_and_obst_to_visionFlag,dry_bulb_faren,dry_bulb_farenFlag,dry_bulb_cel,dry_bulb_celFlag,wet_bulb_faren,wet_bulb_farenFlag,wet_bulb_cel,wet_bulb_celFlag,dew_point_faren,dew_point_farenFlag,dew_point_cel,dew_point_celFlag,relative_humidity,relative_humidityFlag,wind_speed,wind_speedFlag,wind_direction,wind_directionFlag,value_for_wind_character,value_for_wind_characterFlag,station_pressure,station_pressureFlag,pressure_tendency,pressure_tendencyFlag,presschange,presschangeFlag,sea_level_pressure,sea_level_pressureFlag,record_type,hourly_precip,hourly_precipFlag,altimeter,altimeterFlag,junk\\&quot;\\n\\n# define the sub-set list of columns to drop\\nlist_to_drop = [ &#39;sky_conditionFlag&#39;, &#39;visibilityFlag&#39;, &#39;wx_and_obst_to_vision&#39;, &#39;wx_and_obst_to_visionFlag&#39;,\\n  &#39;dry_bulb_farenFlag&#39;, &#39;dry_bulb_celFlag&#39;, &#39;wet_bulb_farenFlag&#39;, &#39;wet_bulb_celFlag&#39;, &#39;dew_point_farenFlag&#39;,\\n  &#39;dew_point_celFlag&#39;, &#39;relative_humidityFlag&#39;, &#39;wind_speedFlag&#39;, &#39;wind_directionFlag&#39;, &#39;value_for_wind_character&#39;,\\n  &#39;value_for_wind_characterFlag&#39;, &#39;station_pressureFlag&#39;, &#39;pressure_tendencyFlag&#39;, &#39;pressure_tendency&#39;,\\n  &#39;presschange&#39;,&#39;presschangeFlag&#39;, &#39;sea_level_pressureFlag&#39;, &#39;hourly_precip&#39;,  &#39;hourly_precipFlag&#39;, &#39;altimeter&#39;,  &#39;record_type&#39;, &#39;altimeterFlag&#39;, &#39;junk&#39;]&quot;,&quot;solution&quot;,&quot;# Split on the comma to create a list: column_labels_list\\ncolumn_labels_list = column_labels.split(\\&quot;,\\&quot;)\\n\\n# Assign the new column labels to the DataFrame: df.columns\\ndf.columns = column_labels_list\\n\\n# Remove the appropriate columns: df_dropped\\ndf_dropped = df.drop(list_to_drop, axis=&#39;columns&#39;)\\n\\n# Print the output of df_dropped.head()\\nprint(df_dropped.head())&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,44657]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Convert the date column to string: df_dropped[&#39;date&#39;]\\ndf_dropped[&#39;date&#39;] = ____\\n\\n# Pad leading zeros to the Time column: df_dropped[&#39;Time&#39;]\\ndf_dropped[&#39;Time&#39;] = df_dropped[&#39;Time&#39;].apply(lambda x:&#39;{:0&gt;4}&#39;.format(x))\\n\\n# Concatenate the new date and Time columns: date_string\\ndate_string = ____\\n\\n# Convert the date_string Series to datetime: date_times\\ndate_times = pd.____(date_string, ____=&#39;%Y%m%d%H%M&#39;)\\n\\n# Set the index to be the new date_times container: df_clean\\ndf_clean = ____\\n\\n# Print the output of df_clean.head()\\nprint(df_clean.head())&quot;,&quot;sct&quot;,&quot;Ex().check_or(\\n    has_equal_ast(\\&quot;Did you convert the `&#39;date&#39;` column of `df_dropped` to strings with `.astype(str)`?\\&quot;,\\n                  code=\\&quot;df_dropped[&#39;date&#39;].astype(str)\\&quot;,\\n                  exact=False),\\n    has_equal_ast(\\&quot;Did you convert the `&#39;date&#39;` column of `df_dropped` to strings with `.astype(str)`?\\&quot;,\\n                  code=\\&quot;df_dropped.date.astype(str)\\&quot;,\\n                  exact=False))\\n\\nEx().check_or(\\n    has_equal_ast(\\&quot;You do not need to alter the provided code to pad leading zeros to the `&#39;Time&#39;` column.\\&quot;,\\n                  code=\\&quot;df_dropped[&#39;Time&#39;].apply(lambda x:&#39;{:0&gt;4}&#39;.format(x))\\&quot;,\\n                  exact=False),\\n    has_equal_ast(\\&quot;You do not need to alter the provided code to pad leading zeros to the `&#39;Time&#39;` column.\\&quot;,\\n                           code=\\&quot;df_dropped.Time.apply(lambda x:&#39;{:0&gt;4}&#39;.format(x))\\&quot;,\\n                           exact=False))\\n\\nEx().check_correct(\\n\\tcheck_object(&#39;date_string&#39;).has_equal_value(),\\n  \\thas_equal_ast(code = &#39;df_dropped[\\\\&#39;date\\\\&#39;] + df_dropped[\\\\&#39;Time\\\\&#39;]&#39;, incorrect_msg = &#39;Did you concatenate the `\\\\&#39;date\\\\&#39;` and `\\\\&#39;time\\\\&#39;` columns of `df_dropped` to create `date_string`?&#39;)\\n)\\n\\nEx().check_correct(\\n\\tcheck_object(\\&quot;date_times\\&quot;).has_equal_value(),\\n\\tcheck_function(\\&quot;pandas.to_datetime\\&quot;).multi(\\n    \\tcheck_args(0).has_equal_value(),\\n      \\tcheck_args(&#39;format&#39;).has_equal_value()\\n    )\\n)\\n\\nEx().check_correct(\\n\\tcheck_object(\\&quot;df_clean\\&quot;).has_equal_value(),\\n    check_function(\\&quot;df_dropped.set_index\\&quot;).check_args(0).has_equal_value(\\&quot;The contents of `df_clean` are not correct. Did you set its index to be `date_times`?\\&quot;)\\n)\\n\\nEx().check_function(&#39;print&#39;).check_args(0).has_equal_value()\\n\\nsuccess_msg(\\&quot;Well done! All that&#39;s left now is to clean the numeric columns.\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Convert the &lt;code&gt;&#39;date&#39;&lt;/code&gt; column to a string with &lt;code&gt;.astype(str)&lt;/code&gt; and assign to &lt;code&gt;df_dropped[&#39;date&#39;]&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Add leading zeros to the &lt;code&gt;&#39;Time&#39;&lt;/code&gt; column. This has been done for you.&lt;/li&gt;\\n&lt;li&gt;Concatenate the new &lt;code&gt;&#39;date&#39;&lt;/code&gt; and &lt;code&gt;&#39;Time&#39;&lt;/code&gt; columns together. Assign to &lt;code&gt;date_string&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Convert the &lt;code&gt;date_string&lt;/code&gt; Series to datetime values with &lt;code&gt;pd.to_datetime()&lt;/code&gt;. Specify the &lt;code&gt;format&lt;/code&gt; parameter.&lt;/li&gt;\\n&lt;li&gt;Set the index of the &lt;code&gt;df_dropped&lt;/code&gt; DataFrame to be &lt;code&gt;date_times&lt;/code&gt;. Assign the result to &lt;code&gt;df_clean&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,47096,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;First, you need to access the &lt;code&gt;&#39;date&#39;&lt;/code&gt; column of &lt;code&gt;df_dropped&lt;/code&gt; with &lt;code&gt;df_dropped[&#39;date&#39;]&lt;/code&gt;. Then, you should call the &lt;code&gt;.astype(str)&lt;/code&gt; method on this to convert it to a string.&lt;/li&gt;\\n&lt;li&gt;To concatenate the new &lt;code&gt;&#39;date&#39;&lt;/code&gt; and &lt;code&gt;&#39;Time&#39;&lt;/code&gt; columns, first access them with &lt;code&gt;df_dropped[&#39;date&#39;]&lt;/code&gt; and &lt;code&gt;df_dropped[&#39;Time&#39;]&lt;/code&gt;. You can then concatenate them by adding them together.&lt;/li&gt;\\n&lt;li&gt;Inside &lt;code&gt;pd.to_datetime()&lt;/code&gt;, pass in the concatenated &lt;code&gt;date_string&lt;/code&gt; Series and specify the &lt;code&gt;format&lt;/code&gt; parameter to convert it to datetime.&lt;/li&gt;\\n&lt;li&gt;Use the &lt;code&gt;.set_index()&lt;/code&gt; method to set the index of &lt;code&gt;df_dropped&lt;/code&gt; to be &lt;code&gt;date_times&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1:&quot;,[]],&quot;number&quot;,5,&quot;randomNumber&quot;,0.2731044844596118,&quot;assignment&quot;,&quot;&lt;p&gt;In order to use the full power of pandas time series, you must construct a &lt;code&gt;DatetimeIndex&lt;/code&gt;. To do so, it is necessary to clean and transform the date and time columns. &lt;/p&gt;\\n&lt;p&gt;The DataFrame &lt;code&gt;df_dropped&lt;/code&gt; you created in the last exercise is provided for you and pandas has been imported as &lt;code&gt;pd&lt;/code&gt;.&lt;/p&gt;\\n&lt;p&gt;Your job is to clean up the &lt;code&gt;date&lt;/code&gt; and &lt;code&gt;Time&lt;/code&gt; columns and combine them into a datetime collection to be used as the Index.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1:&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Cleaning and tidying datetime data&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;#import pandas\\nimport pandas as pd\\n\\n# define the data file name to be read\\ndata_file = \\&quot;/usr/local/share/datasets/NOAA_QCLCD_2011_hourly_13904.csv\\&quot;\\n\\n# read the data file\\ndf = pd.read_csv(data_file, index_col=False, header=None)\\n\\n# define the column labels for the real data file from NOAA: column_labels\\ncolumn_labels = \\&quot;Wban,date,Time,StationType,sky_condition,sky_conditionFlag,visibility,visibilityFlag,wx_and_obst_to_vision,wx_and_obst_to_visionFlag,dry_bulb_faren,dry_bulb_farenFlag,dry_bulb_cel,dry_bulb_celFlag,wet_bulb_faren,wet_bulb_farenFlag,wet_bulb_cel,wet_bulb_celFlag,dew_point_faren,dew_point_farenFlag,dew_point_cel,dew_point_celFlag,relative_humidity,relative_humidityFlag,wind_speed,wind_speedFlag,wind_direction,wind_directionFlag,value_for_wind_character,value_for_wind_characterFlag,station_pressure,station_pressureFlag,pressure_tendency,pressure_tendencyFlag,presschange,presschangeFlag,sea_level_pressure,sea_level_pressureFlag,record_type,hourly_precip,hourly_precipFlag,altimeter,altimeterFlag\\&quot;\\n\\n# define the sub-set list of columns to drop\\nlist_to_drop = [ &#39;sky_conditionFlag&#39;, &#39;visibilityFlag&#39;, &#39;wx_and_obst_to_vision&#39;, &#39;wx_and_obst_to_visionFlag&#39;,\\n  &#39;dry_bulb_farenFlag&#39;, &#39;dry_bulb_celFlag&#39;, &#39;wet_bulb_farenFlag&#39;, &#39;wet_bulb_celFlag&#39;, &#39;dew_point_farenFlag&#39;,\\n  &#39;dew_point_celFlag&#39;, &#39;relative_humidityFlag&#39;, &#39;wind_speedFlag&#39;, &#39;wind_directionFlag&#39;, &#39;value_for_wind_character&#39;,\\n  &#39;value_for_wind_characterFlag&#39;, &#39;station_pressureFlag&#39;, &#39;pressure_tendencyFlag&#39;, &#39;pressure_tendency&#39;,\\n  &#39;presschange&#39;,&#39;presschangeFlag&#39;, &#39;sea_level_pressureFlag&#39;, &#39;hourly_precip&#39;,  &#39;hourly_precipFlag&#39;, &#39;altimeter&#39;,  &#39;record_type&#39;, &#39;altimeterFlag&#39;, &#39;junk&#39;]\\n\\n# split on the comma to create a list, then append one more element as the data files all have a trailing comma at the end of each line: column_labels_list\\ncolumn_labels_list = column_labels.split(\\&quot;,\\&quot;)\\ncolumn_labels_list.append(\\&quot;junk\\&quot;)\\n\\n# assign the new colum labels to the dataframe\\ndf.columns = column_labels_list\\n\\n# Drop all the column data that we don&#39;t need\\ndf_dropped = df.drop(list_to_drop, axis=&#39;columns&#39;)&quot;,&quot;solution&quot;,&quot;# Convert the date column to string: df_dropped[&#39;date&#39;]\\ndf_dropped[&#39;date&#39;] = df_dropped[&#39;date&#39;].astype(str)\\n\\n# Pad leading zeros to the Time column: df_dropped[&#39;Time&#39;]\\ndf_dropped[&#39;Time&#39;] = df_dropped[&#39;Time&#39;].apply(lambda x:&#39;{:0&gt;4}&#39;.format(x))\\n\\n# Concatenate the new date and Time columns: date_string\\ndate_string = df_dropped[&#39;date&#39;] + df_dropped[&#39;Time&#39;]\\n\\n# Convert the date_string Series to datetime: date_times\\ndate_times = pd.to_datetime(date_string, format=&#39;%Y%m%d%H%M&#39;)\\n\\n# Set the index to be the new date_times container: df_clean\\ndf_clean = df_dropped.set_index(date_times)\\n\\n# Print the output of df_clean.head()\\nprint(df_clean.head())&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,47096]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Print the dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\\nprint(df_clean.loc[____:____, ____])\\n\\n# Convert the dry_bulb_faren column to numeric values: df_clean[&#39;dry_bulb_faren&#39;]\\ndf_clean[&#39;dry_bulb_faren&#39;] = ____(df_clean[&#39;dry_bulb_faren&#39;], ____=____)\\n\\n# Print the transformed dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\\nprint(df_clean.____[____:____, ____])\\n\\n# Convert the wind_speed and dew_point_faren columns to numeric values\\ndf_clean[&#39;wind_speed&#39;] = pd.to_numeric(____, ____=____)\\ndf_clean[&#39;dew_point_faren&#39;] = pd.to_numeric(____, ____=____)&quot;,&quot;sct&quot;,&quot;Ex().check_function(\\&quot;print\\&quot;, index=0)\\n\\nEx().check_correct(\\n  \\tcheck_df(\\&quot;df_clean\\&quot;).check_keys(\\&quot;dry_bulb_faren\\&quot;).has_equal_value(),\\n    has_equal_ast(\\&quot;Did you use `pd.to_numeric()` to convert the `&#39;dry_bulb_faren&#39;` column of `df_clean` to floating-point values, specifying `errors=&#39;coerce&#39;`?\\&quot;,\\n                  code=\\&quot;pd.to_numeric(df_clean[&#39;dry_bulb_faren&#39;], errors=&#39;coerce&#39;)\\&quot;,\\n                  exact=False))\\n\\nEx().check_function(\\&quot;print\\&quot;, index=1)\\n\\nEx().check_correct(\\n\\tcheck_df(\\&quot;df_clean\\&quot;).check_keys(\\&quot;wind_speed\\&quot;).has_equal_value(),\\n    has_equal_ast(\\&quot;Did you use `pd.to_numeric()` to convert the `&#39;wind_speed&#39;` column of `df` to floating-point values, specifying `errors=&#39;coerce&#39;`?\\&quot;,\\n                  code=\\&quot;pd.to_numeric(df_clean[&#39;wind_speed&#39;], errors=&#39;coerce&#39;)\\&quot;,\\n                  exact=False)\\n)\\n\\nEx().check_correct(\\n  \\tcheck_df(\\&quot;df_clean\\&quot;).check_keys(\\&quot;dew_point_faren\\&quot;).has_equal_value(),\\n    has_equal_ast(\\&quot;Did you use `pd.to_numeric()` to convert the `&#39;dew_point_faren&#39;` column of `df_clean` to floating-point values, specifying `errors=&#39;coerce&#39;`?\\&quot;,\\n                  code=\\&quot;pd.to_numeric(df_clean[&#39;dew_point_faren&#39;], errors=&#39;coerce&#39;)\\&quot;,\\n                  exact=False))\\n\\nsuccess_msg(\\&quot;Excellent job! Now that your data are clean, you can begin with your exploratory analysis.\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Print the &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; temperature between 8 AM and 9 AM on June 20, 2011.&lt;/li&gt;\\n&lt;li&gt;Convert the &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; column to numeric values with &lt;code&gt;pd.to_numeric()&lt;/code&gt;. Specify &lt;code&gt;errors=&#39;coerce&#39;&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Print the transformed &lt;code&gt;dry_bulb_faren&lt;/code&gt; temperature between 8 AM and 9 AM on June 20, 2011.&lt;/li&gt;\\n&lt;li&gt;Convert the &lt;code&gt;&#39;wind_speed&#39;&lt;/code&gt; and &lt;code&gt;&#39;dew_point_faren&#39;&lt;/code&gt; columns to numeric values with &lt;code&gt;pd.to_numeric()&lt;/code&gt;. Again, specify &lt;code&gt;errors=&#39;coerce&#39;&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,70132,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Inside &lt;code&gt;df_clean.loc[]&lt;/code&gt;, you need to first use bracket slicing to select the time range &lt;code&gt;&#39;2011-6-20 8:00:00&#39;:&#39;2011-6-20 9:00:00&#39;&lt;/code&gt;. Then, you have to specify the column, which in this case, is &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;You can use &lt;code&gt;pd.to_numeric()&lt;/code&gt; to convert the column to numeric values. In addition to specifying the column to convert, don&#39;t forget to include the keyword argument &lt;code&gt;errors=&#39;coerce&#39;&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Follow the same steps as above to print the transformed &lt;code&gt;dry_bulb_faren&lt;/code&gt;, and then convert the &lt;code&gt;&#39;wind_speed&#39;&lt;/code&gt; and &lt;code&gt;&#39;dew_point_faren&#39;&lt;/code&gt; to numeric values.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1:&quot;,[]],&quot;number&quot;,6,&quot;user&quot;,[&quot;^0&quot;,[&quot;isHintShown&quot;,false,&quot;rstudio&quot;,[&quot;^0&quot;,[&quot;isReady&quot;,false,&quot;settings&quot;,[&quot;^0&quot;,[]],&quot;showHistory&quot;,false,&quot;cards&quot;,[&quot;^0&quot;,[&quot;messages&quot;,[&quot;^1:&quot;,[]],&quot;currentRow&quot;,0]]]],&quot;editorTabs&quot;,[&quot;^0&quot;,[&quot;files/script.py&quot;,[&quot;^0&quot;,[&quot;title&quot;,&quot;script.py&quot;,&quot;props&quot;,[&quot;^0&quot;,[&quot;active&quot;,true,&quot;isClosable&quot;,false,&quot;code&quot;,&quot;# Print the dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\\nprint(df_clean.loc[____:____, ____])\\n\\n# Convert the dry_bulb_faren column to numeric values: df_clean[&#39;dry_bulb_faren&#39;]\\ndf_clean[&#39;dry_bulb_faren&#39;] = ____(df_clean[&#39;dry_bulb_faren&#39;], ____=____)\\n\\n# Print the transformed dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\\nprint(df_clean.____[____:____, ____])\\n\\n# Convert the wind_speed and dew_point_faren columns to numeric values\\ndf_clean[&#39;wind_speed&#39;] = pd.to_numeric(____, ____=____)\\ndf_clean[&#39;dew_point_faren&#39;] = pd.to_numeric(____, ____=____)&quot;,&quot;extra&quot;,[&quot;^0&quot;,[]],&quot;resetCode&quot;,&quot;# Print the dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\\nprint(df_clean.loc[____:____, ____])\\n\\n# Convert the dry_bulb_faren column to numeric values: df_clean[&#39;dry_bulb_faren&#39;]\\ndf_clean[&#39;dry_bulb_faren&#39;] = ____(df_clean[&#39;dry_bulb_faren&#39;], ____=____)\\n\\n# Print the transformed dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\\nprint(df_clean.____[____:____, ____])\\n\\n# Convert the wind_speed and dew_point_faren columns to numeric values\\ndf_clean[&#39;wind_speed&#39;] = pd.to_numeric(____, ____=____)\\ndf_clean[&#39;dew_point_faren&#39;] = pd.to_numeric(____, ____=____)&quot;]]]]]],&quot;fileBrowser&quot;,[&quot;^0&quot;,[&quot;sampleCode&quot;,[&quot;^0&quot;,[&quot;fileSelected&quot;,null,&quot;files&quot;,[&quot;^0&quot;,[&quot;name&quot;,&quot;files&quot;,&quot;isOpen&quot;,true,&quot;children&quot;,[&quot;^1:&quot;,[[&quot;^0&quot;,[&quot;name&quot;,&quot;script.py&quot;,&quot;initialContent&quot;,&quot;# Print the dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\\nprint(df_clean.loc[____:____, ____])\\n\\n# Convert the dry_bulb_faren column to numeric values: df_clean[&#39;dry_bulb_faren&#39;]\\ndf_clean[&#39;dry_bulb_faren&#39;] = ____(df_clean[&#39;dry_bulb_faren&#39;], ____=____)\\n\\n# Print the transformed dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\\nprint(df_clean.____[____:____, ____])\\n\\n# Convert the wind_speed and dew_point_faren columns to numeric values\\ndf_clean[&#39;wind_speed&#39;] = pd.to_numeric(____, ____=____)\\ndf_clean[&#39;dew_point_faren&#39;] = pd.to_numeric(____, ____=____)&quot;,&quot;content&quot;,&quot;# Print the dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\\nprint(df_clean.loc[____:____, ____])\\n\\n# Convert the dry_bulb_faren column to numeric values: df_clean[&#39;dry_bulb_faren&#39;]\\ndf_clean[&#39;dry_bulb_faren&#39;] = ____(df_clean[&#39;dry_bulb_faren&#39;], ____=____)\\n\\n# Print the transformed dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\\nprint(df_clean.____[____:____, ____])\\n\\n# Convert the wind_speed and dew_point_faren columns to numeric values\\ndf_clean[&#39;wind_speed&#39;] = pd.to_numeric(____, ____=____)\\ndf_clean[&#39;dew_point_faren&#39;] = pd.to_numeric(____, ____=____)&quot;,&quot;isClosable&quot;,false]]]]]]]],&quot;solution&quot;,[&quot;^0&quot;,[&quot;fileSelected&quot;,null,&quot;files&quot;,[&quot;^0&quot;,[&quot;name&quot;,&quot;solution&quot;,&quot;isOpen&quot;,true,&quot;children&quot;,[&quot;^1:&quot;,[[&quot;^0&quot;,[&quot;name&quot;,&quot;solution.py&quot;,&quot;initialContent&quot;,&quot;# Print the dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\\nprint(df_clean.loc[&#39;2011-6-20 8:00:00&#39;:&#39;2011-6-20 9:00:00&#39;, &#39;dry_bulb_faren&#39;])\\n\\n# Convert the dry_bulb_faren column to numeric values: df_clean[&#39;dry_bulb_faren&#39;]\\ndf_clean[&#39;dry_bulb_faren&#39;] = pd.to_numeric(df_clean[&#39;dry_bulb_faren&#39;], errors=&#39;coerce&#39;)\\n\\n# Print the transformed dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\\nprint(df_clean.loc[&#39;2011-6-20 8:00:00&#39;:&#39;2011-6-20 9:00:00&#39;, &#39;dry_bulb_faren&#39;])\\n\\n# Convert the wind_speed and dew_point_faren columns to numeric values\\ndf_clean[&#39;wind_speed&#39;] = pd.to_numeric(df_clean[&#39;wind_speed&#39;], errors=&#39;coerce&#39;)\\ndf_clean[&#39;dew_point_faren&#39;] = pd.to_numeric(df_clean[&#39;dew_point_faren&#39;], errors=&#39;coerce&#39;)&quot;,&quot;content&quot;,&quot;# Print the dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\\nprint(df_clean.loc[&#39;2011-6-20 8:00:00&#39;:&#39;2011-6-20 9:00:00&#39;, &#39;dry_bulb_faren&#39;])\\n\\n# Convert the dry_bulb_faren column to numeric values: df_clean[&#39;dry_bulb_faren&#39;]\\ndf_clean[&#39;dry_bulb_faren&#39;] = pd.to_numeric(df_clean[&#39;dry_bulb_faren&#39;], errors=&#39;coerce&#39;)\\n\\n# Print the transformed dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\\nprint(df_clean.loc[&#39;2011-6-20 8:00:00&#39;:&#39;2011-6-20 9:00:00&#39;, &#39;dry_bulb_faren&#39;])\\n\\n# Convert the wind_speed and dew_point_faren columns to numeric values\\ndf_clean[&#39;wind_speed&#39;] = pd.to_numeric(df_clean[&#39;wind_speed&#39;], errors=&#39;coerce&#39;)\\ndf_clean[&#39;dew_point_faren&#39;] = pd.to_numeric(df_clean[&#39;dew_point_faren&#39;], errors=&#39;coerce&#39;)&quot;,&quot;isClosable&quot;,false]]]]]]]]]],&quot;outputMarkdownTabs&quot;,[&quot;^0&quot;,[]],&quot;markdown&quot;,[&quot;^0&quot;,[&quot;titles&quot;,[&quot;^1:&quot;,[&quot;Knit PDF&quot;,&quot;Knit HTML&quot;]],&quot;activeTitle&quot;,&quot;Knit HTML&quot;]],&quot;currentXp&quot;,100,&quot;graphicalTabs&quot;,[&quot;^0&quot;,[&quot;plot&quot;,[&quot;^0&quot;,[&quot;extraClass&quot;,&quot;animation--flash&quot;,&quot;title&quot;,&quot;Plots&quot;,&quot;props&quot;,[&quot;^0&quot;,[&quot;sources&quot;,[&quot;^1:&quot;,[]],&quot;currentIndex&quot;,0]],&quot;dimension&quot;,[&quot;^0&quot;,[&quot;isRealSize&quot;,false,&quot;width&quot;,1,&quot;height&quot;,1]]]],&quot;html&quot;,[&quot;^0&quot;,[&quot;extraClass&quot;,&quot;animation--flash&quot;,&quot;title&quot;,&quot;HTML Viewer&quot;,&quot;props&quot;,[&quot;^0&quot;,[&quot;sources&quot;,[&quot;^1:&quot;,[]],&quot;currentIndex&quot;,0]]]]]],&quot;feedbackMessages&quot;,[&quot;^1:&quot;,[]],&quot;lastSubmittedCode&quot;,null,&quot;ltiStatus&quot;,[&quot;^0&quot;,[]],&quot;lastSubmitActiveEditorTab&quot;,null,&quot;consoleSqlTabs&quot;,[&quot;^0&quot;,[&quot;query_result&quot;,[&quot;^0&quot;,[&quot;extraClass&quot;,&quot;&quot;,&quot;title&quot;,&quot;query result&quot;,&quot;props&quot;,[&quot;^0&quot;,[&quot;active&quot;,true,&quot;isNotView&quot;,true,&quot;message&quot;,&quot;No query executed yet...&quot;]]]]]],&quot;consoleTabs&quot;,[&quot;^0&quot;,[&quot;console&quot;,[&quot;^0&quot;,[&quot;title&quot;,&quot;IPython Shell&quot;,&quot;props&quot;,[&quot;^0&quot;,[&quot;active&quot;,true]]]],&quot;slides&quot;,[&quot;^0&quot;,[&quot;title&quot;,&quot;Slides&quot;,&quot;props&quot;,[&quot;^0&quot;,[&quot;active&quot;,false]]]]]],&quot;inputMarkdownTabs&quot;,[&quot;^0&quot;,[]]]],&quot;randomNumber&quot;,0.32052274374606715,&quot;assignment&quot;,&quot;&lt;p&gt;The numeric columns contain missing values labeled as &lt;code&gt;&#39;M&#39;&lt;/code&gt;. In this exercise, your job is to transform these columns such that they contain only numeric values and interpret missing data as &lt;code&gt;NaN&lt;/code&gt;.&lt;/p&gt;\\n&lt;p&gt;The pandas function &lt;code&gt;pd.to_numeric()&lt;/code&gt; is ideal for this purpose: It converts a Series of values to floating-point values. Furthermore, by specifying the keyword argument &lt;code&gt;errors=&#39;coerce&#39;&lt;/code&gt;, you can force strings like &lt;code&gt;&#39;M&#39;&lt;/code&gt; to be interpreted as &lt;code&gt;NaN&lt;/code&gt;. &lt;/p&gt;\\n&lt;p&gt;A DataFrame &lt;code&gt;df_clean&lt;/code&gt; is provided for you at the start of the exercise, and as usual, pandas has been imported as &lt;code&gt;pd&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1:&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Cleaning the numeric columns&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;import pandas as pd\\ndef read_data(file_name):\\n    # read the data file\\n    df = pd.read_csv(file_name, index_col=False, header=None)\\n\\n    # define the column labels for the real data file from NOAA: column_labels\\n    column_labels = \\&quot;Wban,date,Time,StationType,sky_condition,sky_conditionFlag,visibility,visibilityFlag,wx_and_obst_to_vision,wx_and_obst_to_visionFlag,dry_bulb_faren,dry_bulb_farenFlag,dry_bulb_cel,dry_bulb_celFlag,wet_bulb_faren,wet_bulb_farenFlag,wet_bulb_cel,wet_bulb_celFlag,dew_point_faren,dew_point_farenFlag,dew_point_cel,dew_point_celFlag,relative_humidity,relative_humidityFlag,wind_speed,wind_speedFlag,wind_direction,wind_directionFlag,value_for_wind_character,value_for_wind_characterFlag,station_pressure,station_pressureFlag,pressure_tendency,pressure_tendencyFlag,presschange,presschangeFlag,sea_level_pressure,sea_level_pressureFlag,record_type,hourly_precip,hourly_precipFlag,altimeter,altimeterFlag\\&quot;\\n    \\n    # define the sub-set list of columns to drop\\n    list_to_drop = [ &#39;sky_conditionFlag&#39;, &#39;visibilityFlag&#39;, &#39;wx_and_obst_to_vision&#39;, &#39;wx_and_obst_to_visionFlag&#39;,\\n      &#39;dry_bulb_farenFlag&#39;, &#39;dry_bulb_celFlag&#39;, &#39;wet_bulb_farenFlag&#39;, &#39;wet_bulb_celFlag&#39;, &#39;dew_point_farenFlag&#39;,\\n      &#39;dew_point_celFlag&#39;, &#39;relative_humidityFlag&#39;, &#39;wind_speedFlag&#39;, &#39;wind_directionFlag&#39;, &#39;value_for_wind_character&#39;,\\n      &#39;value_for_wind_characterFlag&#39;, &#39;station_pressureFlag&#39;, &#39;pressure_tendencyFlag&#39;, &#39;pressure_tendency&#39;,\\n      &#39;presschange&#39;,&#39;presschangeFlag&#39;, &#39;sea_level_pressureFlag&#39;, &#39;hourly_precip&#39;,  &#39;hourly_precipFlag&#39;, &#39;altimeter&#39;,  &#39;record_type&#39;, &#39;altimeterFlag&#39;, &#39;junk&#39;]\\n    \\n    # split on the comma to create a list, then append one more element as the data files all have a trailing comma at the end of each line: column_labels_list\\n    column_labels_list = column_labels.split(\\&quot;,\\&quot;)\\n    column_labels_list.append(\\&quot;junk\\&quot;)\\n    \\n    # assign the new colum labels to the dataframe\\n    df.columns = column_labels_list\\n    \\n    # Drop all the column data that we don&#39;t need\\n    df = df.drop(list_to_drop, axis=&#39;columns&#39;)\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with leading (left) zeros, for hours\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:0&gt;4}&#39;.format(x))\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with trailing (right) zeros, for seconds\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:&lt;06}&#39;.format(x))\\n    \\n    # Convert the `date` column to a string so it can be combined with the `Time` column\\n    df[&#39;date&#39;] = df[&#39;date&#39;].apply(str)\\n    \\n    # Create a new date-time container from the updated `date` and `Time` columns: date_times\\n    date_times = pd.to_datetime( df[&#39;date&#39;] + \\&quot; \\&quot; + df[&#39;Time&#39;], format=\\&quot;%Y%m%d %H%M%S\\&quot; )\\n    \\n    # Set the DataFrame index to this new `date_times` container:\\n    df.set_index(date_times, inplace=True)\\n\\n    return df\\ndata_file = \\&quot;/usr/local/share/datasets/NOAA_QCLCD_2011_hourly_13904.csv\\&quot;\\ndf_clean = read_data(data_file)&quot;,&quot;solution&quot;,&quot;# Print the dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\\nprint(df_clean.loc[&#39;2011-6-20 8:00:00&#39;:&#39;2011-6-20 9:00:00&#39;, &#39;dry_bulb_faren&#39;])\\n\\n# Convert the dry_bulb_faren column to numeric values: df_clean[&#39;dry_bulb_faren&#39;]\\ndf_clean[&#39;dry_bulb_faren&#39;] = pd.to_numeric(df_clean[&#39;dry_bulb_faren&#39;], errors=&#39;coerce&#39;)\\n\\n# Print the transformed dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\\nprint(df_clean.loc[&#39;2011-6-20 8:00:00&#39;:&#39;2011-6-20 9:00:00&#39;, &#39;dry_bulb_faren&#39;])\\n\\n# Convert the wind_speed and dew_point_faren columns to numeric values\\ndf_clean[&#39;wind_speed&#39;] = pd.to_numeric(df_clean[&#39;wind_speed&#39;], errors=&#39;coerce&#39;)\\ndf_clean[&#39;dew_point_faren&#39;] = pd.to_numeric(df_clean[&#39;dew_point_faren&#39;], errors=&#39;coerce&#39;)&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,70132]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;externalId&quot;,44659,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^1:&quot;,[]],&quot;number&quot;,7,&quot;video_hls&quot;,&quot;//videos.datacamp.com/transcoded/1639_pandas-1/v2/hls-ch4_2.master.m3u8&quot;,&quot;randomNumber&quot;,0.4979193849354817,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^1:&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Statistical exploratory data analysis&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,44659,&quot;projector_key&quot;,&quot;course_1639_3fc6cb80f71423139e0fdd1111fa115a&quot;,&quot;video_link&quot;,null]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Print the median of the dry_bulb_faren column\\nprint(____)\\n\\n# Print the median of the dry_bulb_faren column for the time range &#39;2011-Apr&#39;:&#39;2011-Jun&#39;\\nprint(df_clean.loc[____:____, &#39;dry_bulb_faren&#39;].____)\\n\\n# Print the median of the dry_bulb_faren column for the month of January\\nprint(df_clean.____[____, ____].____)&quot;,&quot;sct&quot;,&quot;Ex().check_function(\\&quot;print\\&quot;, index=0).check_args(0).has_equal_value(\\&quot;Did you apply the `.median()` method on `df_clean[&#39;dry_bulb_faren&#39;]` to compute its median?\\&quot;)\\n              \\nEx().check_function(\\&quot;print\\&quot;, index=1).check_args(0).has_equal_value(\\&quot;Did you apply the `.median()` method on the range `&#39;2011-Apr&#39;:&#39;2011-Jun&#39;` of the `&#39;dry_bulb_faren`&#39; column? \\&quot;)\\n\\nEx().check_function(\\&quot;print\\&quot;, index=2).check_args(0).has_equal_value(\\&quot;Did you select the month `&#39;2011-Jan&#39;` of `&#39;dry_bulb_faren&#39;` with `.loc()` and compute its median with `.median()`?\\&quot;)\\n              \\nsuccess_msg(\\&quot;Great work!\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Select the &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; column and print the output of &lt;code&gt;.median()&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Use &lt;code&gt;.loc[]&lt;/code&gt; to select the range &lt;code&gt;&#39;2011-Apr&#39;:&#39;2011-Jun&#39;&lt;/code&gt; from &lt;code&gt;dry_bulb_faren&#39;&lt;/code&gt; and print the output of &lt;code&gt;.median()&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Use &lt;code&gt;.loc[]&lt;/code&gt; to select the month &lt;code&gt;&#39;2011-Jan&#39;&lt;/code&gt; from &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; and print the output of &lt;code&gt;.median()&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,44660,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Use bracket slicing on &lt;code&gt;df_clean&lt;/code&gt; to select the &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; column. Then, you can use &lt;code&gt;.median()&lt;/code&gt; to compute the median. Make sure that all of this is inside a &lt;code&gt;print()&lt;/code&gt; function.&lt;/li&gt;\\n&lt;li&gt;Use bracket slicing inside &lt;code&gt;df_clean.loc[]&lt;/code&gt; to select the range &lt;code&gt;&#39;2011-Apr&#39;:&#39;2011-Jun&#39;&lt;/code&gt; from &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt;. Then, use &lt;code&gt;.median()&lt;/code&gt; as before to compute the median.&lt;/li&gt;\\n&lt;li&gt;Follow the same approach as above, this time selecting &lt;code&gt;&#39;2011-Jan&#39;&lt;/code&gt; and &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; with &lt;code&gt;df_clean.loc[]&lt;/code&gt;, and then using &lt;code&gt;.median()&lt;/code&gt; to compute the median.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1:&quot;,[]],&quot;number&quot;,8,&quot;randomNumber&quot;,0.25770543745189967,&quot;assignment&quot;,&quot;&lt;p&gt;Now that you have the data read and cleaned, you can begin with statistical EDA. First, you will analyze the 2011 Austin weather data. &lt;/p&gt;\\n&lt;p&gt;Your job in this exercise is to analyze the &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; column and print the median temperatures for specific time ranges. You can do this using &lt;em&gt;partial datetime string&lt;/em&gt; selection.&lt;/p&gt;\\n&lt;p&gt;The cleaned dataframe is provided in the workspace as &lt;code&gt;df_clean&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1:&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Signal min, max, median&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;# pec\\nimport pandas as pd\\n\\ndef read_data(file_name):\\n    # read the data file\\n    df = pd.read_csv(file_name, index_col=False, header=None)\\n\\n    # define the column labels for the real data file from NOAA: column_labels\\n    column_labels = \\&quot;Wban,date,Time,StationType,sky_condition,sky_conditionFlag,visibility,visibilityFlag,wx_and_obst_to_vision,wx_and_obst_to_visionFlag,dry_bulb_faren,dry_bulb_farenFlag,dry_bulb_cel,dry_bulb_celFlag,wet_bulb_faren,wet_bulb_farenFlag,wet_bulb_cel,wet_bulb_celFlag,dew_point_faren,dew_point_farenFlag,dew_point_cel,dew_point_celFlag,relative_humidity,relative_humidityFlag,wind_speed,wind_speedFlag,wind_direction,wind_directionFlag,value_for_wind_character,value_for_wind_characterFlag,station_pressure,station_pressureFlag,pressure_tendency,pressure_tendencyFlag,presschange,presschangeFlag,sea_level_pressure,sea_level_pressureFlag,record_type,hourly_precip,hourly_precipFlag,altimeter,altimeterFlag\\&quot;\\n    \\n    # define the sub-set list of columns to drop\\n    list_to_drop = [ &#39;sky_conditionFlag&#39;, &#39;visibilityFlag&#39;, &#39;wx_and_obst_to_vision&#39;, &#39;wx_and_obst_to_visionFlag&#39;,\\n      &#39;dry_bulb_farenFlag&#39;, &#39;dry_bulb_celFlag&#39;, &#39;wet_bulb_farenFlag&#39;, &#39;wet_bulb_celFlag&#39;, &#39;dew_point_farenFlag&#39;,\\n      &#39;dew_point_celFlag&#39;, &#39;relative_humidityFlag&#39;, &#39;wind_speedFlag&#39;, &#39;wind_directionFlag&#39;, &#39;value_for_wind_character&#39;,\\n      &#39;value_for_wind_characterFlag&#39;, &#39;station_pressureFlag&#39;, &#39;pressure_tendencyFlag&#39;, &#39;pressure_tendency&#39;,\\n      &#39;presschange&#39;,&#39;presschangeFlag&#39;, &#39;sea_level_pressureFlag&#39;, &#39;hourly_precip&#39;,  &#39;hourly_precipFlag&#39;, &#39;altimeter&#39;,  &#39;record_type&#39;, &#39;altimeterFlag&#39;, &#39;junk&#39;]\\n    \\n    # split on the comma to create a list, then append one more element as the data files all have a trailing comma at the end of each line: column_labels_list\\n    column_labels_list = column_labels.split(\\&quot;,\\&quot;)\\n    column_labels_list.append(\\&quot;junk\\&quot;)\\n    \\n    # assign the new colum labels to the dataframe\\n    df.columns = column_labels_list\\n    \\n    # Drop all the column data that we don&#39;t need\\n    df = df.drop(list_to_drop, axis=&#39;columns&#39;)\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with leading (left) zeros, for hours\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:0&gt;4}&#39;.format(x))\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with trailing (right) zeros, for seconds\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:&lt;06}&#39;.format(x))\\n    \\n    # Convert the `date` column to a string so it can be combined with the `Time` column\\n    df[&#39;date&#39;] = df[&#39;date&#39;].apply(str)\\n    \\n    # Create a new date-time container from the updated `date` and `Time` columns: date_times\\n    date_times = pd.to_datetime( df[&#39;date&#39;] + \\&quot; \\&quot; + df[&#39;Time&#39;], format=\\&quot;%Y%m%d %H%M%S\\&quot; )\\n    \\n    # Set the DataFrame index to this new `date_times` container:\\n    df.set_index(date_times, inplace=True)\\n\\n    for col in [&#39;wind_speed&#39;,&#39;dry_bulb_faren&#39;,&#39;dew_point_faren&#39;]:\\n        df[col] = pd.to_numeric(df[col], errors=&#39;coerce&#39;)\\n\\n    return df\\ndata_file = \\&quot;/usr/local/share/datasets/NOAA_QCLCD_2011_hourly_13904.csv\\&quot;\\ndf_clean = read_data(data_file)\\n&quot;,&quot;solution&quot;,&quot;# Print the median of the dry_bulb_faren column\\nprint(df_clean[&#39;dry_bulb_faren&#39;].median())\\n\\n# Print the median of the dry_bulb_faren column for the time range &#39;2011-Apr&#39;:&#39;2011-Jun&#39;\\nprint(df_clean.loc[&#39;2011-Apr&#39;:&#39;2011-Jun&#39;, &#39;dry_bulb_faren&#39;].median())\\n\\n# Print the median of the dry_bulb_faren column for the month of January\\nprint(df_clean.loc[&#39;2011-Jan&#39;, &#39;dry_bulb_faren&#39;].median())&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,44660]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Downsample df_clean by day and aggregate by mean: daily_mean_2011\\ndaily_mean_2011 = ____\\n\\n# Extract the dry_bulb_faren column from daily_mean_2011 using .values: daily_temp_2011\\ndaily_temp_2011 = ____\\n\\n# Downsample df_climate by day and aggregate by mean: daily_climate\\ndaily_climate = ____\\n\\n# Extract the Temperature column from daily_climate using .reset_index(): daily_temp_climate\\ndaily_temp_climate = ____\\n\\n# Compute the difference between the two arrays and print the mean difference\\ndifference = daily_temp_2011 - daily_temp_climate\\nprint(difference.mean())&quot;,&quot;sct&quot;,&quot;Ex().check_correct(\\n  \\tcheck_object(\\&quot;daily_mean_2011\\&quot;),\\n  \\thas_equal_ast(code = &#39;df_clean.resample(\\\\&#39;D\\\\&#39;).mean()&#39;, incorrect_msg = \\&quot;The contents of `&#39;daily_mean_2011&#39;` aren&#39;t correct. Did you correctly downsample `df_clean` to daily data and then call `.mean()` to calculate the mean?\\&quot;)\\n)\\n\\nEx().check_correct(\\n\\tcheck_object(\\&quot;daily_temp_2011\\&quot;).has_equal_value(),\\n  \\thas_equal_ast(code = &#39;daily_mean_2011[\\\\&#39;dry_bulb_faren\\\\&#39;].values&#39;, incorrect_msg = \\&quot;The contents of `&#39;daily_temp_2011&#39;` aren&#39;t correct. Did you use `.values` on the `&#39;dry_bulb_faren&#39;` column of `daily_mean_2011`?\\&quot;)\\n\\t\\n)\\n\\nEx().check_correct(\\n  \\tcheck_object(\\&quot;daily_climate\\&quot;),\\n  \\thas_equal_ast(code = &#39;df_climate.resample(\\\\&#39;D\\\\&#39;).mean()&#39;, incorrect_msg = \\&quot;The contents of `&#39;daily_climate&#39;` aren&#39;t correct. Did you correctly downsample `df_climate` to daily data and then call `.mean()` to calculate the mean?\\&quot;)\\n)\\n\\nEx().check_correct(\\n\\tcheck_object(\\&quot;daily_temp_climate\\&quot;).has_equal_value(),\\n  \\thas_equal_ast(code = &#39;daily_climate.reset_index()[\\\\&#39;Temperature\\\\&#39;]&#39;, incorrect_msg = &#39;Be sure you call `reset_index()` on `daily_climate` and then access the `\\\\&#39;Temperature\\\\&#39;` column using bracket indexing.&#39;)\\n)\\n\\nEx().check_correct(\\n\\tcheck_object(\\&quot;difference\\&quot;).has_equal_value(),\\n    has_equal_ast(code = &#39;daily_temp_2011 - daily_temp_climate&#39;, incorrect_msg = &#39;Did you subtract `daily_temp_climate` from `daily_temp_2011` to create `difference`?&#39;)\\n)\\n\\nEx().check_function(\\&quot;print\\&quot;).check_args(0).has_equal_value()\\n\\nsuccess_msg(\\&quot;Well done!\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Downsample &lt;code&gt;df_clean&lt;/code&gt; with &lt;em&gt;daily&lt;/em&gt; frequency and aggregate by the mean. Store the result as &lt;code&gt;daily_mean_2011&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Extract the &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; column from &lt;code&gt;daily_mean_2011&lt;/code&gt; as a NumPy array using &lt;code&gt;.values&lt;/code&gt;. Store the result as &lt;code&gt;daily_temp_2011&lt;/code&gt;. Note: &lt;code&gt;.values&lt;/code&gt; is an attribute, not a method, so you don&#39;t have to use &lt;code&gt;()&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Downsample &lt;code&gt;df_climate&lt;/code&gt;  with &lt;em&gt;daily&lt;/em&gt; frequency and aggregate by the mean. Store the result as &lt;code&gt;daily_climate&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Extract the &lt;code&gt;&#39;Temperature&#39;&lt;/code&gt; column from &lt;code&gt;daily_climate&lt;/code&gt; using the &lt;code&gt;.reset_index()&lt;/code&gt; method. To do this, first reset the index of &lt;code&gt;daily_climate&lt;/code&gt;, and then use bracket slicing to access &lt;code&gt;&#39;Temperature&#39;&lt;/code&gt;. Store the result as &lt;code&gt;daily_temp_climate&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,44661,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;You can downsample &lt;code&gt;df_clean&lt;/code&gt; with a daily frequency using &lt;code&gt;.resample()&lt;/code&gt; and specifying &lt;code&gt;&#39;D&#39;&lt;/code&gt;. To aggregate by the mean, chain &lt;code&gt;.mean()&lt;/code&gt; onto &lt;code&gt;.resample(&#39;D&#39;)&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;To extract the column as an array, first access it using bracket slicing. Then, access its &lt;code&gt;.values&lt;/code&gt; attribute.&lt;/li&gt;\\n&lt;li&gt;You can downsample &lt;code&gt;df_climate&lt;/code&gt; using the same approach as you did with &lt;code&gt;df_clean&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;First, you need to reset the index of &lt;code&gt;daily_climate&lt;/code&gt; using &lt;code&gt;.reset_index()&lt;/code&gt;. Then, access the &lt;code&gt;&#39;Temperature&#39;&lt;/code&gt; column using bracket slicing.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1:&quot;,[]],&quot;number&quot;,9,&quot;randomNumber&quot;,0.5318762346619463,&quot;assignment&quot;,&quot;&lt;p&gt;You&#39;re now ready to compare the 2011 weather data with the 30-year normals reported in 2010.\\nYou can ask questions such as, on average, how much hotter was every day in 2011 than expected from the 30-year average?&lt;/p&gt;\\n&lt;p&gt;The DataFrames &lt;code&gt;df_clean&lt;/code&gt; and &lt;code&gt;df_climate&lt;/code&gt; from previous exercises are available in the workspace.&lt;/p&gt;\\n&lt;p&gt;Your job is to first resample &lt;code&gt;df_clean&lt;/code&gt; and &lt;code&gt;df_climate&lt;/code&gt; by day and aggregate the mean temperatures. You will then extract the temperature related columns from each - &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; in &lt;code&gt;df_clean&lt;/code&gt;, and &lt;code&gt;&#39;Temperature&#39;&lt;/code&gt; in &lt;code&gt;df_climate&lt;/code&gt; - as NumPy arrays and compute the difference.&lt;/p&gt;\\n&lt;p&gt;Notice that the indexes of &lt;code&gt;df_clean&lt;/code&gt; and &lt;code&gt;df_climate&lt;/code&gt; are not aligned - &lt;code&gt;df_clean&lt;/code&gt; has dates in 2011, while &lt;code&gt;df_climate&lt;/code&gt; has dates in 2010. This is why you extract the temperature columns as NumPy arrays. An alternative approach is to use the pandas &lt;code&gt;.reset_index()&lt;/code&gt; method to make sure the Series align properly. You will practice this approach as well.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1:&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Signal variance&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;import pandas as pd\\n\\ndef read_data(file_name):\\n    # read the data file\\n    df = pd.read_csv(file_name, index_col=False, header=None)\\n\\n    # define the column labels for the real data file from NOAA: column_labels\\n    column_labels = \\&quot;Wban,date,Time,StationType,sky_condition,sky_conditionFlag,visibility,visibilityFlag,wx_and_obst_to_vision,wx_and_obst_to_visionFlag,dry_bulb_faren,dry_bulb_farenFlag,dry_bulb_cel,dry_bulb_celFlag,wet_bulb_faren,wet_bulb_farenFlag,wet_bulb_cel,wet_bulb_celFlag,dew_point_faren,dew_point_farenFlag,dew_point_cel,dew_point_celFlag,relative_humidity,relative_humidityFlag,wind_speed,wind_speedFlag,wind_direction,wind_directionFlag,value_for_wind_character,value_for_wind_characterFlag,station_pressure,station_pressureFlag,pressure_tendency,pressure_tendencyFlag,presschange,presschangeFlag,sea_level_pressure,sea_level_pressureFlag,record_type,hourly_precip,hourly_precipFlag,altimeter,altimeterFlag\\&quot;\\n    \\n    # define the sub-set list of columns to drop\\n    list_to_drop = [ &#39;sky_conditionFlag&#39;, &#39;visibilityFlag&#39;, &#39;wx_and_obst_to_vision&#39;, &#39;wx_and_obst_to_visionFlag&#39;,\\n      &#39;dry_bulb_farenFlag&#39;, &#39;dry_bulb_celFlag&#39;, &#39;wet_bulb_farenFlag&#39;, &#39;wet_bulb_celFlag&#39;, &#39;dew_point_farenFlag&#39;,\\n      &#39;dew_point_celFlag&#39;, &#39;relative_humidityFlag&#39;, &#39;wind_speedFlag&#39;, &#39;wind_directionFlag&#39;, &#39;value_for_wind_character&#39;,\\n      &#39;value_for_wind_characterFlag&#39;, &#39;station_pressureFlag&#39;, &#39;pressure_tendencyFlag&#39;, &#39;pressure_tendency&#39;,\\n      &#39;presschange&#39;,&#39;presschangeFlag&#39;, &#39;sea_level_pressureFlag&#39;, &#39;hourly_precip&#39;,  &#39;hourly_precipFlag&#39;, &#39;altimeter&#39;,  &#39;record_type&#39;, &#39;altimeterFlag&#39;, &#39;junk&#39;]\\n    \\n    # split on the comma to create a list, then append one more element as the data files all have a trailing comma at the end of each line: column_labels_list\\n    column_labels_list = column_labels.split(\\&quot;,\\&quot;)\\n    column_labels_list.append(\\&quot;junk\\&quot;)\\n    \\n    # assign the new colum labels to the dataframe\\n    df.columns = column_labels_list\\n    \\n    # Drop all the column data that we don&#39;t need\\n    df = df.drop(list_to_drop, axis=&#39;columns&#39;)\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with leading (left) zeros, for hours\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:0&gt;4}&#39;.format(x))\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with trailing (right) zeros, for seconds\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:&lt;06}&#39;.format(x))\\n    \\n    # Convert the `date` column to a string so it can be combined with the `Time` column\\n    df[&#39;date&#39;] = df[&#39;date&#39;].apply(str)\\n    \\n    # Create a new date-time container from the updated `date` and `Time` columns: date_times\\n    date_times = pd.to_datetime( df[&#39;date&#39;] + \\&quot; \\&quot; + df[&#39;Time&#39;], format=\\&quot;%Y%m%d %H%M%S\\&quot; )\\n    \\n    # Set the DataFrame index to this new `date_times` container:\\n    df.set_index(date_times, inplace=True)\\n\\n    for col in [&#39;wind_speed&#39;,&#39;dry_bulb_faren&#39;,&#39;dew_point_faren&#39;]:\\n        df[col] = pd.to_numeric(df[col], errors=&#39;coerce&#39;)\\n\\n    return df\\ndata_file = \\&quot;/usr/local/share/datasets/NOAA_QCLCD_2011_hourly_13904.csv\\&quot;\\ndf_clean = read_data(data_file)\\n\\nclimatology_data_file = \\&quot;/usr/local/share/datasets/weather_data_austin_2010.csv\\&quot;\\n\\ndf_climate = pd.read_csv(climatology_data_file, parse_dates=True, index_col=&#39;Date&#39;)&quot;,&quot;solution&quot;,&quot;# Downsample df_clean by day and aggregate by mean: daily_mean_2011\\ndaily_mean_2011 = df_clean.resample(&#39;D&#39;).mean()\\n\\n# Extract the dry_bulb_faren column from daily_mean_2011 using .values: daily_temp_2011\\ndaily_temp_2011 = daily_mean_2011[&#39;dry_bulb_faren&#39;].values\\n\\n# Downsample df_climate by day and aggregate by mean: daily_climate\\ndaily_climate = df_climate.resample(&#39;D&#39;).mean()\\n\\n# Extract the Temperature column from daily_climate using .reset_index(): daily_temp_climate\\ndaily_temp_climate = daily_climate.reset_index()[&#39;Temperature&#39;]\\n\\n# Compute the difference between the two arrays and print the mean difference\\ndifference = daily_temp_2011 - daily_temp_climate\\nprint(difference.mean())&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,44661]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;instructions&quot;,null,&quot;externalId&quot;,59225,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^1:&quot;,[]],&quot;number&quot;,10,&quot;randomNumber&quot;,0.25104653643763086,&quot;assignment&quot;,&quot;&lt;p&gt;On average, how much hotter is it when the sun is shining? In this exercise, you will compare temperatures on sunny days against temperatures on overcast days.&lt;/p&gt;\\n&lt;p&gt;Your job is to use Boolean selection to filter out sunny and overcast days, and then compute the difference of the mean daily maximum temperatures between each type of day.&lt;/p&gt;\\n&lt;p&gt;The DataFrame &lt;code&gt;df_clean&lt;/code&gt; from previous exercises has been provided for you. The column &lt;code&gt;&#39;sky_condition&#39;&lt;/code&gt; provides information about whether the day was sunny (&lt;code&gt;&#39;CLR&#39;&lt;/code&gt;) or overcast (&lt;code&gt;&#39;OVC&#39;&lt;/code&gt;).&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1:&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Sunny or cloudy&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;import pandas as pd\\n\\ndef read_data(file_name):\\n    # read the data file\\n    df = pd.read_csv(file_name, index_col=False, header=None)\\n\\n    # define the column labels for the real data file from NOAA: column_labels\\n    column_labels = \\&quot;Wban,date,Time,StationType,sky_condition,sky_conditionFlag,visibility,visibilityFlag,wx_and_obst_to_vision,wx_and_obst_to_visionFlag,dry_bulb_faren,dry_bulb_farenFlag,dry_bulb_cel,dry_bulb_celFlag,wet_bulb_faren,wet_bulb_farenFlag,wet_bulb_cel,wet_bulb_celFlag,dew_point_faren,dew_point_farenFlag,dew_point_cel,dew_point_celFlag,relative_humidity,relative_humidityFlag,wind_speed,wind_speedFlag,wind_direction,wind_directionFlag,value_for_wind_character,value_for_wind_characterFlag,station_pressure,station_pressureFlag,pressure_tendency,pressure_tendencyFlag,presschange,presschangeFlag,sea_level_pressure,sea_level_pressureFlag,record_type,hourly_precip,hourly_precipFlag,altimeter,altimeterFlag\\&quot;\\n    \\n    # define the sub-set list of columns to drop\\n    list_to_drop = [ &#39;sky_conditionFlag&#39;, &#39;visibilityFlag&#39;, &#39;wx_and_obst_to_vision&#39;, &#39;wx_and_obst_to_visionFlag&#39;,\\n      &#39;dry_bulb_farenFlag&#39;, &#39;dry_bulb_celFlag&#39;, &#39;wet_bulb_farenFlag&#39;, &#39;wet_bulb_celFlag&#39;, &#39;dew_point_farenFlag&#39;,\\n      &#39;dew_point_celFlag&#39;, &#39;relative_humidityFlag&#39;, &#39;wind_speedFlag&#39;, &#39;wind_directionFlag&#39;, &#39;value_for_wind_character&#39;,\\n      &#39;value_for_wind_characterFlag&#39;, &#39;station_pressureFlag&#39;, &#39;pressure_tendencyFlag&#39;, &#39;pressure_tendency&#39;,\\n      &#39;presschange&#39;,&#39;presschangeFlag&#39;, &#39;sea_level_pressureFlag&#39;, &#39;hourly_precip&#39;,  &#39;hourly_precipFlag&#39;, &#39;altimeter&#39;,  &#39;record_type&#39;, &#39;altimeterFlag&#39;, &#39;junk&#39;]\\n    \\n    # split on the comma to create a list, then append one more element as the data files all have a trailing comma at the end of each line: column_labels_list\\n    column_labels_list = column_labels.split(\\&quot;,\\&quot;)\\n    column_labels_list.append(\\&quot;junk\\&quot;)\\n    \\n    # assign the new colum labels to the dataframe\\n    df.columns = column_labels_list\\n    \\n    # Drop all the column data that we don&#39;t need\\n    df = df.drop(list_to_drop, axis=&#39;columns&#39;)\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with leading (left) zeros, for hours\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:0&gt;4}&#39;.format(x))\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with trailing (right) zeros, for seconds\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:&lt;06}&#39;.format(x))\\n    \\n    # Convert the `date` column to a string so it can be combined with the `Time` column\\n    df[&#39;date&#39;] = df[&#39;date&#39;].apply(str)\\n    \\n    # Create a new date-time container from the updated `date` and `Time` columns: date_times\\n    date_times = pd.to_datetime( df[&#39;date&#39;] + \\&quot; \\&quot; + df[&#39;Time&#39;], format=\\&quot;%Y%m%d %H%M%S\\&quot; )\\n    \\n    # Set the DataFrame index to this new `date_times` container:\\n    df.set_index(date_times, inplace=True)\\n\\n    for col in [&#39;wind_speed&#39;,&#39;dry_bulb_faren&#39;,&#39;dew_point_faren&#39;]:\\n        df[col] = pd.to_numeric(df[col], errors=&#39;coerce&#39;)\\n\\n    return df\\ndata_file = \\&quot;/usr/local/share/datasets/NOAA_QCLCD_2011_hourly_13904.csv\\&quot;\\ndf_clean = read_data(data_file)&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;TabExercise&quot;,&quot;id&quot;,59225,&quot;subexercises&quot;,[&quot;^1:&quot;,[[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Using df_clean, when is sky_condition &#39;CLR&#39;?\\nis_sky_clear = ____[&#39;____&#39;]==&#39;____&#39;\\n\\n# Filter df_clean using is_sky_clear\\nsunny = ____[____]\\n\\n# Resample sunny by day then calculate the max\\nsunny_daily_max = ____.____(&#39;____&#39;).____()\\n\\n# See the result\\nsunny_daily_max.head()&quot;,&quot;sct&quot;,&quot;Ex().check_correct(\\n    check_object(&#39;is_sky_clear&#39;).has_equal_value(),\\n    multi(\\n        has_code(\\&quot;df_clean\\\\[ *&#39;sky_condition&#39; *\\\\]\\&quot;, not_typed_msg = \\&quot;Did you select the `sky_condition` column with `df_clean[&#39;sky_condition&#39;]`?\\&quot;),\\n        has_code(\\&quot;\\\\] *== *&#39;CLR&#39;\\&quot;, not_typed_msg = \\&quot;Did you check for `sky_condition` equalling `&#39;CLR&#39;`?\\&quot;)\\n    )\\n)\\nEx().check_correct(\\n    check_object(&#39;sunny&#39;).has_equal_value(),\\n    has_equal_ast(code = &#39;df_clean.loc[is_sky_clear]&#39;, incorrect_msg = &#39;Did you filter `df_clean` using `.loc[]` and `is_sky_clear`?&#39;)\\n)\\nEx().check_correct(\\n    check_object(&#39;sunny_daily_max&#39;).has_equal_value(),\\n    multi(\\n        check_function(&#39;sunny.resample&#39;).check_args(0).has_equal_value(),\\n        check_function(&#39;sunny.resample.max&#39;, signature=False)\\n    )\\n)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Get the cases in &lt;code&gt;df_clean&lt;/code&gt; where the sky is clear. That is, when &lt;code&gt;&#39;sky_condition&#39;&lt;/code&gt; equals &lt;code&gt;&#39;CLR&#39;&lt;/code&gt;, assigning to &lt;code&gt;is_sky_clear&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Use &lt;code&gt;.loc[]&lt;/code&gt; to filter &lt;code&gt;df_clean&lt;/code&gt; by &lt;code&gt;is_sky_clear&lt;/code&gt;, assigning to &lt;code&gt;sunny&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Resample &lt;code&gt;sunny&lt;/code&gt; by day (&lt;code&gt;&#39;D&#39;&lt;/code&gt;), and take the max to find the maximum daily temperature.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;The logical condition takes the form &lt;code&gt;dataset[&#39;column&#39;]==&#39;value&#39;&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;The filtering command takes the form &lt;code&gt;dataset[filter_condition]&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Use &lt;code&gt;resample()&lt;/code&gt;, passing &lt;code&gt;&#39;D&#39;&lt;/code&gt; for daily, then take the &lt;code&gt;max()&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1:&quot;,[]],&quot;number&quot;,1,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^1:&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,null,&quot;xp&quot;,35,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Using df_clean, when is sky_condition &#39;CLR&#39;?\\nis_sky_clear = df_clean[&#39;sky_condition&#39;]==&#39;CLR&#39;\\n\\n# Filter df_clean using is_sky_clear\\nsunny = df_clean.loc[is_sky_clear]\\n\\n# Resample sunny by day then calculate the max\\nsunny_daily_max = sunny.resample(&#39;D&#39;).max()\\n\\n# See the result\\nsunny_daily_max.head()&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,624210]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Using df_clean, when does sky_condition contain &#39;OVC&#39;?\\nis_sky_overcast = ____[&#39;____&#39;].____.____(&#39;____&#39;)\\n\\n# Filter df_clean using is_sky_overcast\\novercast = ____\\n\\n# Resample overcast by day then calculate the max\\novercast_daily_max = ____\\n\\n# See the result\\novercast_daily_max.head()&quot;,&quot;sct&quot;,&quot;Ex().check_correct(\\n    check_object(&#39;is_sky_overcast&#39;).has_equal_value(),\\n    multi(\\n        has_code(\\&quot;df_clean\\\\[ *&#39;sky_condition&#39; *\\\\]\\&quot;, not_typed_msg = \\&quot;Did you select the `sky_condition` column with `df_clean[&#39;sky_condition&#39;]`?\\&quot;),\\n        has_code(\\&quot;\\\\] *== *&#39;OVC&#39;\\&quot;, not_typed_msg = \\&quot;Did you check for `sky_condition` equalling `&#39;OVC&#39;`?\\&quot;)\\n    )\\n)\\nEx().check_correct(\\n    check_object(&#39;overcast&#39;).has_equal_value(),\\n    has_equal_ast(code = &#39;df_clean.loc[is_sky_overcast]&#39;, incorrect_msg = &#39;Did you filter `df_clean` using `.loc[]` and `is_sky_overcast`?&#39;)\\n)\\nEx().check_correct(\\n    check_object(&#39;overcast_daily_max&#39;).has_equal_value(),\\n    multi(\\n        check_function(&#39;overcast.resample&#39;).check_args(0).has_equal_value(),\\n        check_function(&#39;overcast.resample.max&#39;, signature=False)\\n    )\\n)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Get the cases in &lt;code&gt;df_clean&lt;/code&gt; where the sky is overcast. Using &lt;code&gt;.str.contains()&lt;/code&gt;, find when &lt;code&gt;&#39;sky_condition&#39;&lt;/code&gt; contains &lt;code&gt;&#39;OVC&#39;&lt;/code&gt;, assigning to &lt;code&gt;is_sky_overcast&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Use &lt;code&gt;.loc[]&lt;/code&gt; to filter &lt;code&gt;df_clean&lt;/code&gt; by &lt;code&gt;is_sky_overcast&lt;/code&gt;, assigning to &lt;code&gt;overcast&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Resample &lt;code&gt;overcast&lt;/code&gt; by day (&lt;code&gt;&#39;D&#39;&lt;/code&gt;), and take the max to find the maximum daily temperature.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;The logical condition takes the form &lt;code&gt;dataset[&#39;column&#39;].str.contains(&#39;value&#39;)&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;The filtering command takes the form &lt;code&gt;dataset[filter_condition]&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Use &lt;code&gt;resample()&lt;/code&gt;, passing &lt;code&gt;&#39;D&#39;&lt;/code&gt; for daily, then take the &lt;code&gt;max()&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1:&quot;,[]],&quot;number&quot;,2,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^1:&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,null,&quot;xp&quot;,35,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Using df_clean, when does sky_condition contain &#39;OVC&#39;?\\nis_sky_overcast = df_clean[&#39;sky_condition&#39;].str.contains(&#39;OVC&#39;)\\n\\n# Filter df_clean using is_sky_overcast\\novercast = df_clean.loc[is_sky_overcast]\\n\\n# Resample overcast by day then calculate the max\\novercast_daily_max = overcast.resample(&#39;D&#39;).max()\\n\\n# See the result\\novercast_daily_max.head()&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,624211]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# From previous steps\\nis_sky_clear = df_clean[&#39;sky_condition&#39;]==&#39;CLR&#39;\\nsunny = df_clean.loc[is_sky_clear]\\nsunny_daily_max = sunny.resample(&#39;D&#39;).max()\\nis_sky_overcast = df_clean[&#39;sky_condition&#39;].str.contains(&#39;OVC&#39;)\\novercast = df_clean.loc[is_sky_overcast]\\novercast_daily_max = overcast.resample(&#39;D&#39;).max()\\n\\n# Calculate the mean of sunny_daily_max\\nsunny_daily_max_mean = ____\\n\\n# Calculate the mean of overcast_daily_max\\novercast_daily_max_mean = ____\\n\\n# Print the difference (sunny minus overcast)\\n____&quot;,&quot;sct&quot;,&quot;Ex().check_correct(\\n    check_object(&#39;sunny_daily_max_mean&#39;).has_equal_value(),\\n    check_function(&#39;sunny_daily_max.mean&#39;, signature=False)\\n)\\nEx().check_correct(\\n    check_object(&#39;overcast_daily_max_mean&#39;).has_equal_value(),\\n    check_function(&#39;overcast_daily_max.mean&#39;, signature=False)\\n)\\nEx().has_printout(0, not_printed_msg=\\&quot;Did you print `sunny_daily_max_mean` minus `overcast_daily_max_mean`?\\&quot;)\\nsuccess_msg(\\&quot;Terrific temperature computing! The average daily maximum dry bulb temperature was 6.5 degrees Fahrenheit higher on sunny days compared to overcast days.\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Calculate the mean of &lt;code&gt;sunny_daily_max&lt;/code&gt;, assigning to &lt;code&gt;sunny_daily_max_mean&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Calculate the mean of &lt;code&gt;overcast_daily_max&lt;/code&gt;, assigning to &lt;code&gt;overcast_daily_max_mean&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Print &lt;code&gt;sunny_daily_max_mean&lt;/code&gt; minus &lt;code&gt;overcast_daily_max_mean&lt;/code&gt;. &lt;em&gt;How much hotter are sunny days?&lt;/em&gt;&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Use &lt;code&gt;.mean()&lt;/code&gt; to calculate the mean of a value.&lt;/li&gt;\\n&lt;li&gt;The print command takes the form &lt;code&gt;print(x - y)&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1:&quot;,[]],&quot;number&quot;,3,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^1:&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,null,&quot;xp&quot;,30,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# From previous steps\\nis_sky_clear = df_clean[&#39;sky_condition&#39;]==&#39;CLR&#39;\\nsunny = df_clean.loc[is_sky_clear]\\nsunny_daily_max = sunny.resample(&#39;D&#39;).max()\\nis_sky_overcast = df_clean[&#39;sky_condition&#39;].str.contains(&#39;OVC&#39;)\\novercast = df_clean.loc[is_sky_overcast]\\novercast_daily_max = overcast.resample(&#39;D&#39;).max()\\n\\n# Calculate the mean of sunny_daily_max\\nsunny_daily_max_mean = sunny_daily_max.mean()\\n\\n# Calculate the mean of overcast_daily_max\\novercast_daily_max_mean = overcast_daily_max.mean()\\n\\n# Print the difference (sunny minus overcast)\\nprint(sunny_daily_max_mean - overcast_daily_max_mean)&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,624212]]]]]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;externalId&quot;,44663,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^1:&quot;,[]],&quot;number&quot;,11,&quot;video_hls&quot;,&quot;//videos.datacamp.com/transcoded/1639_pandas-1/v2/hls-ch4_3.master.m3u8&quot;,&quot;randomNumber&quot;,0.885095036502584,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^1:&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Visual exploratory data analysis&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,44663,&quot;projector_key&quot;,&quot;course_1639_9f7496c6bcaa543b2cfa9d8c4410aff2&quot;,&quot;video_link&quot;,null]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Import matplotlib.pyplot as plt\\n\\n\\n# Select the visibility and dry_bulb_faren columns and resample them: weekly_mean\\nweekly_mean = ____\\n\\n# Print the output of weekly_mean.corr()\\nprint(____)\\n\\n# Plot weekly_mean with subplots=True\\nweekly_mean.____(____=____)\\nplt.show()&quot;,&quot;sct&quot;,&quot;Ex().has_import(\\&quot;matplotlib.pyplot\\&quot;)\\n\\nEx().check_correct(\\n  \\tcheck_object(\\&quot;weekly_mean\\&quot;).has_equal_value(),\\n    has_equal_ast(\\&quot;It seems like you did not resample the data correctly. Did you select the `&#39;visibility&#39;` and `&#39;dry_bulb_faren&#39;` columns, then use `.resample(&#39;W&#39;)` along with `.mean()`?\\&quot;,\\n                                code=\\&quot;df_clean[[&#39;visibility&#39;,&#39;dry_bulb_faren&#39;]].resample(&#39;W&#39;).mean()\\&quot;,\\n                                exact=False))\\n\\nEx().check_function(\\&quot;print\\&quot;).check_args(0).has_equal_value()\\n\\nEx().check_function(\\&quot;weekly_mean.plot\\&quot;)\\nEx().check_function(\\&quot;matplotlib.pyplot.show\\&quot;)\\n\\nsuccess_msg(\\&quot;Great work!\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Import &lt;code&gt;matplotlib.pyplot&lt;/code&gt; as &lt;code&gt;plt&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Select the &lt;code&gt;&#39;visibility&#39;&lt;/code&gt; and &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; columns and resample them by week, aggregating the mean. Assign the result to &lt;code&gt;weekly_mean&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Print the output of &lt;code&gt;weekly_mean.corr()&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Plot the &lt;code&gt;weekly_mean&lt;/code&gt; dataframe with &lt;code&gt;.plot()&lt;/code&gt;, specifying &lt;code&gt;subplots=True&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,44664,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;You can import x from y using the command &lt;code&gt;from y import x&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;To select &lt;code&gt;&#39;visibility&#39;&lt;/code&gt; and &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt;, use bracket slicing and pass them in as a list inside &lt;code&gt;df_clean[]&lt;/code&gt;. Then, use &lt;code&gt;.resample()&lt;/code&gt; with &lt;code&gt;&#39;W&#39;&lt;/code&gt; to resample by week, and chain on the &lt;code&gt;.mean()&lt;/code&gt; method. &lt;/li&gt;\\n&lt;li&gt;Compute the correlation of &lt;code&gt;weekly_mean&lt;/code&gt; with &lt;code&gt;.corr()&lt;/code&gt;, and print it by passing it to the provided &lt;code&gt;print()&lt;/code&gt; function.&lt;/li&gt;\\n&lt;li&gt;Use the &lt;code&gt;.plot()&lt;/code&gt; with the keyword argument &lt;code&gt;subplots=True&lt;/code&gt; to plot the weekly average temperature and visibility as subplots.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1:&quot;,[]],&quot;number&quot;,12,&quot;randomNumber&quot;,0.7042982793996471,&quot;assignment&quot;,&quot;&lt;p&gt;Is there a correlation between temperature and visibility? Let&#39;s find out. &lt;/p&gt;\\n&lt;p&gt;In this exercise, your job is to plot the weekly average temperature and visibility as subplots. To do this, you need to first select the appropriate columns and then resample by week, aggregating the mean. &lt;/p&gt;\\n&lt;p&gt;In addition to creating the subplots, you will compute the Pearson correlation coefficient using &lt;code&gt;.corr()&lt;/code&gt;. The Pearson correlation coefficient, known also as Pearson&#39;s r, ranges from -1 (indicating total negative linear correlation) to 1 (indicating total positive linear correlation). A value close to 1 here would indicate that there is a strong correlation between temperature and visibility.&lt;/p&gt;\\n&lt;p&gt;The DataFrame &lt;code&gt;df_clean&lt;/code&gt; has been pre-loaded for you.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1:&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Weekly average temperature and visibility&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;# pec\\nimport pandas as pd\\n\\ndef read_data(file_name):\\n    # read the data file\\n    df = pd.read_csv(file_name, index_col=False, header=None)\\n\\n    # define the column labels for the real data file from NOAA: column_labels\\n    column_labels = \\&quot;Wban,date,Time,StationType,sky_condition,sky_conditionFlag,visibility,visibilityFlag,wx_and_obst_to_vision,wx_and_obst_to_visionFlag,dry_bulb_faren,dry_bulb_farenFlag,dry_bulb_cel,dry_bulb_celFlag,wet_bulb_faren,wet_bulb_farenFlag,wet_bulb_cel,wet_bulb_celFlag,dew_point_faren,dew_point_farenFlag,dew_point_cel,dew_point_celFlag,relative_humidity,relative_humidityFlag,wind_speed,wind_speedFlag,wind_direction,wind_directionFlag,value_for_wind_character,value_for_wind_characterFlag,station_pressure,station_pressureFlag,pressure_tendency,pressure_tendencyFlag,presschange,presschangeFlag,sea_level_pressure,sea_level_pressureFlag,record_type,hourly_precip,hourly_precipFlag,altimeter,altimeterFlag\\&quot;\\n    \\n    # define the sub-set list of columns to drop\\n    list_to_drop = [ &#39;sky_conditionFlag&#39;, &#39;visibilityFlag&#39;, &#39;wx_and_obst_to_vision&#39;, &#39;wx_and_obst_to_visionFlag&#39;,\\n      &#39;dry_bulb_farenFlag&#39;, &#39;dry_bulb_celFlag&#39;, &#39;wet_bulb_farenFlag&#39;, &#39;wet_bulb_celFlag&#39;, &#39;dew_point_farenFlag&#39;,\\n      &#39;dew_point_celFlag&#39;, &#39;relative_humidityFlag&#39;, &#39;wind_speedFlag&#39;, &#39;wind_directionFlag&#39;, &#39;value_for_wind_character&#39;,\\n      &#39;value_for_wind_characterFlag&#39;, &#39;station_pressureFlag&#39;, &#39;pressure_tendencyFlag&#39;, &#39;pressure_tendency&#39;,\\n      &#39;presschange&#39;,&#39;presschangeFlag&#39;, &#39;sea_level_pressureFlag&#39;, &#39;hourly_precip&#39;,  &#39;hourly_precipFlag&#39;, &#39;altimeter&#39;,  &#39;record_type&#39;, &#39;altimeterFlag&#39;, &#39;junk&#39;]\\n    \\n    # split on the comma to create a list, then append one more element as the data files all have a trailing comma at the end of each line: column_labels_list\\n    column_labels_list = column_labels.split(\\&quot;,\\&quot;)\\n    column_labels_list.append(\\&quot;junk\\&quot;)\\n    \\n    # assign the new colum labels to the dataframe\\n    df.columns = column_labels_list\\n    \\n    # Drop all the column data that we don&#39;t need\\n    df = df.drop(list_to_drop, axis=&#39;columns&#39;)\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with leading (left) zeros, for hours\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:0&gt;4}&#39;.format(x))\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with trailing (right) zeros, for seconds\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:&lt;06}&#39;.format(x))\\n    \\n    # Convert the `date` column to a string so it can be combined with the `Time` column\\n    df[&#39;date&#39;] = df[&#39;date&#39;].apply(str)\\n    \\n    # Create a new date-time container from the updated `date` and `Time` columns: date_times\\n    date_times = pd.to_datetime( df[&#39;date&#39;] + \\&quot; \\&quot; + df[&#39;Time&#39;], format=\\&quot;%Y%m%d %H%M%S\\&quot; )\\n    \\n    # Set the DataFrame index to this new `date_times` container:\\n    df.set_index(date_times, inplace=True)\\n\\n    for col in [&#39;wind_speed&#39;,&#39;dry_bulb_faren&#39;,&#39;dew_point_faren&#39;]:\\n        df[col] = pd.to_numeric(df[col], errors=&#39;coerce&#39;)\\n\\n    return df\\ndata_file = \\&quot;/usr/local/share/datasets/NOAA_QCLCD_2011_hourly_13904.csv\\&quot;\\ndf_clean = read_data(data_file)\\ndf_clean[&#39;visibility&#39;] = pd.to_numeric(df_clean[&#39;visibility&#39;], errors=&#39;coerce&#39;)&quot;,&quot;solution&quot;,&quot;# Import matplotlib.pyplot as plt\\nimport matplotlib.pyplot as plt\\n\\n# Select the visibility and dry_bulb_faren columns and resample them: weekly_mean\\nweekly_mean = df_clean[[&#39;visibility&#39;,&#39;dry_bulb_faren&#39;]].resample(&#39;W&#39;).mean()\\n\\n# Print the output of weekly_mean.corr()\\nprint(weekly_mean.corr())\\n\\n# Plot weekly_mean with subplots=True\\nweekly_mean.plot(subplots=True)\\nplt.show()&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,44664]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;instructions&quot;,null,&quot;externalId&quot;,44665,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^1:&quot;,[]],&quot;number&quot;,13,&quot;randomNumber&quot;,0.9794599945909943,&quot;assignment&quot;,&quot;&lt;p&gt;In a previous exercise, you analyzed the &lt;code&gt;&#39;sky_condition&#39;&lt;/code&gt; column to explore the difference in temperature on sunny days compared to overcast days. Recall that a &lt;code&gt;&#39;sky_condition&#39;&lt;/code&gt; of &lt;code&gt;&#39;CLR&#39;&lt;/code&gt; represents a sunny day. In this exercise, you will explore sunny days in greater detail. Specifically, you will use a box plot to visualize the fraction of days that are sunny.&lt;/p&gt;\\n&lt;p&gt;The &lt;code&gt;&#39;sky_condition&#39;&lt;/code&gt; column is recorded hourly. Your job is to resample this column appropriately such that you can extract the number of sunny hours in a day and the number of total hours. Then, you can divide the number of sunny hours by the number of total hours, and generate a box plot of the resulting fraction.&lt;/p&gt;\\n&lt;p&gt;As before, &lt;code&gt;df_clean&lt;/code&gt; is available for you in the workspace.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1:&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Daily hours of clear sky&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;import pandas as pd\\n\\ndef read_data(file_name):\\n    # read the data file\\n    df = pd.read_csv(file_name, index_col=False, header=None)\\n\\n    # define the column labels for the real data file from NOAA: column_labels\\n    column_labels = \\&quot;Wban,date,Time,StationType,sky_condition,sky_conditionFlag,visibility,visibilityFlag,wx_and_obst_to_vision,wx_and_obst_to_visionFlag,dry_bulb_faren,dry_bulb_farenFlag,dry_bulb_cel,dry_bulb_celFlag,wet_bulb_faren,wet_bulb_farenFlag,wet_bulb_cel,wet_bulb_celFlag,dew_point_faren,dew_point_farenFlag,dew_point_cel,dew_point_celFlag,relative_humidity,relative_humidityFlag,wind_speed,wind_speedFlag,wind_direction,wind_directionFlag,value_for_wind_character,value_for_wind_characterFlag,station_pressure,station_pressureFlag,pressure_tendency,pressure_tendencyFlag,presschange,presschangeFlag,sea_level_pressure,sea_level_pressureFlag,record_type,hourly_precip,hourly_precipFlag,altimeter,altimeterFlag\\&quot;\\n    \\n    # define the sub-set list of columns to drop\\n    list_to_drop = [ &#39;sky_conditionFlag&#39;, &#39;visibilityFlag&#39;, &#39;wx_and_obst_to_vision&#39;, &#39;wx_and_obst_to_visionFlag&#39;,\\n      &#39;dry_bulb_farenFlag&#39;, &#39;dry_bulb_celFlag&#39;, &#39;wet_bulb_farenFlag&#39;, &#39;wet_bulb_celFlag&#39;, &#39;dew_point_farenFlag&#39;,\\n      &#39;dew_point_celFlag&#39;, &#39;relative_humidityFlag&#39;, &#39;wind_speedFlag&#39;, &#39;wind_directionFlag&#39;, &#39;value_for_wind_character&#39;,\\n      &#39;value_for_wind_characterFlag&#39;, &#39;station_pressureFlag&#39;, &#39;pressure_tendencyFlag&#39;, &#39;pressure_tendency&#39;,\\n      &#39;presschange&#39;,&#39;presschangeFlag&#39;, &#39;sea_level_pressureFlag&#39;, &#39;hourly_precip&#39;,  &#39;hourly_precipFlag&#39;, &#39;altimeter&#39;,  &#39;record_type&#39;, &#39;altimeterFlag&#39;, &#39;junk&#39;]\\n    \\n    # split on the comma to create a list, then append one more element as the data files all have a trailing comma at the end of each line: column_labels_list\\n    column_labels_list = column_labels.split(\\&quot;,\\&quot;)\\n    column_labels_list.append(\\&quot;junk\\&quot;)\\n    \\n    # assign the new colum labels to the dataframe\\n    df.columns = column_labels_list\\n    \\n    # Drop all the column data that we don&#39;t need\\n    df = df.drop(list_to_drop, axis=&#39;columns&#39;)\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with leading (left) zeros, for hours\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:0&gt;4}&#39;.format(x))\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with trailing (right) zeros, for seconds\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:&lt;06}&#39;.format(x))\\n    \\n    # Convert the `date` column to a string so it can be combined with the `Time` column\\n    df[&#39;date&#39;] = df[&#39;date&#39;].apply(str)\\n    \\n    # Create a new date-time container from the updated `date` and `Time` columns: date_times\\n    date_times = pd.to_datetime( df[&#39;date&#39;] + \\&quot; \\&quot; + df[&#39;Time&#39;], format=\\&quot;%Y%m%d %H%M%S\\&quot; )\\n    \\n    # Set the DataFrame index to this new `date_times` container:\\n    df.set_index(date_times, inplace=True)\\n\\n    for col in [&#39;wind_speed&#39;,&#39;dry_bulb_faren&#39;,&#39;dew_point_faren&#39;]:\\n        df[col] = pd.to_numeric(df[col], errors=&#39;coerce&#39;)\\n\\n    return df\\ndata_file = \\&quot;/usr/local/share/datasets/NOAA_QCLCD_2011_hourly_13904.csv\\&quot;\\ndf_clean = read_data(data_file)\\n\\n\\n# Import matplotlib.pyplot\\nimport matplotlib.pyplot as plt&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;TabExercise&quot;,&quot;id&quot;,44665,&quot;subexercises&quot;,[&quot;^1:&quot;,[[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Using df_clean, when is sky_condition &#39;CLR&#39;?\\nis_sky_clear = ____\\n\\n# Resample is_sky_clear by day\\nresampled = ____\\n\\n# See the result\\nresampled&quot;,&quot;sct&quot;,&quot;Ex().check_correct(\\n    check_object(&#39;is_sky_clear&#39;).has_equal_value(),\\n    multi(\\n        has_code(\\&quot;df_clean\\\\[ *&#39;sky_condition&#39; *\\\\]\\&quot;, not_typed_msg = \\&quot;Did you select the `sky_condition` column with `df_clean[&#39;sky_condition&#39;]`?\\&quot;),\\n        has_code(\\&quot;\\\\] *== *&#39;CLR&#39;\\&quot;, not_typed_msg = \\&quot;Did you check for `sky_condition` equalling `&#39;CLR&#39;`?\\&quot;)\\n    )\\n)\\nEx().multi(\\n    check_object(&#39;resampled&#39;),\\n    check_function(&#39;is_sky_clear.resample&#39;).check_args(0).has_equal_value()\\n)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Get the cases in &lt;code&gt;df_clean&lt;/code&gt; where the sky is clear. That is, when &lt;code&gt;&#39;sky_condition&#39;&lt;/code&gt; equals &lt;code&gt;&#39;CLR&#39;&lt;/code&gt;, assigning to &lt;code&gt;is_sky_clear&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Resample &lt;code&gt;is_sky_clear&lt;/code&gt; by day, assigning to &lt;code&gt;resampled&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;The logical condition takes the form &lt;code&gt;dataset[&#39;column&#39;]==&#39;value&#39;&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Use the &lt;code&gt;.resample()&lt;/code&gt; method with &lt;code&gt;&#39;D&#39;&lt;/code&gt; for daily.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1:&quot;,[]],&quot;number&quot;,1,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^1:&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,null,&quot;xp&quot;,35,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Using df_clean, when is sky_condition &#39;CLR&#39;?\\nis_sky_clear = df_clean[&#39;sky_condition&#39;] == &#39;CLR&#39;\\n\\n# Resample is_sky_clear by day\\nresampled = is_sky_clear.resample(&#39;D&#39;)\\n\\n# See the result\\nresampled&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,624213]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# From previous step\\nis_sky_clear = df_clean[&#39;sky_condition&#39;] == &#39;CLR&#39;\\nresampled = is_sky_clear.resample(&#39;D&#39;)\\n\\n# Calculate the number of sunny hours per day\\nsunny_hours = ____\\n\\n# Calculate the number of measured hours per day\\ntotal_hours = ____\\n\\n# Calculate the fraction of hours per day that were sunny\\nsunny_fraction = ____&quot;,&quot;sct&quot;,&quot;Ex().check_correct(\\n    check_object(&#39;sunny_hours&#39;).has_equal_value(),\\n    check_function(&#39;resampled.sum&#39;, signature=False)\\n)\\nEx().check_correct(\\n    check_object(&#39;total_hours&#39;).has_equal_value(),\\n    check_function(&#39;resampled.count&#39;, signature=False)\\n)\\nEx().check_correct(\\n    check_object(&#39;sunny_fraction&#39;).has_equal_value(),\\n    has_code(text=&#39;sunny_hours / total_hours&#39;, not_typed_msg=\\&quot;Did you divide `sunny_hours` by `total_hours`?\\&quot;)\\n)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Calculate the number of measured sunny hours per day as the sum of &lt;code&gt;resampled&lt;/code&gt;, assigning to &lt;code&gt;sunny_hours&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Calculate the total number of measured hours per day as the count of &lt;code&gt;resampled&lt;/code&gt;, assigning to &lt;code&gt;total_hours&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Calculate the fraction of hours per day that were sunny as the ratio of sunny hours to total hours.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Call &lt;code&gt;.sum()&lt;/code&gt;, then &lt;code&gt;.count()&lt;/code&gt; on &lt;code&gt;resampled&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;The fraction on sunny days is &lt;code&gt;sunny_hours&lt;/code&gt; divided by &lt;code&gt;total_hours&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1:&quot;,[]],&quot;number&quot;,2,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^1:&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,null,&quot;xp&quot;,35,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# From previous step\\nis_sky_clear = df_clean[&#39;sky_condition&#39;] == &#39;CLR&#39;\\nresampled = is_sky_clear.resample(&#39;D&#39;)\\n\\n# Calculate the number of sunny hours per day\\nsunny_hours = resampled.sum()\\n\\n# Calculate the number of measured hours per day\\ntotal_hours = resampled.count()\\n\\n# Calculate the fraction of hours per day that were sunny\\nsunny_fraction = sunny_hours / total_hours&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,624214]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# From previous steps\\nis_sky_clear = df_clean[&#39;sky_condition&#39;] == &#39;CLR&#39;\\nresampled = is_sky_clear.resample(&#39;D&#39;)\\nsunny_hours = resampled.sum()\\ntotal_hours = resampled.count()\\nsunny_fraction = sunny_hours / total_hours\\n\\n# Make a box plot of sunny_fraction\\n____.____(kind=&#39;____&#39;)\\nplt.show()&quot;,&quot;sct&quot;,&quot;Ex().check_function(\\&quot;sunny_fraction.plot\\&quot;).check_args(&#39;kind&#39;).has_equal_value(copy=False)\\nEx().check_function(\\&quot;matplotlib.pyplot.show\\&quot;)\\nsuccess_msg(\\&quot;The outlook for your Pandas skills is sunny! By contrast, the weather in the dataset is typically sunny less than 40% of the time.\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Draw a box plot of &lt;code&gt;sunny_fraction&lt;/code&gt; using &lt;code&gt;.plot()&lt;/code&gt; with &lt;code&gt;kind&lt;/code&gt; set to `&#39;box&#39;``.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&quot;,&quot;possible_answers&quot;,[&quot;^1:&quot;,[]],&quot;number&quot;,3,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^1:&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,null,&quot;xp&quot;,30,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# From previous steps\\nis_sky_clear = df_clean[&#39;sky_condition&#39;] == &#39;CLR&#39;\\nresampled = is_sky_clear.resample(&#39;D&#39;)\\nsunny_hours = resampled.sum()\\ntotal_hours = resampled.count()\\nsunny_fraction = sunny_hours / total_hours\\n\\n# Make a box plot of sunny_fraction\\nsunny_fraction.plot(kind=&#39;box&#39;)\\nplt.show()&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,624215]]]]]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Resample dew_point_faren and dry_bulb_faren by Month, aggregating the maximum values: monthly_max\\nmonthly_max = ____\\n\\n# Generate a histogram with bins=8, alpha=0.5, subplots=True\\n____\\n\\n# Show the plot\\nplt.show()&quot;,&quot;sct&quot;,&quot;Ex().check_correct(\\n  \\tcheck_object(\\&quot;monthly_max\\&quot;).has_equal_value(\\&quot;The contents of `monthly_max` are not correct. It seems like you did not resample the data correctly. Did you place the two column names in a list?\\&quot;),\\n    has_equal_ast(\\&quot;It seems like you did not resample the data correctly. Did you select the `&#39;dew_point_faren&#39;` and `&#39;dry_bulb_faren&#39;` columns, then use `.resample(&#39;M&#39;)` along with `.max()`?\\&quot;,\\n                                code=\\&quot;df_clean[[&#39;dew_point_faren&#39;,&#39;dry_bulb_faren&#39;]].resample(&#39;M&#39;).max()\\&quot;,\\n                                exact=False))\\n\\nEx().check_function(&#39;monthly_max.plot&#39;)\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.show\\&quot;)\\n\\nsuccess_msg(\\&quot;Excellent job!\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Select the &lt;code&gt;&#39;dew_point_faren&#39;&lt;/code&gt; and &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; columns (in that order). Resample by month and aggregate the maximum monthly temperatures. Assign the result to &lt;code&gt;monthly_max&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Plot a histogram of the resampled data with &lt;code&gt;bins=8&lt;/code&gt;, &lt;code&gt;alpha=0.5&lt;/code&gt;, and &lt;code&gt;subplots=True&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,44666,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;To select the two columns, pass them in as a list inside &lt;code&gt;df.clean[]&lt;/code&gt;. Once you&#39;ve done this, you can resample by month using &lt;code&gt;.resample(&#39;M&#39;)&lt;/code&gt; and aggregate the maximum monthly temperatures with &lt;code&gt;.max()&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;To generate the histogram, apply the &lt;code&gt;.plot()&lt;/code&gt; method on &lt;code&gt;monthly_max&lt;/code&gt; and specify the keyword arguments &lt;code&gt;bins=8&lt;/code&gt;, &lt;code&gt;alpha=0.5&lt;/code&gt;, and &lt;code&gt;subplots=True&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1:&quot;,[]],&quot;number&quot;,14,&quot;randomNumber&quot;,0.35646497171946145,&quot;assignment&quot;,&quot;&lt;p&gt;Dew point is a measure of relative humidity based on pressure and temperature.\\nA dew point above 65 is considered uncomfortable while a temperature above 90 is also considered\\nuncomfortable.&lt;/p&gt;\\n&lt;p&gt;In this exercise, you will explore the maximum temperature and dew point of each month. The columns of interest are &lt;code&gt;&#39;dew_point_faren&#39;&lt;/code&gt; and &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt;. After resampling them appropriately to get the maximum temperature and dew point in each month, generate a histogram of these values as subplots. Uncomfortably, you will notice that the maximum dew point is above 65 every month! &lt;/p&gt;\\n&lt;p&gt;&lt;code&gt;df_clean&lt;/code&gt; has been pre-loaded for you.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1:&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Heat or humidity&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;import pandas as pd\\n\\ndef read_data(file_name):\\n    # read the data file\\n    df = pd.read_csv(file_name, index_col=False, header=None)\\n\\n    # define the column labels for the real data file from NOAA: column_labels\\n    column_labels = \\&quot;Wban,date,Time,StationType,sky_condition,sky_conditionFlag,visibility,visibilityFlag,wx_and_obst_to_vision,wx_and_obst_to_visionFlag,dry_bulb_faren,dry_bulb_farenFlag,dry_bulb_cel,dry_bulb_celFlag,wet_bulb_faren,wet_bulb_farenFlag,wet_bulb_cel,wet_bulb_celFlag,dew_point_faren,dew_point_farenFlag,dew_point_cel,dew_point_celFlag,relative_humidity,relative_humidityFlag,wind_speed,wind_speedFlag,wind_direction,wind_directionFlag,value_for_wind_character,value_for_wind_characterFlag,station_pressure,station_pressureFlag,pressure_tendency,pressure_tendencyFlag,presschange,presschangeFlag,sea_level_pressure,sea_level_pressureFlag,record_type,hourly_precip,hourly_precipFlag,altimeter,altimeterFlag\\&quot;\\n    \\n    # define the sub-set list of columns to drop\\n    list_to_drop = [ &#39;sky_conditionFlag&#39;, &#39;visibilityFlag&#39;, &#39;wx_and_obst_to_vision&#39;, &#39;wx_and_obst_to_visionFlag&#39;,\\n      &#39;dry_bulb_farenFlag&#39;, &#39;dry_bulb_celFlag&#39;, &#39;wet_bulb_farenFlag&#39;, &#39;wet_bulb_celFlag&#39;, &#39;dew_point_farenFlag&#39;,\\n      &#39;dew_point_celFlag&#39;, &#39;relative_humidityFlag&#39;, &#39;wind_speedFlag&#39;, &#39;wind_directionFlag&#39;, &#39;value_for_wind_character&#39;,\\n      &#39;value_for_wind_characterFlag&#39;, &#39;station_pressureFlag&#39;, &#39;pressure_tendencyFlag&#39;, &#39;pressure_tendency&#39;,\\n      &#39;presschange&#39;,&#39;presschangeFlag&#39;, &#39;sea_level_pressureFlag&#39;, &#39;hourly_precip&#39;,  &#39;hourly_precipFlag&#39;, &#39;altimeter&#39;,  &#39;record_type&#39;, &#39;altimeterFlag&#39;, &#39;junk&#39;]\\n    \\n    # split on the comma to create a list, then append one more element as the data files all have a trailing comma at the end of each line: column_labels_list\\n    column_labels_list = column_labels.split(\\&quot;,\\&quot;)\\n    column_labels_list.append(\\&quot;junk\\&quot;)\\n    \\n    # assign the new colum labels to the dataframe\\n    df.columns = column_labels_list\\n    \\n    # Drop all the column data that we don&#39;t need\\n    df = df.drop(list_to_drop, axis=&#39;columns&#39;)\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with leading (left) zeros, for hours\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:0&gt;4}&#39;.format(x))\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with trailing (right) zeros, for seconds\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:&lt;06}&#39;.format(x))\\n    \\n    # Convert the `date` column to a string so it can be combined with the `Time` column\\n    df[&#39;date&#39;] = df[&#39;date&#39;].apply(str)\\n    \\n    # Create a new date-time container from the updated `date` and `Time` columns: date_times\\n    date_times = pd.to_datetime( df[&#39;date&#39;] + \\&quot; \\&quot; + df[&#39;Time&#39;], format=\\&quot;%Y%m%d %H%M%S\\&quot; )\\n    \\n    # Set the DataFrame index to this new `date_times` container:\\n    df.set_index(date_times, inplace=True)\\n\\n    for col in [&#39;wind_speed&#39;,&#39;dry_bulb_faren&#39;,&#39;dew_point_faren&#39;]:\\n        df[col] = pd.to_numeric(df[col], errors=&#39;coerce&#39;)\\n\\n    return df\\ndata_file = \\&quot;/usr/local/share/datasets/NOAA_QCLCD_2011_hourly_13904.csv\\&quot;\\ndf_clean = read_data(data_file)\\nimport matplotlib.pyplot as plt&quot;,&quot;solution&quot;,&quot;# Resample dew_point_faren and dry_bulb_faren by Month, aggregating the maximum values: monthly_max\\nmonthly_max = df_clean[[&#39;dew_point_faren&#39;,&#39;dry_bulb_faren&#39;]].resample(&#39;M&#39;).max()\\n\\n# Generate a histogram with bins=8, alpha=0.5, subplots=True\\nmonthly_max.plot(kind=&#39;hist&#39;, bins=8, alpha=0.5, subplots=True)\\n\\n# Show the plot\\nplt.show()&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,44666]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Extract the maximum temperature in August 2010 from df_climate: august_max\\naugust_max = ____\\nprint(august_max)\\n\\n# Resample August 2011 temps in df_clean by day &amp; aggregate the max value: august_2011\\naugust_2011 = ____\\n\\n\\n# Filter for days in august_2011 where the value exceeds august_max: august_2011_high\\n\\naugust_2011_high = ____\\n\\n# Construct a CDF of august_2011_high\\n____\\n\\n# Display the plot\\nplt.show()&quot;,&quot;sct&quot;,&quot;Ex().check_correct(\\n  \\tcheck_object(\\&quot;august_max\\&quot;).has_equal_value(\\&quot;The contents of `august_max` are not correct. Did you correctly extract the maximum temperature in August 2010 by selecting the `&#39;2010-Aug&#39;` and `&#39;Temperature&#39;` columns from `df_climate` using `.loc[]` and chaining the `.max()` method?\\&quot;),\\n    has_equal_ast(\\&quot;Did you correctly extract the maximum temperature in August 2010 by selecting the `&#39;2010-Aug&#39;` and `&#39;Temperature&#39;` columns from `df_climate` using `.loc[]` and chaining the `.max()` method?\\&quot;,\\n                  code=\\&quot;df_climate.loc[&#39;2010-Aug&#39;,&#39;Temperature&#39;].max()\\&quot;,\\n                  exact=False))\\n\\nEx().check_function(\\&quot;print\\&quot;).check_args(0).has_equal_value()\\n\\nEx().check_correct(\\n  \\tcheck_object(\\&quot;august_2011\\&quot;).has_equal_value(\\&quot;The contents of `august_2011` are not correct. It seems like you did not resample the data correctly.\\&quot;),\\n    has_equal_ast(\\&quot;It seems like you did not resample the data correctly. Did you select the `&#39;2011-Aug&#39;` and `&#39;dry_bulb_faren&#39;` columns, then use `.resample(&#39;D&#39;)` along with `.max()`?\\&quot;,\\n                  code=\\&quot;df_clean.loc[&#39;2011-Aug&#39;,&#39;dry_bulb_faren&#39;].resample(&#39;D&#39;).max()\\&quot;,\\n                  exact=False))\\n\\nEx().check_correct(\\n  \\tcheck_object(\\&quot;august_2011_high\\&quot;).has_equal_value(\\&quot;The contents of `august_2011_high` aren&#39;t correct. It looks like you didn&#39;t correctly filter out the days in `august_2011` where the value exceeded `august_max`.\\&quot;),\\n    has_equal_ast(\\&quot;The contents of `august_2011_high` aren&#39;t correct. It looks like you didn&#39;t correctly filter out the days in `august_2011` where the value exceeded `august_max`.\\&quot;,\\n                  code=\\&quot;august_2011_high = august_2011.loc[august_2011 &gt; august_max]\\&quot;,\\n                  exact=False))\\n\\nEx().check_function(\\&quot;august_2011_high.plot\\&quot;)\\nEx().check_function(\\&quot;matplotlib.pyplot.show\\&quot;)\\n\\nsuccess_msg(\\&quot;Fantastic work - you&#39;ve reached the end of the course!\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;From &lt;code&gt;df_climate&lt;/code&gt;, extract the maximum temperature observed in August 2010. The relevant column here is &lt;code&gt;&#39;Temperature&#39;&lt;/code&gt;. You can select the rows corresponding to August 2010 in multiple ways. For example, &lt;code&gt;df_climate.loc[&#39;2011-Feb&#39;]&lt;/code&gt; selects all rows corresponding to February 2011, while &lt;code&gt;df_climate.loc[&#39;2009-09&#39;, &#39;Pressure&#39;]&lt;/code&gt; selects the rows corresponding to September 2009 from the &lt;code&gt;&#39;Pressure&#39;&lt;/code&gt; column.&lt;/li&gt;\\n&lt;li&gt;From &lt;code&gt;df_clean&lt;/code&gt;, select the August 2011 temperature data from the &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt;. Resample this data by day and aggregate the maximum value. Store the result in &lt;code&gt;august_2011&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Filter rows of &lt;code&gt;august_2011&lt;/code&gt; to keep days where the value exceeded &lt;code&gt;august_max&lt;/code&gt;. Store the result in &lt;code&gt;august_2011_high&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Construct a CDF of &lt;code&gt;august_2011_high&lt;/code&gt; using 25 bins. Remember to specify the &lt;code&gt;kind&lt;/code&gt;, &lt;code&gt;normed&lt;/code&gt;, and &lt;code&gt;cumulative&lt;/code&gt; parameters in addition to &lt;code&gt;bins&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,71565,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Use &lt;code&gt;.loc[]&lt;/code&gt; on &lt;code&gt;df_climate&lt;/code&gt; to select &lt;code&gt;&#39;2010-Aug&#39;&lt;/code&gt; from the &lt;code&gt;&#39;Temperature&#39;&lt;/code&gt; column. Then, apply the &lt;code&gt;.max()&lt;/code&gt; method to your &lt;code&gt;.loc[]&lt;/code&gt; selection.&lt;/li&gt;\\n&lt;li&gt;As you did above, use &lt;code&gt;.loc[]&lt;/code&gt;, this time on &lt;code&gt;df_clean&lt;/code&gt;, selecting &lt;code&gt;&#39;2011-Aug&#39;&lt;/code&gt; from the &lt;code&gt;&#39;dry_bulb_faren&#39;&lt;/code&gt; column. To resample by day, use &lt;code&gt;.resample(&#39;D&#39;)&lt;/code&gt;, and to aggregate the maximum value, use &lt;code&gt;.max()&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Use &lt;code&gt;.loc[]&lt;/code&gt; on &lt;code&gt;august_2011&lt;/code&gt; with a Boolean condition inside that returns &lt;code&gt;True&lt;/code&gt; if &lt;code&gt;august_2011&lt;/code&gt; is &lt;code&gt;&amp;gt;&lt;/code&gt; &lt;code&gt;august_max&lt;/code&gt;. &lt;/li&gt;\\n&lt;li&gt;To construct a CDF of &lt;code&gt;august_2011_high&lt;/code&gt;, use the &lt;code&gt;.plot()&lt;/code&gt; method on &lt;code&gt;august_2011_high&lt;/code&gt; with &lt;code&gt;kind=&#39;hist&#39;&lt;/code&gt;, &lt;code&gt;normed=True&lt;/code&gt;, &lt;code&gt;cumulative=True&lt;/code&gt;, and the desired number of &lt;code&gt;bins&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1:&quot;,[]],&quot;number&quot;,15,&quot;randomNumber&quot;,0.053210473284661,&quot;assignment&quot;,&quot;&lt;p&gt;We already know that 2011 was hotter than the climate normals for the previous thirty years. In this final exercise, you will compare the maximum temperature in August 2011 against that of the August 2010 climate normals. More specifically, you will use a CDF plot to determine the probability of the 2011 daily maximum temperature in August being above the 2010 climate normal value. To do this, you will leverage the data manipulation, filtering, resampling, and visualization skills you have acquired throughout this course. &lt;/p&gt;\\n&lt;p&gt;The two DataFrames &lt;code&gt;df_clean&lt;/code&gt; and &lt;code&gt;df_climate&lt;/code&gt; are available in the workspace. Your job is to select the maximum temperature in August in &lt;code&gt;df_climate&lt;/code&gt;, and then maximum daily temperatures in  August 2011. You will then filter out the days in August 2011 that were above the August 2010 maximum, and use this to construct a CDF plot. &lt;/p&gt;\\n&lt;p&gt;Once you&#39;ve generated the CDF, notice how it shows that there was a 50% probability of the 2011 daily maximum temperature in August being 5 degrees\\nabove the 2010 climate normal value!&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1:&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Probability of high temperatures&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;import pandas as pd\\n\\ndef read_data(file_name):\\n    # read the data file\\n    df = pd.read_csv(file_name, index_col=False, header=None)\\n\\n    # define the column labels for the real data file from NOAA: column_labels\\n    column_labels = \\&quot;Wban,date,Time,StationType,sky_condition,sky_conditionFlag,visibility,visibilityFlag,wx_and_obst_to_vision,wx_and_obst_to_visionFlag,dry_bulb_faren,dry_bulb_farenFlag,dry_bulb_cel,dry_bulb_celFlag,wet_bulb_faren,wet_bulb_farenFlag,wet_bulb_cel,wet_bulb_celFlag,dew_point_faren,dew_point_farenFlag,dew_point_cel,dew_point_celFlag,relative_humidity,relative_humidityFlag,wind_speed,wind_speedFlag,wind_direction,wind_directionFlag,value_for_wind_character,value_for_wind_characterFlag,station_pressure,station_pressureFlag,pressure_tendency,pressure_tendencyFlag,presschange,presschangeFlag,sea_level_pressure,sea_level_pressureFlag,record_type,hourly_precip,hourly_precipFlag,altimeter,altimeterFlag\\&quot;\\n    \\n    # define the sub-set list of columns to drop\\n    list_to_drop = [ &#39;sky_conditionFlag&#39;, &#39;visibilityFlag&#39;, &#39;wx_and_obst_to_vision&#39;, &#39;wx_and_obst_to_visionFlag&#39;,\\n      &#39;dry_bulb_farenFlag&#39;, &#39;dry_bulb_celFlag&#39;, &#39;wet_bulb_farenFlag&#39;, &#39;wet_bulb_celFlag&#39;, &#39;dew_point_farenFlag&#39;,\\n      &#39;dew_point_celFlag&#39;, &#39;relative_humidityFlag&#39;, &#39;wind_speedFlag&#39;, &#39;wind_directionFlag&#39;, &#39;value_for_wind_character&#39;,\\n      &#39;value_for_wind_characterFlag&#39;, &#39;station_pressureFlag&#39;, &#39;pressure_tendencyFlag&#39;, &#39;pressure_tendency&#39;,\\n      &#39;presschange&#39;,&#39;presschangeFlag&#39;, &#39;sea_level_pressureFlag&#39;, &#39;hourly_precip&#39;,  &#39;hourly_precipFlag&#39;, &#39;altimeter&#39;,  &#39;record_type&#39;, &#39;altimeterFlag&#39;, &#39;junk&#39;]\\n    \\n    # split on the comma to create a list, then append one more element as the data files all have a trailing comma at the end of each line: column_labels_list\\n    column_labels_list = column_labels.split(\\&quot;,\\&quot;)\\n    column_labels_list.append(\\&quot;junk\\&quot;)\\n    \\n    # assign the new colum labels to the dataframe\\n    df.columns = column_labels_list\\n    \\n    # Drop all the column data that we don&#39;t need\\n    df = df.drop(list_to_drop, axis=&#39;columns&#39;)\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with leading (left) zeros, for hours\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:0&gt;4}&#39;.format(x))\\n    \\n    # Clean the `Time` column to prepare for creating a date-time index:\\n    # Pad with trailing (right) zeros, for seconds\\n    df[&#39;Time&#39;] = df[&#39;Time&#39;].apply(lambda x: &#39;{:&lt;06}&#39;.format(x))\\n    \\n    # Convert the `date` column to a string so it can be combined with the `Time` column\\n    df[&#39;date&#39;] = df[&#39;date&#39;].apply(str)\\n    \\n    # Create a new date-time container from the updated `date` and `Time` columns: date_times\\n    date_times = pd.to_datetime( df[&#39;date&#39;] + \\&quot; \\&quot; + df[&#39;Time&#39;], format=\\&quot;%Y%m%d %H%M%S\\&quot; )\\n    \\n    # Set the DataFrame index to this new `date_times` container:\\n    df.set_index(date_times, inplace=True)\\n\\n    for col in [&#39;wind_speed&#39;,&#39;dry_bulb_faren&#39;,&#39;dew_point_faren&#39;]:\\n        df[col] = pd.to_numeric(df[col], errors=&#39;coerce&#39;)\\n\\n    return df\\ndata_file = \\&quot;/usr/local/share/datasets/NOAA_QCLCD_2011_hourly_13904.csv\\&quot;\\ndf_clean = read_data(data_file)\\n\\nclimatology_data_file = \\&quot;/usr/local/share/datasets/weather_data_austin_2010.csv\\&quot;\\n\\ndf_climate = pd.read_csv(climatology_data_file, parse_dates=True, index_col=&#39;Date&#39;)\\n\\n# Import matplotlib.pyplot\\nimport matplotlib.pyplot as plt\\n&quot;,&quot;solution&quot;,&quot;# Extract the maximum temperature in August 2010 from df_climate: august_max\\naugust_max = df_climate.loc[&#39;2010-Aug&#39;,&#39;Temperature&#39;].max()\\nprint(august_max)\\n\\n# Resample August 2011 temps in df_clean by day &amp; aggregate the max value: august_2011\\naugust_2011 = df_clean.loc[&#39;2011-Aug&#39;,&#39;dry_bulb_faren&#39;].resample(&#39;D&#39;).max()\\n\\n# Filter for days in august_2011 where the value exceeds august_max: august_2011_high\\n\\naugust_2011_high = august_2011.loc[august_2011 &gt; august_max]\\n\\n# Construct a CDF of august_2011_high\\naugust_2011_high.plot(kind=&#39;hist&#39;, normed=True, cumulative=True, bins=25)\\n\\n# Display the plot\\nplt.show()&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,71565]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;externalId&quot;,71697,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^1:&quot;,[]],&quot;number&quot;,16,&quot;video_hls&quot;,&quot;//videos.datacamp.com/transcoded/1639_pandas-1/v2/hls-ch4_4.master.m3u8&quot;,&quot;randomNumber&quot;,0.3854862055280488,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^1:&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Congratulations!&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,71697,&quot;projector_key&quot;,&quot;course_1639_346f5e3f0814296f20bfe050c6c12275&quot;,&quot;video_link&quot;,null]]]]]]]]";</script><div id="root"><div class="theme progress-indicator--visible theme--light" data-reactroot=""><header data-cy="header-container" class="dc-header-campus"><a href="https://www.datacamp.com/" data-cy="header-logo" class="dc-header-campus__home"><img alt="datacamp-logo" src="https://campus.datacamp.com/static/media/logo-full-color.018b48cc.svg" style="max-height:29px"></a><div class="dc-nav-course__container"><div class="__react_component_tooltip place-top type-dark " data-id="tooltip"></div><div class="__react_component_tooltip place-top type-dark " data-id="tooltip"></div><div class="__react_component_tooltip place-top type-dark " data-id="tooltip"></div><nav class="dc-nav-course" data-onboarding="course-navigation"><a data-cy="header-previous" class="dc-nav-course__backward" data-tip="true" data-for="nav-tp-prev" href="case-study-sunlight-in-austin495b.html?ex=5"><svg width="12" height="12" aria-label="arrow_2_left icon" class="dc-icon-arrow_2_left dc-nav-course__icon" role="Img" fill="currentColor"><use xlink:href="https://campus.datacamp.com/static/media/symbols.cace91b1.svg#arrow_2_left"/></svg></a><a data-cy="header-outline" class="dc-nav-course__outline" data-tip="true" data-for="nav-tp-outline" href="javascript:void(0)"><svg width="12" height="12" aria-label="bars icon" class="dc-icon-bars dc-nav-course__icon" role="Img" fill="currentColor"><use xlink:href="https://campus.datacamp.com/static/media/symbols.cace91b1.svg#bars"/></svg>Course Outline</a><a data-cy="header-next" class="dc-nav-course__forward" data-tip="true" data-for="nav-tp-next" href="case-study-sunlight-in-austin38d7.html?ex=7"><svg width="12" height="12" aria-label="arrow_2_right icon" class="dc-icon-arrow_2_right dc-nav-course__icon" role="Img" fill="currentColor"><use xlink:href="https://campus.datacamp.com/static/media/symbols.cace91b1.svg#arrow_2_right"/></svg></a></nav></div><nav class="dc-u-fx dc-u-fx-ais dc-u-fx-jcfe dc-u-w-96"><div data-cy="header-session" class="app-status dc-u-fx dc-u-mr-8"><div class="hcSlide-wrapper"></div><svg width="18" height="18" aria-label="circle icon" class="dc-icon-circle dc-u-color-green" role="Img" fill="currentColor"><use xlink:href="https://campus.datacamp.com/static/media/symbols.cace91b1.svg#circle"/></svg></div><div data-tip="true" data-for="tp-notifications"><div class="__react_component_tooltip place-top type-dark " data-id="tooltip"></div><a data-cy="header-notifications" class="ds-icon-action dc-u-fx" href="javascript:void(0)"><svg width="18" height="18" aria-label="notification icon" class="dc-icon-notification" role="Img" fill="currentColor"><use xlink:href="https://campus.datacamp.com/static/media/symbols.cace91b1.svg#notification"/></svg></a></div><a data-cy="header-slides" class="ds-icon-action dc-u-fx dc-u-ml-8" href="javascript:void(0)"><div data-tip="true" data-for="tp-slides"><div class="__react_component_tooltip place-top type-dark " data-id="tooltip"></div><svg width="18" height="18" aria-label="pdf icon" class="dc-icon-pdf dc-u-fx" role="Img" fill="currentColor"><use xlink:href="https://campus.datacamp.com/static/media/symbols.cace91b1.svg#pdf"/></svg></div></a><a data-cy="header-issue" class="ds-icon-action dc-u-fx dc-u-ml-8" data-test-id="header-report-issue-button" href="javascript:void(0)"><div data-tip="true" data-for="tp-issue"><div class="__react_component_tooltip place-top type-dark " data-id="tooltip"></div><svg width="18" height="18" aria-label="attention icon" class="dc-icon-attention dc-u-fx" role="Img" fill="currentColor"><use xlink:href="https://campus.datacamp.com/static/media/symbols.cace91b1.svg#attention"/></svg></div></a></nav></header><div class="exercise-area "><div data-cy="server-side-loader-placeholder"><aside class="exercise--sidebar" style="width:40%"><div class="exercise--sidebar-content"><div class="listview__outer"><div class="listview__inner"><div class="listview__section" data-onboarding="assignment"><div><div role="button" class="listview__header"><div class="exercise--sidebar-header"><h5 class="dc-panel__title"><svg width="12" height="12" aria-label="exercise icon" class="dc-icon-exercise dc-u-color-grey-dark dc-u-mr-8" role="Img" fill="currentColor"><use xlink:href="https://campus.datacamp.com/static/media/symbols.cace91b1.svg#exercise"/></svg>Exercise</h5></div></div></div><div class="listview__content"><div class="exercise--assignment exercise--typography"><h1 class="exercise--title">Cleaning the numeric columns</h1><div class=""><p>The numeric columns contain missing values labeled as <code>&apos;M&apos;</code>. In this exercise, your job is to transform these columns such that they contain only numeric values and interpret missing data as <code>NaN</code>.</p>
<p>The pandas function <code>pd.to_numeric()</code> is ideal for this purpose: It converts a Series of values to floating-point values. Furthermore, by specifying the keyword argument <code>errors=&apos;coerce&apos;</code>, you can force strings like <code>&apos;M&apos;</code> to be interpreted as <code>NaN</code>. </p>
<p>A DataFrame <code>df_clean</code> is provided for you at the start of the exercise, and as usual, pandas has been imported as <code>pd</code>.</p></div></div></div></div><div class="listview__section" style="min-height:calc(100% - 33px)" data-onboarding="instructions"><div><div role="button" class="listview__header"><div class="exercise--sidebar-header"><h5 class="dc-panel__title"><svg width="12" height="12" aria-label="checkmark_circle icon" class="dc-icon-checkmark_circle dc-u-color-grey-dark dc-u-mr-8" role="Img" fill="currentColor"><use xlink:href="https://campus.datacamp.com/static/media/symbols.cace91b1.svg#checkmark_circle"/></svg>Instructions</h5><span class="tag tag--xp">100<!-- --> XP</span></div></div></div><div class="listview__content"><div><div class=""><div data-onboarding="instructions" class="exercise--instructions exercise--typography"><div class="exercise--instructions__content"><ul>
<li>Print the <code>&apos;dry_bulb_faren&apos;</code> temperature between 8 AM and 9 AM on June 20, 2011.</li>
<li>Convert the <code>&apos;dry_bulb_faren&apos;</code> column to numeric values with <code>pd.to_numeric()</code>. Specify <code>errors=&apos;coerce&apos;</code>.</li>
<li>Print the transformed <code>dry_bulb_faren</code> temperature between 8 AM and 9 AM on June 20, 2011.</li>
<li>Convert the <code>&apos;wind_speed&apos;</code> and <code>&apos;dew_point_faren&apos;</code> columns to numeric values with <code>pd.to_numeric()</code>. Again, specify <code>errors=&apos;coerce&apos;</code>.</li>
</ul></div><div class="campus-dc-sct-feedback" tabindex="-1"><div style="position:absolute;width:0;height:0;visibility:hidden;display:none"></div><ul class="campus-dc-sct-feedback__tab-list"><div style="display:inline-block" data-tip="true" data-for="tp-hint"><div class="__react_component_tooltip place-top type-dark " data-id="tooltip"></div><a class="exercise--show-hint" data-cy="exercise-show-hint" href="javascript:void(0)"><svg width="18" height="18" aria-label="lightbulb icon" class="dc-icon-lightbulb dc-u-mr-4" role="Img" fill="currentColor"><use xlink:href="https://campus.datacamp.com/static/media/symbols.cace91b1.svg#lightbulb"/></svg><span>Take Hint<!-- --> (-<!-- -->30<!-- --> XP)</span></a></div></ul></div></div></div></div></div></div></div></div></div></aside><section class="exercise--content" style="width:60%"><div class="exercise-waiting"><div class="global-spinner"><object type="image/svg+xml" data="https://campus.datacamp.com/static/media/spinner.dd0612cb.svg" aria-label="Loading"></object></div><noscript></noscript></div></section></div><div class="Toastify"></div></div><div class="exercise-footer"><ul data-cy="progress-container" class="dc-progress-indicator"><li class="dc-progress-indicator__item"><a class="dc-progress-indicator__bar" href="javascript:void(0)"><div class="dc-progress-indicator__fill" style="width:0%"></div></a></li><li class="dc-progress-indicator__item"><a class="dc-progress-indicator__bar" href="javascript:void(0)"><div class="dc-progress-indicator__fill" style="width:0%"></div></a></li><li class="dc-progress-indicator__item"><a class="dc-progress-indicator__bar" href="javascript:void(0)"><div class="dc-progress-indicator__fill" style="width:0%"></div></a></li><li class="dc-progress-indicator__item"><a class="dc-progress-indicator__bar" href="javascript:void(0)"><div class="dc-progress-indicator__fill" style="width:0%"></div></a></li></ul></div></div></div><script type="text/x-mathjax-config">MathJax && MathJax.Hub && MathJax.Hub.Config && MathJax.Hub.Config({
        messageStyle: "none"
      });</script><script type="text/javascript" src="../../../cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJaxdda6.js?config=TeX-AMS-MML_HTMLorMML"></script></body>
<!-- Mirrored from campus.datacamp.com/courses/pandas-foundations/case-study-sunlight-in-austin?ex=6 by HTTrack Website Copier/3.x [XR&CO'2014], Mon, 25 Feb 2019 20:43:35 GMT -->
</html>